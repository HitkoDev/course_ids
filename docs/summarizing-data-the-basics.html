<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Chapter 6 Summarizing data - the basics | Introduction to data science</title>
  <meta name="description" content="Course notes">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="Chapter 6 Summarizing data - the basics | Introduction to data science" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://fri-datascience.github.io/course_itds/" />
  
  <meta property="og:description" content="Course notes" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 6 Summarizing data - the basics | Introduction to data science" />
  
  <meta name="twitter:description" content="Course notes" />
  

<meta name="author" content="Slavko Žitnik, Tomaž Curk, Erik Štrumbelj">


<meta name="date" content="2019-08-04">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="dynamic-reports-and-reproducibility.html">
<link rel="next" href="summarizing-data-visualization.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Introduction to data science</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="1" data-path="python-programming-language.html"><a href="python-programming-language.html"><i class="fa fa-check"></i><b>1</b> Python programming language</a><ul>
<li class="chapter" data-level="1.1" data-path="python-programming-language.html"><a href="python-programming-language.html#basic-characteristics"><i class="fa fa-check"></i><b>1.1</b> Basic characteristics</a></li>
<li class="chapter" data-level="1.2" data-path="python-programming-language.html"><a href="python-programming-language.html#why-python"><i class="fa fa-check"></i><b>1.2</b> Why Python?</a></li>
<li class="chapter" data-level="1.3" data-path="python-programming-language.html"><a href="python-programming-language.html#setting-up-the-environment"><i class="fa fa-check"></i><b>1.3</b> Setting up the environment</a><ul>
<li class="chapter" data-level="1.3.1" data-path="python-programming-language.html"><a href="python-programming-language.html#anaconda-distribution-installation"><i class="fa fa-check"></i><b>1.3.1</b> Anaconda distribution installation</a></li>
<li class="chapter" data-level="1.3.2" data-path="python-programming-language.html"><a href="python-programming-language.html#pure-python-distribution-installation"><i class="fa fa-check"></i><b>1.3.2</b> Pure Python distribution installation</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="python-programming-language.html"><a href="python-programming-language.html#installing-dependencies"><i class="fa fa-check"></i><b>1.4</b> Installing dependencies</a></li>
<li class="chapter" data-level="1.5" data-path="python-programming-language.html"><a href="python-programming-language.html#jupyter-notebooks"><i class="fa fa-check"></i><b>1.5</b> Jupyter notebooks</a><ul>
<li class="chapter" data-level="1.5.1" data-path="python-programming-language.html"><a href="python-programming-language.html#running-a-jupyter-notebook"><i class="fa fa-check"></i><b>1.5.1</b> Running a Jupyter notebook</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="python-programming-language.html"><a href="python-programming-language.html#short-introduction-to-python"><i class="fa fa-check"></i><b>1.6</b> Short introduction to Python</a><ul>
<li class="chapter" data-level="" data-path="python-programming-language.html"><a href="python-programming-language.html#basics"><i class="fa fa-check"></i>Basics</a></li>
<li class="chapter" data-level="1.6.1" data-path="python-programming-language.html"><a href="python-programming-language.html#control-flow-operations"><i class="fa fa-check"></i><b>1.6.1</b> Control flow operations</a></li>
<li class="chapter" data-level="1.6.2" data-path="python-programming-language.html"><a href="python-programming-language.html#functions"><i class="fa fa-check"></i><b>1.6.2</b> Functions</a></li>
<li class="chapter" data-level="1.6.3" data-path="python-programming-language.html"><a href="python-programming-language.html#classes-and-objects"><i class="fa fa-check"></i><b>1.6.3</b> Classes and objects</a></li>
<li class="chapter" data-level="1.6.4" data-path="python-programming-language.html"><a href="python-programming-language.html#python-ides-and-code-editors"><i class="fa fa-check"></i><b>1.6.4</b> Python IDE’s and code editors</a></li>
</ul></li>
<li class="chapter" data-level="1.7" data-path="python-programming-language.html"><a href="python-programming-language.html#python-ecosystem-for-data-science"><i class="fa fa-check"></i><b>1.7</b> Python ecosystem for Data Science</a></li>
<li class="chapter" data-level="1.8" data-path="python-programming-language.html"><a href="python-programming-language.html#further-reading-and-references"><i class="fa fa-check"></i><b>1.8</b> Further reading and references</a></li>
<li class="chapter" data-level="1.9" data-path="python-programming-language.html"><a href="python-programming-language.html#learning-outcomes"><i class="fa fa-check"></i><b>1.9</b> Learning outcomes</a></li>
<li class="chapter" data-level="1.10" data-path="python-programming-language.html"><a href="python-programming-language.html#practice-problems"><i class="fa fa-check"></i><b>1.10</b> Practice problems</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="source-code-control.html"><a href="source-code-control.html"><i class="fa fa-check"></i><b>2</b> Source code control</a></li>
<li class="chapter" data-level="3" data-path="docker-container-platform.html"><a href="docker-container-platform.html"><i class="fa fa-check"></i><b>3</b> Docker container platform</a><ul>
<li class="chapter" data-level="3.1" data-path="docker-container-platform.html"><a href="docker-container-platform.html#why-docker"><i class="fa fa-check"></i><b>3.1</b> Why Docker?</a></li>
<li class="chapter" data-level="3.2" data-path="docker-container-platform.html"><a href="docker-container-platform.html#setting-up-the-environment-1"><i class="fa fa-check"></i><b>3.2</b> Setting up the environment</a></li>
<li class="chapter" data-level="3.3" data-path="docker-container-platform.html"><a href="docker-container-platform.html#short-introduction-to-docker"><i class="fa fa-check"></i><b>3.3</b> Short introduction to Docker</a><ul>
<li class="chapter" data-level="3.3.1" data-path="docker-container-platform.html"><a href="docker-container-platform.html#basics-1"><i class="fa fa-check"></i><b>3.3.1</b> Basics</a></li>
<li class="chapter" data-level="3.3.2" data-path="docker-container-platform.html"><a href="docker-container-platform.html#docker-application-example"><i class="fa fa-check"></i><b>3.3.2</b> Docker application example</a></li>
<li class="chapter" data-level="3.3.3" data-path="docker-container-platform.html"><a href="docker-container-platform.html#volumes"><i class="fa fa-check"></i><b>3.3.3</b> Volumes</a></li>
<li class="chapter" data-level="3.3.4" data-path="docker-container-platform.html"><a href="docker-container-platform.html#docker-application-example-with-multiple-services"><i class="fa fa-check"></i><b>3.3.4</b> Docker application example with multiple services</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="docker-container-platform.html"><a href="docker-container-platform.html#further-reading-and-references-1"><i class="fa fa-check"></i><b>3.4</b> Further reading and references</a></li>
<li class="chapter" data-level="3.5" data-path="docker-container-platform.html"><a href="docker-container-platform.html#learning-outcomes-1"><i class="fa fa-check"></i><b>3.5</b> Learning outcomes</a></li>
<li class="chapter" data-level="3.6" data-path="docker-container-platform.html"><a href="docker-container-platform.html#practice-problems-1"><i class="fa fa-check"></i><b>3.6</b> Practice problems</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="web-scraping.html"><a href="web-scraping.html"><i class="fa fa-check"></i><b>4</b> Web scraping</a></li>
<li class="chapter" data-level="5" data-path="dynamic-reports-and-reproducibility.html"><a href="dynamic-reports-and-reproducibility.html"><i class="fa fa-check"></i><b>5</b> Dynamic reports and reproducibility</a></li>
<li class="chapter" data-level="6" data-path="summarizing-data-the-basics.html"><a href="summarizing-data-the-basics.html"><i class="fa fa-check"></i><b>6</b> Summarizing data - the basics</a><ul>
<li class="chapter" data-level="6.1" data-path="summarizing-data-the-basics.html"><a href="summarizing-data-the-basics.html#descriptive-statistics-for-univariate-distributions"><i class="fa fa-check"></i><b>6.1</b> Descriptive statistics for univariate distributions</a><ul>
<li class="chapter" data-level="6.1.1" data-path="summarizing-data-the-basics.html"><a href="summarizing-data-the-basics.html#central-tendency"><i class="fa fa-check"></i><b>6.1.1</b> Central tendency</a></li>
<li class="chapter" data-level="6.1.2" data-path="summarizing-data-the-basics.html"><a href="summarizing-data-the-basics.html#dispersion"><i class="fa fa-check"></i><b>6.1.2</b> Dispersion</a></li>
<li class="chapter" data-level="6.1.3" data-path="summarizing-data-the-basics.html"><a href="summarizing-data-the-basics.html#skewness-and-kurtosis"><i class="fa fa-check"></i><b>6.1.3</b> Skewness and kurtosis</a></li>
<li class="chapter" data-level="6.1.4" data-path="summarizing-data-the-basics.html"><a href="summarizing-data-the-basics.html#nominal-variables"><i class="fa fa-check"></i><b>6.1.4</b> Nominal variables</a></li>
<li class="chapter" data-level="6.1.5" data-path="summarizing-data-the-basics.html"><a href="summarizing-data-the-basics.html#testing-the-shape-of-a-distribution"><i class="fa fa-check"></i><b>6.1.5</b> Testing the shape of a distribution</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="summarizing-data-the-basics.html"><a href="summarizing-data-the-basics.html#descriptive-statistics-for-bivariate-distributions"><i class="fa fa-check"></i><b>6.2</b> Descriptive statistics for bivariate distributions</a></li>
<li class="chapter" data-level="6.3" data-path="summarizing-data-the-basics.html"><a href="summarizing-data-the-basics.html#further-reading-and-references-2"><i class="fa fa-check"></i><b>6.3</b> Further reading and references</a></li>
<li class="chapter" data-level="6.4" data-path="summarizing-data-the-basics.html"><a href="summarizing-data-the-basics.html#learning-outcomes-2"><i class="fa fa-check"></i><b>6.4</b> Learning outcomes</a></li>
<li class="chapter" data-level="6.5" data-path="summarizing-data-the-basics.html"><a href="summarizing-data-the-basics.html#practice-problems-2"><i class="fa fa-check"></i><b>6.5</b> Practice problems</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="summarizing-data-visualization.html"><a href="summarizing-data-visualization.html"><i class="fa fa-check"></i><b>7</b> Summarizing data - visualization</a><ul>
<li class="chapter" data-level="7.1" data-path="summarizing-data-visualization.html"><a href="summarizing-data-visualization.html#histograms-and-density-plots"><i class="fa fa-check"></i><b>7.1</b> Histograms and density plots</a></li>
<li class="chapter" data-level="7.2" data-path="summarizing-data-visualization.html"><a href="summarizing-data-visualization.html#bar-plot"><i class="fa fa-check"></i><b>7.2</b> Bar plot</a></li>
<li class="chapter" data-level="7.3" data-path="summarizing-data-visualization.html"><a href="summarizing-data-visualization.html#pie-chart"><i class="fa fa-check"></i><b>7.3</b> Pie chart</a></li>
<li class="chapter" data-level="7.4" data-path="summarizing-data-visualization.html"><a href="summarizing-data-visualization.html#scatterplot"><i class="fa fa-check"></i><b>7.4</b> Scatterplot</a></li>
<li class="chapter" data-level="7.5" data-path="summarizing-data-visualization.html"><a href="summarizing-data-visualization.html#d-density-plot"><i class="fa fa-check"></i><b>7.5</b> 2D density plot</a></li>
<li class="chapter" data-level="7.6" data-path="summarizing-data-visualization.html"><a href="summarizing-data-visualization.html#boxplot"><i class="fa fa-check"></i><b>7.6</b> Boxplot</a></li>
<li class="chapter" data-level="7.7" data-path="summarizing-data-visualization.html"><a href="summarizing-data-visualization.html#violin-plot"><i class="fa fa-check"></i><b>7.7</b> Violin plot</a></li>
<li class="chapter" data-level="7.8" data-path="summarizing-data-visualization.html"><a href="summarizing-data-visualization.html#correlogram"><i class="fa fa-check"></i><b>7.8</b> Correlogram</a></li>
<li class="chapter" data-level="7.9" data-path="summarizing-data-visualization.html"><a href="summarizing-data-visualization.html#a-comprehensive-summary"><i class="fa fa-check"></i><b>7.9</b> A comprehensive summary</a></li>
<li class="chapter" data-level="7.10" data-path="summarizing-data-visualization.html"><a href="summarizing-data-visualization.html#further-reading-and-references-3"><i class="fa fa-check"></i><b>7.10</b> Further reading and references</a></li>
<li class="chapter" data-level="7.11" data-path="summarizing-data-visualization.html"><a href="summarizing-data-visualization.html#learning-outcomes-3"><i class="fa fa-check"></i><b>7.11</b> Learning outcomes</a></li>
<li class="chapter" data-level="7.12" data-path="summarizing-data-visualization.html"><a href="summarizing-data-visualization.html#practice-problems-3"><i class="fa fa-check"></i><b>7.12</b> Practice problems</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="summarizing-data-multivariate-data.html"><a href="summarizing-data-multivariate-data.html"><i class="fa fa-check"></i><b>8</b> Summarizing data - multivariate data</a><ul>
<li class="chapter" data-level="8.1" data-path="summarizing-data-multivariate-data.html"><a href="summarizing-data-multivariate-data.html#principal-component-analysis-pca"><i class="fa fa-check"></i><b>8.1</b> Principal Component Analysis (PCA)</a></li>
<li class="chapter" data-level="8.2" data-path="summarizing-data-multivariate-data.html"><a href="summarizing-data-multivariate-data.html#factor-analysis-fa"><i class="fa fa-check"></i><b>8.2</b> Factor analysis (FA)</a></li>
<li class="chapter" data-level="8.3" data-path="summarizing-data-multivariate-data.html"><a href="summarizing-data-multivariate-data.html#multi-dimensional-scaling-mds"><i class="fa fa-check"></i><b>8.3</b> Multi-dimensional scaling (MDS)</a></li>
<li class="chapter" data-level="8.4" data-path="summarizing-data-multivariate-data.html"><a href="summarizing-data-multivariate-data.html#t-distributed-stochastic-neighbor-embedding-t-sne"><i class="fa fa-check"></i><b>8.4</b> t-Distributed Stochastic Neighbor Embedding (t-SNE)</a></li>
<li class="chapter" data-level="8.5" data-path="summarizing-data-multivariate-data.html"><a href="summarizing-data-multivariate-data.html#clustering"><i class="fa fa-check"></i><b>8.5</b> Clustering</a><ul>
<li class="chapter" data-level="8.5.1" data-path="summarizing-data-multivariate-data.html"><a href="summarizing-data-multivariate-data.html#k-means-clustering"><i class="fa fa-check"></i><b>8.5.1</b> k-means clustering</a></li>
<li class="chapter" data-level="8.5.2" data-path="summarizing-data-multivariate-data.html"><a href="summarizing-data-multivariate-data.html#determining-the-number-of-clusters"><i class="fa fa-check"></i><b>8.5.2</b> Determining the number of clusters</a></li>
<li class="chapter" data-level="8.5.3" data-path="summarizing-data-multivariate-data.html"><a href="summarizing-data-multivariate-data.html#agglomerative-hierarchical-clustering"><i class="fa fa-check"></i><b>8.5.3</b> Agglomerative hierarchical clustering</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="summarizing-data-multivariate-data.html"><a href="summarizing-data-multivariate-data.html#further-reading-and-references-4"><i class="fa fa-check"></i><b>8.6</b> Further reading and references</a></li>
<li class="chapter" data-level="8.7" data-path="summarizing-data-multivariate-data.html"><a href="summarizing-data-multivariate-data.html#learning-outcomes-4"><i class="fa fa-check"></i><b>8.7</b> Learning outcomes</a></li>
<li class="chapter" data-level="8.8" data-path="summarizing-data-multivariate-data.html"><a href="summarizing-data-multivariate-data.html#practice-problems-4"><i class="fa fa-check"></i><b>8.8</b> Practice problems</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="relational-databases.html"><a href="relational-databases.html"><i class="fa fa-check"></i><b>9</b> Relational databases</a></li>
<li class="chapter" data-level="10" data-path="predictive-modelling-introduction.html"><a href="predictive-modelling-introduction.html"><i class="fa fa-check"></i><b>10</b> Predictive modelling - Introduction</a></li>
<li class="chapter" data-level="11" data-path="predictive-modelling-learning-paradigms.html"><a href="predictive-modelling-learning-paradigms.html"><i class="fa fa-check"></i><b>11</b> Predictive modelling - Learning paradigms</a></li>
<li class="chapter" data-level="12" data-path="predictive-modelling-feature-selection.html"><a href="predictive-modelling-feature-selection.html"><i class="fa fa-check"></i><b>12</b> Predictive modelling - Feature selection</a></li>
<li class="chapter" data-level="13" data-path="dealing-with-missing-data.html"><a href="dealing-with-missing-data.html"><i class="fa fa-check"></i><b>13</b> Dealing with missing data</a><ul>
<li class="chapter" data-level="13.1" data-path="dealing-with-missing-data.html"><a href="dealing-with-missing-data.html#the-severity-of-the-missing-data-problem"><i class="fa fa-check"></i><b>13.1</b> The severity of the missing data problem</a></li>
<li class="chapter" data-level="13.2" data-path="dealing-with-missing-data.html"><a href="dealing-with-missing-data.html#visually-exploring-missingness"><i class="fa fa-check"></i><b>13.2</b> Visually exploring missingness</a></li>
<li class="chapter" data-level="13.3" data-path="dealing-with-missing-data.html"><a href="dealing-with-missing-data.html#deletion-methods"><i class="fa fa-check"></i><b>13.3</b> Deletion methods</a><ul>
<li class="chapter" data-level="13.3.1" data-path="dealing-with-missing-data.html"><a href="dealing-with-missing-data.html#column-deletion"><i class="fa fa-check"></i><b>13.3.1</b> Column deletion</a></li>
<li class="chapter" data-level="13.3.2" data-path="dealing-with-missing-data.html"><a href="dealing-with-missing-data.html#row-deletion"><i class="fa fa-check"></i><b>13.3.2</b> Row deletion</a></li>
<li class="chapter" data-level="13.3.3" data-path="dealing-with-missing-data.html"><a href="dealing-with-missing-data.html#pairwise-deletion"><i class="fa fa-check"></i><b>13.3.3</b> Pairwise deletion</a></li>
</ul></li>
<li class="chapter" data-level="13.4" data-path="dealing-with-missing-data.html"><a href="dealing-with-missing-data.html#imputation-methods"><i class="fa fa-check"></i><b>13.4</b> Imputation methods</a><ul>
<li class="chapter" data-level="13.4.1" data-path="dealing-with-missing-data.html"><a href="dealing-with-missing-data.html#single-imputation-with-the-mean"><i class="fa fa-check"></i><b>13.4.1</b> Single imputation with the mean</a></li>
<li class="chapter" data-level="13.4.2" data-path="dealing-with-missing-data.html"><a href="dealing-with-missing-data.html#single-imputation-with-prediction"><i class="fa fa-check"></i><b>13.4.2</b> Single imputation with prediction</a></li>
<li class="chapter" data-level="13.4.3" data-path="dealing-with-missing-data.html"><a href="dealing-with-missing-data.html#multiple-imputation"><i class="fa fa-check"></i><b>13.4.3</b> Multiple imputation</a></li>
</ul></li>
<li class="chapter" data-level="13.5" data-path="dealing-with-missing-data.html"><a href="dealing-with-missing-data.html#summary"><i class="fa fa-check"></i><b>13.5</b> Summary</a></li>
<li class="chapter" data-level="13.6" data-path="dealing-with-missing-data.html"><a href="dealing-with-missing-data.html#further-reading-and-references-5"><i class="fa fa-check"></i><b>13.6</b> Further reading and references</a></li>
<li class="chapter" data-level="13.7" data-path="dealing-with-missing-data.html"><a href="dealing-with-missing-data.html#learning-outcomes-5"><i class="fa fa-check"></i><b>13.7</b> Learning outcomes</a></li>
<li class="chapter" data-level="13.8" data-path="dealing-with-missing-data.html"><a href="dealing-with-missing-data.html#practice-problems-5"><i class="fa fa-check"></i><b>13.8</b> Practice problems</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>      
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Introduction to data science</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="summarizing-data---the-basics" class="section level1">
<h1><span class="header-section-number">Chapter 6</span> Summarizing data - the basics</h1>
<p>Data summarization is the science and art of conveying information more effectivelly and efficiently. Data summarization is typically numerical, visual or a combination of the two. It is a key skill in data analysis - we use it to provide insights both to others and to ourselves. Data summarization is also an integral part of exploratory data analysis.</p>
<p>In this chapter we focus on the basic techniques for univariate and bivariate data. Visualization and more advanced data summarization techniques will be covered in later chapters.</p>
<div id="descriptive-statistics-for-univariate-distributions" class="section level2">
<h2><span class="header-section-number">6.1</span> Descriptive statistics for univariate distributions</h2>
<p>We humans are not particularly good at thinking in multiple dimensions, so, in practice, there will be a tendency to look at individual variables and dimensions. That is, in practice, we will most of the time be summarizing univariate distributions.</p>
<p>Univariate distributions come from various sources. It might be a theoretical distribution, an empirical distribution of a data sample, a probabilistic opinion from a person, a posterior distribution of a parameter from a Bayesian model, and many others. Descriptive statistics apply to all of these cases in the same way, regardless of the source of the distribution.</p>
<p>Before we proceed with introducing the most commonly used descriptive statistics, we discuss their main purpose. The main purpose of any sort of data summarization technique is to (a) reduce the time and effort of delivering information to the reader in a way that (b) we lose as little relevant information as possible. That is, to compress the information.</p>
<p>All summarization methods do (a) but we must be careful to choose an appropriate method so that we also get (b). Summarizing out relevant information can lead to misleading summaries, as we will illustrate with several examples.</p>
<div id="central-tendency" class="section level3">
<h3><span class="header-section-number">6.1.1</span> Central tendency</h3>
<p>The most common first summary of a distribution is its typical value, also known as the location or central tendency of a distribution.</p>
<p>The most common summaries of the location of a distribution are:</p>
<ul>
<li>the <strong>mean</strong> (the mass centre of the distribution),</li>
<li>the <strong>median</strong> or 2nd quartile (the value such that half of the mass is on one and half on the other side),</li>
<li>the <strong>mode</strong> (the most probable value or the value with the highest density).</li>
</ul>
<p>Given a sample of data, the estimate of the mean is the easiest to compute (we compute the average), but the median and mode are more robust to outliers - extreme and possibly unrepresentative values.</p>
<p>In the case of unimodal approximately symmetrical distributions, such as the univariate normal distribution, all these measures of central tendency will be similar and all will be an excellent summary of location. However, if the distribution is asymmetrical or skewed, they will differ. In such cases it is our job to determine what information we want to convey and which summary of central tendency is the most appropriate, if any.</p>
<p>For example, observe the Gamma(1.5, 0.1) distribution and its mean (red), median (blue) and mode (green):</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">x &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="dv">0</span>, <span class="dv">50</span>, <span class="fl">0.01</span>)
a &lt;-<span class="st"> </span><span class="fl">1.5</span>
b &lt;-<span class="st"> </span><span class="fl">0.1</span>
y &lt;-<span class="st"> </span><span class="kw">dgamma</span>(x, a, b)

<span class="kw">library</span>(ggplot2)
<span class="kw">ggplot</span>(<span class="kw">data.frame</span>(x,y), <span class="kw">aes</span>(<span class="dt">x =</span> x, <span class="dt">y =</span> y)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_line</span>() <span class="op">+</span><span class="st"> </span><span class="kw">ylab</span>(<span class="st">&quot;p(x)&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_vline</span>(<span class="dt">xintercept =</span> (a<span class="op">-</span><span class="dv">1</span>) <span class="op">/</span><span class="st"> </span>b, <span class="dt">colour =</span> <span class="st">&quot;green&quot;</span>, <span class="dt">lty =</span> <span class="st">&quot;dashed&quot;</span>, <span class="dt">lwd =</span> <span class="fl">1.5</span>) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_vline</span>(<span class="dt">xintercept =</span> a <span class="op">/</span><span class="st"> </span>b, <span class="dt">colour =</span> <span class="st">&quot;red&quot;</span>, <span class="dt">lty =</span> <span class="st">&quot;dashed&quot;</span>, <span class="dt">lwd =</span> <span class="fl">1.5</span>) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_vline</span>(<span class="dt">xintercept =</span> <span class="kw">qgamma</span>(<span class="fl">0.5</span>, a, b), <span class="dt">colour =</span> <span class="st">&quot;blue&quot;</span>, <span class="dt">lty =</span> <span class="st">&quot;dashed&quot;</span>, <span class="dt">lwd =</span> <span class="fl">1.5</span>)</code></pre></div>
<p><img src="Summarizing-data-01_files/figure-html/unnamed-chunk-1-1.png" width="384" /></p>
<p>In the case of multi-modal distributions, no single measure of central tendency will adequately summarize the distribution - they will all be misleading. For example, look at this bimodal distribution:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">x &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="op">-</span><span class="dv">10</span>, <span class="dv">20</span>, <span class="fl">0.01</span>)
y &lt;-<span class="st"> </span><span class="fl">0.6</span> <span class="op">*</span><span class="st"> </span><span class="kw">dnorm</span>(x, <span class="dv">2</span>, <span class="dv">1</span>) <span class="op">+</span><span class="st"> </span><span class="fl">0.4</span> <span class="op">*</span><span class="st"> </span><span class="kw">dnorm</span>(x, <span class="dv">12</span>, <span class="dv">2</span>)

<span class="kw">library</span>(ggplot2)
<span class="kw">ggplot</span>(<span class="kw">data.frame</span>(x,y), <span class="kw">aes</span>(<span class="dt">x =</span> x, <span class="dt">y =</span> y)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_line</span>() <span class="op">+</span><span class="st"> </span><span class="kw">ylab</span>(<span class="st">&quot;p(x)&quot;</span>) <span class="op">+</span>
<span class="st">    </span><span class="kw">geom_vline</span>(<span class="dt">xintercept =</span> <span class="fl">0.6</span><span class="op">*</span><span class="st"> </span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span><span class="fl">0.4</span> <span class="op">*</span><span class="st"> </span><span class="dv">15</span>, <span class="dt">colour =</span> <span class="st">&quot;red&quot;</span>, <span class="dt">lty =</span> <span class="st">&quot;dashed&quot;</span>, <span class="dt">lwd =</span> <span class="fl">1.5</span>) <span class="op">+</span>
<span class="st">   </span><span class="kw">geom_vline</span>(<span class="dt">xintercept =</span> <span class="dv">2</span>, <span class="dt">colour =</span> <span class="st">&quot;green&quot;</span>, <span class="dt">lty =</span> <span class="st">&quot;dashed&quot;</span>, <span class="dt">lwd =</span> <span class="fl">1.5</span>)</code></pre></div>
<p><img src="Summarizing-data-01_files/figure-html/unnamed-chunk-2-1.png" width="384" /></p>
</div>
<div id="dispersion" class="section level3">
<h3><span class="header-section-number">6.1.2</span> Dispersion</h3>
<p>Once location is established, we are typically interested in whether the values of the distribution cluster close to the location or are spread far from the location.</p>
<p>The most common ways of measuring such dispersion (or spread or scale) of a distribution are:</p>
<ul>
<li><strong>variance</strong> (mean of quadratic distances from mean) or, more commonly, <strong>standard deviation</strong> (root of variance, so we are on the same scale as the measurement)</li>
<li><strong><em>median absolute deviation</em></strong> (median of absolute distances from mean),</li>
<li><strong>quantile-based intervals</strong>, in particular the inter-quartile range (IQR) (interval between the 1st and 3rd quartiles, 50% of the mass/density lies in this interval).</li>
</ul>
<p>Standard deviation is the most commonly used and median absolute deviation is more robust to outliers.</p>
<p>Again, in the case of distributions that are approximately normal, the standard deviation and the mean will be the practically optimal choice for summarization, because they correspond directly to the two parameters of the normal distribution. That is, they completely summarize the distribution without loss of information. We also know that approximately 95% (99%) of the normal density lies within 2 (3) standard deviations from the mean. Standard deviation is useful even if the distribution is not approximately normal as it does provide some information, combined with the sample size (producing the standard error), on how certain we can be in our estimate of the mean.</p>
<p>But, as before, the more we deviate from normality, the less meaningful standard deviation becomes and it makes more sense to use quantile-based intervals. For example, if we estimate the mean and <span class="math inline">\(\pm\)</span> 2 standard deviations for samples from the Gamma distribution from before, we get the following:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">0</span>)
x &lt;-<span class="st"> </span><span class="kw">rgamma</span>(<span class="dv">1000</span>, a, b)
<span class="kw">cat</span>(<span class="kw">sprintf</span>(<span class="st">&quot;%.2f +/- %.2f</span><span class="ch">\n</span><span class="st">&quot;</span>, <span class="kw">mean</span>(x), <span class="dv">2</span><span class="op">*</span><span class="kw">sd</span>(x)))</code></pre></div>
<pre><code>## 14.66 +/- 24.49</code></pre>
<p>That is, the 95% interval estimated this way also includes negative values, which is misleading and absurd - Gamma distributed variables are positive. Computing the IQR or the 95% range interval provides a more sensible summary of this skewed distribution and, together with the mean also serve as an indicator that the distribution is skewed (the mean is not the centre of the intervals):</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">0</span>)
x &lt;-<span class="st"> </span><span class="kw">rgamma</span>(<span class="dv">1000</span>, a, b)
<span class="kw">cat</span>(<span class="kw">sprintf</span>(<span class="st">&quot;%.2f, IQR = [%.2f, %.2f], 95pct = [%.2f, %.2f]</span><span class="ch">\n</span><span class="st">&quot;</span>, <span class="kw">mean</span>(x), <span class="kw">quantile</span>(x, <span class="fl">0.25</span>), <span class="kw">quantile</span>(x, <span class="fl">0.75</span>), <span class="kw">quantile</span>(x, <span class="fl">0.025</span>), <span class="kw">quantile</span>(x, <span class="fl">0.975</span>)))</code></pre></div>
<pre><code>## 14.66, IQR = [5.72, 20.76], 95pct = [1.18, 46.09]</code></pre>
<p>And, again, for multi-modal distributions, we can adequately summarize them only by identifying the modes visually and/or describing each mode individually.</p>
</div>
<div id="skewness-and-kurtosis" class="section level3">
<h3><span class="header-section-number">6.1.3</span> Skewness and kurtosis</h3>
<p>As mentioned above, ranges can be used to indicate a distributions asymmetry (skewness) or fat-tailedness (kurtosis). Although less commonly used, there exist numerical summaries of skewness and kurtosis that can be used instead.</p>
<p>The following example shows the kurtosis and skewness for a gamma, normal, logistic and bimodal distribution. Observe how how the standard way of calculating kurtosis fails for the bimodal and assigns it the lowest kurtosis:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(moments)
<span class="kw">set.seed</span>(<span class="dv">0</span>)
tmp &lt;-<span class="st"> </span><span class="ot">NULL</span>

<span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">4</span>) {
  <span class="cf">if</span> (i <span class="op">==</span><span class="st"> </span><span class="dv">1</span>) x &lt;-<span class="st"> </span><span class="kw">rgamma</span>(<span class="dv">1000</span>, a, b)
  <span class="cf">if</span> (i <span class="op">==</span><span class="st"> </span><span class="dv">2</span>) x &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">1000</span>, <span class="dv">0</span>, <span class="dv">9</span>)
  <span class="cf">if</span> (i <span class="op">==</span><span class="st"> </span><span class="dv">3</span>) x &lt;-<span class="st"> </span><span class="kw">rlogis</span>(<span class="dv">1000</span>, <span class="dv">0</span>, <span class="dv">9</span>)
  <span class="cf">if</span> (i <span class="op">==</span><span class="st"> </span><span class="dv">4</span>) x &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="kw">rnorm</span>(<span class="dv">700</span>, <span class="dv">0</span>, <span class="dv">9</span>), <span class="kw">rnorm</span>(<span class="dv">300</span>, <span class="dv">35</span>, <span class="dv">4</span>))
  
  s &lt;-<span class="st"> </span><span class="kw">paste0</span>(<span class="st">&quot;kurtosis = &quot;</span>, <span class="kw">round</span>(<span class="kw">kurtosis</span>(x), <span class="dv">2</span>), <span class="st">&quot; skewness =&quot;</span>, <span class="kw">round</span>(<span class="kw">skewness</span>(x), <span class="dv">2</span>))
  tmp &lt;-<span class="st"> </span><span class="kw">rbind</span>(tmp, <span class="kw">data.frame</span>(<span class="dt">x =</span> x, <span class="dt">name =</span> s))
}

<span class="kw">ggplot</span>(tmp, <span class="kw">aes</span>(<span class="dt">x =</span> x)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_histogram</span>(<span class="dt">bins =</span> <span class="dv">50</span>) <span class="op">+</span><span class="st"> </span><span class="kw">facet_wrap</span>(.<span class="op">~</span>name)</code></pre></div>
<p><img src="Summarizing-data-01_files/figure-html/unnamed-chunk-5-1.png" width="480" /></p>
</div>
<div id="nominal-variables" class="section level3">
<h3><span class="header-section-number">6.1.4</span> Nominal variables</h3>
<p>Nominal variables are typically represented with the relative frequencies or probabilities, numerically or visually. Note that the methods discussed so far in this chapter apply to numerical variables (rational, interval and to some extent, ordinal) but not nominal variables, because the notions of location and distance (dispersion) do not exist in the nominal case. The only exception to this is the mode, which is the level of the nominal variable with the highest relative frequency or probability.</p>
<p>One summary that is often useful for summarizing the dispersion or the uncertainty associated with a nominal variable is <strong>entropy</strong>. Observe the following example:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">entropy &lt;-<span class="st"> </span><span class="cf">function</span>(x) {
  x &lt;-<span class="st"> </span>x[x <span class="op">!=</span><span class="st"> </span><span class="dv">0</span>]
  <span class="op">-</span><span class="kw">sum</span>(x <span class="op">*</span><span class="st"> </span><span class="kw">log2</span>(x))
}

<span class="kw">entropy</span>(<span class="kw">c</span>(<span class="fl">0.5</span>, <span class="fl">0.5</span>)) <span class="co"># fair coin</span></code></pre></div>
<pre><code>## [1] 1</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">entropy</span>(<span class="kw">c</span>(<span class="fl">0.8</span>, <span class="fl">0.2</span>)) <span class="co"># biased coin</span></code></pre></div>
<pre><code>## [1] 0.7219281</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">entropy</span>(<span class="kw">c</span>(<span class="fl">1.0</span>, <span class="fl">0.0</span>)) <span class="co"># coin with heads on both sides</span></code></pre></div>
<pre><code>## [1] 0</code></pre>
<p>A fair coin has exactly 1 bit of entropy - we receive 1 bit of information by observing the outcome of a flip. This is also the maximum achievable entropy for a binary variable. A biased coin has lower entropy - we receive less information. In the extreme case of a coin with heads on both sides, the entropy is 0 - the outcome of a flip brings no new information, as we already know it will be heads.</p>
<p>When we want to compare entropy across variables with different numbers of levels/categories, we can normalize it by dividing it with the maximum achieavable entropy. For example, observe a fair coin and a fair 6-sided die - in absolute terms, the 6-sided die has higher entropy due to having more possible values. However, relatively to the maximum achievable entropy, both represent maximally uncertain distributions:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">entropy</span>(<span class="kw">c</span>(<span class="fl">0.5</span>, <span class="fl">0.5</span>)) <span class="co"># fair coin</span></code></pre></div>
<pre><code>## [1] 1</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">entropy</span>(<span class="kw">rep</span>(<span class="dv">1</span><span class="op">/</span><span class="dv">6</span>, <span class="dv">6</span>)) <span class="co"># fair 6-sided die</span></code></pre></div>
<pre><code>## [1] 2.584963</code></pre>
<p>Note that entropy can easily be calculated for any discrete random variable. Entropy also has a continuous analogue - differential entropy, which we will not discuss here.</p>
</div>
<div id="testing-the-shape-of-a-distribution" class="section level3">
<h3><span class="header-section-number">6.1.5</span> Testing the shape of a distribution</h3>
<p>Often we want to check if the distribution that underlies our data has the shape of some hypothesized distribution (for example, the normal distribution) or if two samples come from the same distribution.</p>
<p>Here, we will present two of the most common methods used: the <strong>Kolmogorov-Smirnov</strong> test and the *<strong>Chi-squared goodness-of-fit</strong> test. Both of these are Null-hypothesis significance tests (NHST), so, before we proceed, be aware of two things. First, do not use NHST blindly, without a good understanding of their properties and how to interpret their results. And second, if you are more comfortable with thinking in terms of probabilities of hypotheses as opposed to significance and p-values, there always exist Bayesian alternatives to NHST.</p>
<p>The <strong>Kolmogorov-Smirnov test</strong> (KS) is a non-parametric test for testing the equality of two cumulative distribution functions (CDF). These can be two empirical CDFs or an empirical CDF and a theoretical CDF. The KS test statistic is the maximum distance between the two corresponding CDFs. That is, we compute the distribution of this statistic under the null-hypothesis that the CDFs are the same and then observe how extreme the maximum distance is on the sample.</p>
<p>To illustrate the KS test, we use it to test the normality of the underlying distributions for two samples - one from a logistic distribution, one from a standard normal distribution. And then to test if the two samples come from the same distribution:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">0</span>)
x1 &lt;-<span class="st"> </span><span class="kw">rlogis</span>(<span class="dv">80</span>, <span class="dv">0</span>, <span class="dv">1</span>)
x2 &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">80</span>, <span class="dv">0</span>, <span class="dv">1</span>)
<span class="kw">ks.test</span>(x1, <span class="dt">y =</span> <span class="st">&quot;pnorm&quot;</span>, <span class="dv">0</span>, <span class="dv">1</span>)</code></pre></div>
<pre><code>## 
##  One-sample Kolmogorov-Smirnov test
## 
## data:  x1
## D = 0.1575, p-value = 0.03362
## alternative hypothesis: two-sided</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ks.test</span>(x2, <span class="dt">y =</span> <span class="st">&quot;pnorm&quot;</span>, <span class="dv">0</span>, <span class="dv">1</span>)</code></pre></div>
<pre><code>## 
##  One-sample Kolmogorov-Smirnov test
## 
## data:  x2
## D = 0.070067, p-value = 0.801
## alternative hypothesis: two-sided</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ks.test</span>(x1, x2)</code></pre></div>
<pre><code>## 
##  Two-sample Kolmogorov-Smirnov test
## 
## data:  x1 and x2
## D = 0.175, p-value = 0.173
## alternative hypothesis: two-sided</code></pre>
<p>So, with a 5% risk (95% confidence), we would reject the null hypothesis that our sample x1 is from a standard normal distribution. On the other hand, we would not reject that our sample x1 is from a standard normal distribution. Finally, we would not reject the null hypothesis that x1 and x2 come from the same distribution. The only guarantee that comes with these results is that we will in the long run falsely reject a true null-hypothesis at most 5% of the time. It says very little about our overall performance, because we do not know the ratio of cases when the null-hypothesis will be true.</p>
<p>This example also illustrates the complexity of interpreting NHST results or rather all the tempting traps laid out for us - we might be tempted to conclude, based on the high p-value, that x2 does indeed come from a standard normal, but that then leads us to a weird predicament that we are willing to claim that x1 is not standard normal, x2 is standard normal, but we are less sure that x1 and x2 have different underlying distributions.</p>
<p>Note that typical implementations of the KS test assume that the underlying distributions are continuous and ties are therefore impossible. However, the KS test can be generalized to discrete and mixed distributions (see R package <a href="https://cran.r-project.org/web/packages/KSgeneral/index.html">KSgeneral</a>).</p>
<p>Differences in between distributions can also be assessed visually, through the <strong>QQ-plot</strong>, a plot that compares the quantiles of the two distributions. If the distributions have the same shape, their quantiles, plotted together, should lie on a line. The samples from the logistic distribution obviously deviate from the theoretical quantiles of a normal distribution:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">tmp &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">y =</span> x1)
<span class="kw">ggplot</span>(tmp, <span class="kw">aes</span>(<span class="dt">sample =</span> y)) <span class="op">+</span><span class="st"> </span><span class="kw">stat_qq</span>() <span class="op">+</span><span class="st"> </span><span class="kw">stat_qq_line</span>()</code></pre></div>
<p><img src="Summarizing-data-01_files/figure-html/unnamed-chunk-9-1.png" width="288" /></p>
<p>The <strong>Chi-squared goodness-of-fit</strong> (CHISQ) test is a non-parametric test for testing the equality of two categorical distributions. The CHISQ test can also be used on discrete or even continuous data, if there is a reasonable way of binning the data into a finite number of bins. The test statistic is based on a similar idea as the KS test statistic, but instead of observing just the maximum difference, we sum the squared difference between the relative frequency of the two distributions for a bin across all bins.</p>
<p>We illustrate the CHISQ test by testing the samples for a biased coin against a theoretical fair coin and the samples from an unbiased 6-sided die against a theoretical fair 6-sided die.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">0</span>)
x &lt;-<span class="st"> </span><span class="kw">table</span>(<span class="kw">sample</span>(<span class="dv">0</span><span class="op">:</span><span class="dv">1</span>, <span class="dv">30</span>, <span class="dt">rep =</span> T, <span class="dt">prob =</span> <span class="kw">c</span>(<span class="fl">0.7</span>, <span class="fl">0.3</span>)))
<span class="kw">chisq.test</span>(x, <span class="dt">p =</span> <span class="kw">c</span>(<span class="fl">0.5</span>, <span class="fl">0.5</span>)) <span class="co"># the default is to compare with uniform theoretical, but we make it explicit here</span></code></pre></div>
<pre><code>## 
##  Chi-squared test for given probabilities
## 
## data:  x
## X-squared = 3.3333, df = 1, p-value = 0.06789</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">x &lt;-<span class="st"> </span><span class="kw">table</span>(<span class="kw">sample</span>(<span class="dv">0</span><span class="op">:</span><span class="dv">1</span>, <span class="dv">40</span>, <span class="dt">rep =</span> T, <span class="dt">prob =</span> <span class="kw">c</span>(<span class="fl">0.7</span>, <span class="fl">0.3</span>)))
<span class="kw">chisq.test</span>(x, <span class="dt">p =</span> <span class="kw">c</span>(<span class="fl">0.5</span>, <span class="fl">0.5</span>))</code></pre></div>
<pre><code>## 
##  Chi-squared test for given probabilities
## 
## data:  x
## X-squared = 10, df = 1, p-value = 0.001565</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">x &lt;-<span class="st"> </span><span class="kw">table</span>(<span class="kw">sample</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">6</span>, <span class="dv">40</span>, <span class="dt">rep =</span> T))
<span class="kw">chisq.test</span>(x)</code></pre></div>
<pre><code>## 
##  Chi-squared test for given probabilities
## 
## data:  x
## X-squared = 5.3, df = 5, p-value = 0.3804</code></pre>
<p>So, with a 5% risk (95% confidence), we would reject the null hypothesis that our coin is fair, but only in the case with 40 samples. Because fair or close-to fair coins have high entropy, we typically require a lot of samples to distinguish between their underlying probabilities.</p>
<p>For a more real-world example, let us take the exit-poll data for the 2016 US Presidential election, broken down by gender, taken from <a href="https://edition.cnn.com/election/2016/results/exit-polls">here</a>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">n &lt;-<span class="st"> </span><span class="dv">24588</span>
male     &lt;-<span class="st"> </span><span class="kw">round</span>(<span class="fl">0.47</span> <span class="op">*</span><span class="st"> </span>n <span class="op">*</span><span class="st"> </span><span class="kw">c</span>(<span class="fl">0.41</span>, <span class="fl">0.52</span>, <span class="fl">0.07</span>)) <span class="co"># some rounding, but it should not affect results</span>
female   &lt;-<span class="st"> </span><span class="kw">round</span>(<span class="fl">0.53</span> <span class="op">*</span><span class="st"> </span>n <span class="op">*</span><span class="st"> </span><span class="kw">c</span>(<span class="fl">0.54</span>, <span class="fl">0.41</span>, <span class="fl">0.05</span>))
x        &lt;-<span class="st"> </span><span class="kw">rbind</span>(male, female)
<span class="kw">colnames</span>(x) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;Clinton&quot;</span>, <span class="st">&quot;Trump&quot;</span>, <span class="st">&quot;other/no answer&quot;</span>)
<span class="kw">print</span>(x)        <span class="co"># this is also known as a contingency table and the subsequent test as a contingency test</span></code></pre></div>
<pre><code>##        Clinton Trump other/no answer
## male      4738  6009             809
## female    7037  5343             652</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">chisq.test</span>(x)</code></pre></div>
<pre><code>## 
##  Pearson&#39;s Chi-squared test
## 
## data:  x
## X-squared = 417.71, df = 2, p-value &lt; 2.2e-16</code></pre>
<p>So, at any reasonable confidence level, we would reject the null-hypothesis and conclude that there is a difference in how men and women voted. In fact, we do not even need a test, because the difference is so obvious and the sample size so large. The differences between those who earned less or more than 100k$, however, appear smaller, so a test makes more sense:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">n &lt;-<span class="st"> </span><span class="dv">24588</span>
less100     &lt;-<span class="st"> </span><span class="kw">round</span>(<span class="fl">0.66</span> <span class="op">*</span><span class="st"> </span>n <span class="op">*</span><span class="st"> </span><span class="kw">c</span>(<span class="fl">0.49</span>, <span class="fl">0.45</span>, <span class="fl">0.06</span>)) <span class="co"># some rounding, but it should not affect results</span>
more100     &lt;-<span class="st"> </span><span class="kw">round</span>(<span class="fl">0.34</span> <span class="op">*</span><span class="st"> </span>n <span class="op">*</span><span class="st"> </span><span class="kw">c</span>(<span class="fl">0.47</span>, <span class="fl">0.47</span>, <span class="fl">0.06</span>))
x           &lt;-<span class="st"> </span><span class="kw">rbind</span>(less100, more100)
<span class="kw">colnames</span>(x) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;Clinton&quot;</span>, <span class="st">&quot;Trump&quot;</span>, <span class="st">&quot;other/no answer&quot;</span>)
<span class="kw">print</span>(x)</code></pre></div>
<pre><code>##         Clinton Trump other/no answer
## less100    7952  7303             974
## more100    3929  3929             502</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">chisq.test</span>(x)</code></pre></div>
<pre><code>## 
##  Pearson&#39;s Chi-squared test
## 
## data:  x
## X-squared = 9.3945, df = 2, p-value = 0.00912</code></pre>
<p>Still, we can at most typical levels of confidence reject the null-hypothesis and conclude that that there is a pattern here as well.</p>
</div>
</div>
<div id="descriptive-statistics-for-bivariate-distributions" class="section level2">
<h2><span class="header-section-number">6.2</span> Descriptive statistics for bivariate distributions</h2>
<p>When dealing with a joint distribution of two variables (that is, paired samples), the first thing we are typically interested in is dependence between the two variables or lack thereof. <strong>If two distributions are independent, we can summarize each separately without loss of information.</strong> If they are not, then the distributions carry information about eachother. The predictability of one variable from another is another (equivalent) way of looking at dependence of variables.</p>
<p>The most commonly used numerical summary of dependence is the <strong>Pearson correlation coefficient</strong> or Pearson’s <span class="math inline">\(\rho\)</span>. It summarizes the linear dependence, with <span class="math inline">\(\rho = 1\)</span> and <span class="math inline">\(\rho = - 1\)</span> indicating perfect colinearity (increasing or decreasing) and <span class="math inline">\(\rho = 0\)</span> indicating linear independence. As such, Pearson’s <span class="math inline">\(\rho\)</span> is directly related (the squared root) to the coefficient of determination <span class="math inline">\(R^2\)</span>, a goodness-of-fit measure for linear models and the proportion of variance in one explained by the other variable. An important consideration is that the statement that linear independence implies independence is not true in general (the converse implication is). One notable exception where this implication is true is the multivariate Normal distribution, where the dependence structure is expressed through linear dependence only.</p>
<p>Two of the most popular alternatives to Pearson’s <span class="math inline">\(\rho\)</span> are Spearman’s <span class="math inline">\(\rho\)</span> and Kendalls <span class="math inline">\(\tau\)</span>. The former measures the degree to which one variable can be expressed as monotonic function of the other. The latter measures the proportion of concordant pairs among all possible pairs (pairs (x1,y1) and (x2, y2), wher if x1 &gt; x2 then y1 &gt; y2). As such, they can capture non-linear dependence and is more appropriate for data with outliers or data where distance might have no meaning, such as ordinal data. Spearman’s <span class="math inline">\(\rho\)</span> and Kendall’s <span class="math inline">\(\tau\)</span> are more robust but do not have as clear an interpretation as Pearson’s <span class="math inline">\(\rho\)</span>. Kendall’s tau is also computationally more expensive.</p>
<p>Below are a few examples of bivariate samples that illustrate the strengths and limitations of the above correlation coefficients:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">0</span>)
<span class="kw">library</span>(MASS)
m &lt;-<span class="st"> </span><span class="dv">100</span>

dat &lt;-<span class="st"> </span><span class="ot">NULL</span>
<span class="co"># data 1</span>
sigma &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>), <span class="dt">nrow =</span> <span class="dv">2</span>)
x &lt;-<span class="st"> </span><span class="kw">mvrnorm</span>(m, <span class="dt">mu =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">0</span>), <span class="dt">Sigma =</span> sigma )
txt &lt;-<span class="st"> </span><span class="kw">sprintf</span>(<span class="st">&quot;Pearson&#39;s = %.2f</span><span class="ch">\n</span><span class="st">Spearman&#39;s = %.2f</span><span class="ch">\n</span><span class="st">Kendall&#39;s = %.2f&quot;</span>, <span class="kw">cor</span>(x)[<span class="dv">1</span>,<span class="dv">2</span>], <span class="kw">cor</span>(x, <span class="dt">method =</span> <span class="st">&quot;spearman&quot;</span>)[<span class="dv">1</span>,<span class="dv">2</span>], <span class="kw">cor</span>(x, <span class="dt">method =</span> <span class="st">&quot;kendall&quot;</span>)[<span class="dv">1</span>,<span class="dv">2</span>])
dat &lt;-<span class="st"> </span><span class="kw">rbind</span>(dat, <span class="kw">data.frame</span>(<span class="dt">x =</span> x[,<span class="dv">1</span>], <span class="dt">y =</span> x[,<span class="dv">2</span>], <span class="dt">example =</span> txt))

<span class="co"># data 2</span>
sigma &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">1</span>, <span class="op">-</span><span class="fl">0.5</span>, <span class="op">-</span><span class="fl">0.5</span>, <span class="dv">1</span>), <span class="dt">nrow =</span> <span class="dv">2</span>)
x &lt;-<span class="st"> </span><span class="kw">mvrnorm</span>(m, <span class="dt">mu =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">0</span>), <span class="dt">Sigma =</span> sigma )
txt &lt;-<span class="st"> </span><span class="kw">sprintf</span>(<span class="st">&quot;Pearson&#39;s = %.2f</span><span class="ch">\n</span><span class="st">Spearman&#39;s = %.2f</span><span class="ch">\n</span><span class="st">Kendall&#39;s = %.2f&quot;</span>, <span class="kw">cor</span>(x)[<span class="dv">1</span>,<span class="dv">2</span>], <span class="kw">cor</span>(x, <span class="dt">method =</span> <span class="st">&quot;spearman&quot;</span>)[<span class="dv">1</span>,<span class="dv">2</span>], <span class="kw">cor</span>(x, <span class="dt">method =</span> <span class="st">&quot;kendall&quot;</span>)[<span class="dv">1</span>,<span class="dv">2</span>])
dat &lt;-<span class="st"> </span><span class="kw">rbind</span>(dat, <span class="kw">data.frame</span>(<span class="dt">x =</span> x[,<span class="dv">1</span>], <span class="dt">y =</span> x[,<span class="dv">2</span>], <span class="dt">example =</span> txt))

<span class="co"># data 3</span>
sigma &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">1</span>, <span class="fl">0.95</span>, <span class="fl">0.95</span>, <span class="dv">1</span>), <span class="dt">nrow =</span> <span class="dv">2</span>)
x &lt;-<span class="st"> </span><span class="kw">mvrnorm</span>(m, <span class="dt">mu =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">0</span>), <span class="dt">Sigma =</span> sigma )
txt &lt;-<span class="st"> </span><span class="kw">sprintf</span>(<span class="st">&quot;Pearson&#39;s = %.2f</span><span class="ch">\n</span><span class="st">Spearman&#39;s = %.2f</span><span class="ch">\n</span><span class="st">Kendall&#39;s = %.2f&quot;</span>, <span class="kw">cor</span>(x)[<span class="dv">1</span>,<span class="dv">2</span>], <span class="kw">cor</span>(x, <span class="dt">method =</span> <span class="st">&quot;spearman&quot;</span>)[<span class="dv">1</span>,<span class="dv">2</span>], <span class="kw">cor</span>(x, <span class="dt">method =</span> <span class="st">&quot;kendall&quot;</span>)[<span class="dv">1</span>,<span class="dv">2</span>])
dat &lt;-<span class="st"> </span><span class="kw">rbind</span>(dat, <span class="kw">data.frame</span>(<span class="dt">x =</span> x[,<span class="dv">1</span>], <span class="dt">y =</span> x[,<span class="dv">2</span>], <span class="dt">example =</span> txt))

<span class="co"># data 4</span>
x &lt;-<span class="st"> </span><span class="kw">rnorm</span>(m, <span class="dv">0</span>, <span class="dv">1</span>)
y &lt;-<span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span><span class="kw">pnorm</span>(x) <span class="op">+</span><span class="st"> </span><span class="kw">rnorm</span>(m, <span class="dv">0</span>, <span class="fl">0.05</span>)
x &lt;-<span class="st"> </span><span class="kw">cbind</span>(x, y)
txt &lt;-<span class="st"> </span><span class="kw">sprintf</span>(<span class="st">&quot;Pearson&#39;s = %.2f</span><span class="ch">\n</span><span class="st">Spearman&#39;s = %.2f</span><span class="ch">\n</span><span class="st">Kendall&#39;s = %.2f&quot;</span>, <span class="kw">cor</span>(x)[<span class="dv">1</span>,<span class="dv">2</span>], <span class="kw">cor</span>(x, <span class="dt">method =</span> <span class="st">&quot;spearman&quot;</span>)[<span class="dv">1</span>,<span class="dv">2</span>], <span class="kw">cor</span>(x, <span class="dt">method =</span> <span class="st">&quot;kendall&quot;</span>)[<span class="dv">1</span>,<span class="dv">2</span>])
dat &lt;-<span class="st"> </span><span class="kw">rbind</span>(dat, <span class="kw">data.frame</span>(<span class="dt">x =</span> x[,<span class="dv">1</span>], <span class="dt">y =</span> x[,<span class="dv">2</span>], <span class="dt">example =</span> txt))

<span class="co"># data 5</span>
sigma1 &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">1</span>, <span class="op">-</span><span class="fl">0.5</span>, <span class="op">-</span><span class="fl">0.5</span>, <span class="dv">1</span>), <span class="dt">nrow =</span> <span class="dv">2</span>)
sigma2 &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="fl">0.1</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="fl">0.1</span>), <span class="dt">nrow =</span> <span class="dv">2</span>)
x &lt;-<span class="st"> </span><span class="kw">rbind</span>(<span class="kw">mvrnorm</span>(m, <span class="dt">mu =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">0</span>), <span class="dt">Sigma =</span> sigma ), <span class="kw">mvrnorm</span>(<span class="dv">50</span>, <span class="dt">mu =</span> <span class="kw">c</span>(<span class="dv">3</span>, <span class="op">-</span><span class="dv">2</span>), <span class="dt">Sigma =</span> sigma2 ))
txt &lt;-<span class="st"> </span><span class="kw">sprintf</span>(<span class="st">&quot;Pearson&#39;s = %.2f</span><span class="ch">\n</span><span class="st">Spearman&#39;s = %.2f</span><span class="ch">\n</span><span class="st">Kendall&#39;s = %.2f&quot;</span>, <span class="kw">cor</span>(x)[<span class="dv">1</span>,<span class="dv">2</span>], <span class="kw">cor</span>(x, <span class="dt">method =</span> <span class="st">&quot;spearman&quot;</span>)[<span class="dv">1</span>,<span class="dv">2</span>], <span class="kw">cor</span>(x, <span class="dt">method =</span> <span class="st">&quot;kendall&quot;</span>)[<span class="dv">1</span>,<span class="dv">2</span>])
dat &lt;-<span class="st"> </span><span class="kw">rbind</span>(dat, <span class="kw">data.frame</span>(<span class="dt">x =</span> x[,<span class="dv">1</span>], <span class="dt">y =</span> x[,<span class="dv">2</span>], <span class="dt">example =</span> txt))

<span class="co"># data 6</span>
z &lt;-<span class="st"> </span><span class="kw">runif</span>(m, <span class="dv">0</span>, <span class="dv">2</span><span class="op">*</span>pi)
x &lt;-<span class="st"> </span><span class="kw">cbind</span>(<span class="kw">cos</span>(z), <span class="kw">sin</span>(z))
txt &lt;-<span class="st"> </span><span class="kw">sprintf</span>(<span class="st">&quot;Pearson&#39;s = %.2f</span><span class="ch">\n</span><span class="st">Spearman&#39;s = %.2f</span><span class="ch">\n</span><span class="st">Kendall&#39;s = %.2f&quot;</span>, <span class="kw">cor</span>(x)[<span class="dv">1</span>,<span class="dv">2</span>], <span class="kw">cor</span>(x, <span class="dt">method =</span> <span class="st">&quot;spearman&quot;</span>)[<span class="dv">1</span>,<span class="dv">2</span>], <span class="kw">cor</span>(x, <span class="dt">method =</span> <span class="st">&quot;kendall&quot;</span>)[<span class="dv">1</span>,<span class="dv">2</span>])
dat &lt;-<span class="st"> </span><span class="kw">rbind</span>(dat, <span class="kw">data.frame</span>(<span class="dt">x =</span> x[,<span class="dv">1</span>], <span class="dt">y =</span> x[,<span class="dv">2</span>], <span class="dt">example =</span> txt))


<span class="kw">ggplot</span>(dat, <span class="kw">aes</span>(<span class="dt">x =</span> x, <span class="dt">y =</span> y)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_point</span>() <span class="op">+</span><span class="st"> </span><span class="kw">facet_wrap</span>(.<span class="op">~</span>example, <span class="dt">ncol =</span> <span class="dv">3</span>, <span class="dt">scales =</span> <span class="st">&quot;free&quot;</span>)</code></pre></div>
<p><img src="Summarizing-data-01_files/figure-html/unnamed-chunk-13-1.png" width="576" /></p>
<p>Like similar questions about other parameters of interest, the question <em>Is <insert number here> a strong correlation?</em> is a practical question. Unless the correlation is 0 (no correlation) or 1/-1 (perfectly correlated, can’t be more correlated than this), the meaning of the magnitude of correlation depends on the practical setting and its interpretation depends on some reference level. Even a very low correlation, such as 0.001 (if we are reasonably sure that it is around 0.001) can be practically meaningful. For example, if it is correlation between the even and odd numbers generated by a uniform random number generator (RNG), that would be more than enough correlation to stop using this RNG.</p>
</div>
<div id="further-reading-and-references-2" class="section level2">
<h2><span class="header-section-number">6.3</span> Further reading and references</h2>
<ul>
<li>For a more comprehensive treatment of the most commonly used summarization techniques see: Holcomb, Z. C. (2016). Fundamentals of descriptive statistics. Routledge.</li>
<li>More on the practice of summarization techniques and hypothesis testing: Bruce, P., &amp; Bruce, A. (2017). Practical statistics for data scientists: 50 essential concepts. &quot; O’Reilly Media, Inc.“. <em>(Chapters 1 and 3)</em></li>
</ul>
</div>
<div id="learning-outcomes-2" class="section level2">
<h2><span class="header-section-number">6.4</span> Learning outcomes</h2>
<p>Data science students should work towards obtaining the knowledge and the skills that enable them to:</p>
<ul>
<li>Reproduce the techniques demonstrated in this chapter using their language/tool of choice.</li>
<li>Recognize when a type of summary is appropriate and when it is not.</li>
<li>Apply data summarization techiques to obtain insights from data.</li>
<li>Once introduced to the bootstrap and other estimation techniques, to be able to combine descriptive statistics with a quantification of uncertainty, such as confidence intervals.</li>
</ul>
</div>
<div id="practice-problems-2" class="section level2">
<h2><span class="header-section-number">6.5</span> Practice problems</h2>
<ol style="list-style-type: decimal">
<li>Download the <a href="https://www.kaggle.com/ajinkyablaze/football-manager-data/downloads/football-manager-data.zip/2">Football Manager Players</a> dataset or use a similarly rich dataset with numerical, binary and categorical variables. With Python or R demonstrate the application and interpretation of results for each of the summarization techniques from this chapter.</li>
<li>Find one or more real-world examples (data sets) where a standard summary of univariate or bivariate data fails. That is, where important information is lost in the summary.</li>
</ol>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="dynamic-reports-and-reproducibility.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="summarizing-data-visualization.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
