[
["index.html", "Introduction to data science Preface", " Introduction to data science Slavko Žitnik, Tomaž Curk, Erik Štrumbelj 2019-10-21 Preface These are the course notes for the Introduction to data science course of the Data Science Master’s at University of Ljubljana, Faculty of computer and information science. The authors would like to thank the people who contributed to these notes with their reviews, comments and suggestions: Janez Demšar, Jure Demšar, Dejan Lavbič, Matjaž Pančur, Gregor Pirš, Marko Robnik Šikonja. "],
["what-is-data-science-anyway.html", "What is data science anyway? 0.1 The role of this course", " What is data science anyway? There is no generally accepted definition of what data science is. We would probably all agree that it is a relatively new term and that it is field that requires a mix of IT, analytics and soft skills. Our view is that data science is synonymous with empirical science and that this is a result of two major changes. First, the evolution of empirical science to a level that now requires a much broader skill set. And second, the evolution of business, industry and other non-scientific areas to a level where more and more processes require treatment at the level of empirical science. Examples of empirical science go back as far as recorded history itself. From censuses in ancient Egypt that were used to scale the pyramid labor force to weather forecasting to profit from olive oil futures in ancient Greece. Historically, even as little as 100 years ago, empirical science was much more difficult than it is today (not that it is not still very difficult if done properly). Data were recorded and manipulated by hand. If one was not an expert mathematician and well-equipped with logarithm and quantile tables, it was near-impossible to do even the most basic statistical analyses. And to spread the results, one would write their report and plot their graphs by hand. Over time, this skill set has evolved, with computers playing the leading role. Pens were replaced by typsetting software. Graphing paper was replaced by visualization packages. And logarithm tables were replaced by computer code. Indeed, a lot of the brain-intensive mathematics gave way to computationally intensive numerical approaches, both out of convenience and out of necessity. On the other hand, progress also brought new challenges and raised standards in data management, software development, reproducible research and visualization. The names that describe people with this skill set have also evolved. From scientist, empirical scientist, researcher, and statistician to more cool names, such as quantitative analyst, quant, predictive analyst, data miner, machine learning expert and now data scientist. Tomorrow, data science will probably be replaced by another even catchier term. But the core skills will remain the same: analytical thinking, attention to detail, communication and domain-specific knowledge. Mathematics and computation. Science. Like the skill set itself the demand for it has also always been present. And growing. To the point where we now live in an increasingly data-driven world: all empirical sciences, finance, marketing, sports training, politics, HR and medicine, to name just a few, rely on data. Today, the practical reality is that there is a big shortage of people with such skills or even just some of them. This is great opportunity for those considering to become data scientists, but also a challenge for educational institutions on how to best prepare students for a career in this data-driven world. 0.1 The role of this course This course aims at breadth not depth. The goal is to familiarize ourselves with all key ingredients of data science from a practitioners perspective. We will learn enough methods, techniques and tools to deal with the vast majority of data science scenarios we encounter in real life. However, only with practice and through more academic courses will we be able to develop a fundamental understanding of some of these methods and refine their use to a professional standard. That is, this course focuses on doing. With time, study, practice and through mistakes, we will learn how to do better. So what do we think are the key ingredients of a data science practitioner? The first and most important is computer programming. Data science requires data manipulation and computation that can not be done without a computer. Working with data also represents most of the workload in a typical data science scenario. And if we are not able to tell the computer exactly what to do, we can’t be a professional data scientist. So, it should come as no surprise that Chapter 1 is dedicated to the Python programming language. Python and R are the two most common languages in data science and we advocate that a professional data scientist should learn both. Python is the more versatile of the two and thus more suitable for an introductory course such as this one. Once we are able to do something, it becomes very important that we do it in a transparent and easily repeatable way. We put a lot of emphasis on reproducibility and the tools that help us achieve it. Why? First, because we need to be true to the word science in data science. If it is not clear how something was done, if some steps cannot be reproduced, then we cannot have confidence in the results and that is not science! And second, from a purely practical perspective, if our work is easily reproducible, we ourselves will have a much easier job of repeating our analyses on different data or with slightly modified methodology. What time we invest into reproducibility we will recieve 10-fold savings later on. We will cover source code control , Docker for portability and Jupyter notebooks and other dynamic reporting tools . In practice, data are in most cases not in tabular format and stored in a csv file. It is important to know how to retrieve and deal with less structured data and how to store data in a more systematic and efficent way. We will cover web scraping and relational databases and SQL . While most of the work is in getting, preprocessing and storing the data, most of the added value in data science is in extracting valuable information from data. First, we need to learn how to efficiently and effectively summarize data and do exploratory data analysis. In most cases, these techniques combined with some method of quantifying uncertainty in our summaries will be all that is required to successfully complete our analysis! This broad topic will be covered in three chapters: basic numerical summaries for univariate and bivariate data , standard visualization techiques and techniques for multivariate data . Predictive modelling, the family of tasks that includes a vast majority of the analytic tasks we encounter in practice. And finally, dealing with missing data - a very important topic and a topic that requires us to combine all the summarization, exploratory and predictive techniques. "],
["python-introduction-chapter.html", "Chapter 1 Python programming language 1.1 Basic characteristics 1.2 Why Python? 1.3 Setting up the environment 1.4 Installing dependencies 1.5 Jupyter notebooks 1.6 Short introduction to Python 1.7 Python ecosystem for Data Science 1.8 Further reading and references 1.9 Learning outcomes 1.10 Practice problems", " Chapter 1 Python programming language 1.1 Basic characteristics Python is an interpreted, dynamically typed, object-oriented programming language. Advantages: Simple. Easy to learn. Extensive packages. Cross-platform. Free and open source. Large user base. Disadvantages: Being interpreted results in slower execution. Dynamic typing can lead to errors that only show up at runtime. Higher memory consumption, less suitable for memory-intensive tasks. 1.2 Why Python? The Python language is one of the two most popular languages for data science (the other being R). The two main reasons are: The advantages of Python fit the typical data science workflow and its disadvantages are not that deterimental to the data science workflow. Python has a large ecosystem of packages, libraries and tools for data science, some of which are discussed later in this chapter. Often libraries and software developed in other languages provide Python API or bindings. The typical data science workflow consists of acquiring and manipulating data and applying standard machine learning or statistical methods. In essence, the data flows through different methods. The emphasis is on obtaining results - extracting meaningful information from data. The advantages of Python are extremely beneficial to such a workflow: Being simple, easy to learn and free, it is accessible to a larger user base, including users with little or no prior programming experience. Being an interpreted language (and straightforward piecewise execution through read-eval-print loop shells or REPL) makes Python very flexible - multiple alternatives can be explored and quick decisions made on how to procede, depending on intermediary results. The disadvantages of Python are of minor consequence: The data science workflow is not time-critical - even an order-of-magnitude slowdown typically makes little difference. Memory and time consumption can be significantly decreased by using popular libraries such as numpy and scipy. Code maintainability is less important - data science scripts are often short and discarded after use. Specific platforms development or enterprise-level applications development are also not part of the typical data science workflow. Data science products used in such applications are normally rewritten as final models in a production-ready code. 1.3 Setting up the environment Before using Python, we need to select and install a desired Python distribution. We can choose to install a pure Python distribution or an Anaconda Python distribution. Some advantages of using an Anaconda distribution are: Anaconda makes it easy for the user to install the Python version of choice. Anaconda will also resolve issues with administrator privileges if a user does not have administrative rights for his system. Anaconda Accelerate can provide the user with high performance computing and several other components. Anaconda removes bottlenecks involved in installing the right packages while taking into considerations their compatibility with various other packages as might be encountered while using the traditional package manager (pip). There is no risk of breaking required system libraries. There are also many open source packages available for Anaconda, which are not within the pip repository. We encourage you to use the Anaconda Python distribution. 1.3.1 Anaconda distribution installation Install the desired Anaconda Python distribution. A useful way of managing multiple Python projects is to use Conda environments. An environment enables you to use a specific version of Python along with specific dependencies completely separately on a single system. To create and use an environment with a name itds, issue the following command: $ conda create -n itds $ conda activate itds At the beginning of a line in the console you can see currently active environment. To run Python within this evironment, issue the python command in the console. You should see something similar to the following: (itds)$ python Python 3.6.8 |Anaconda, Inc.| (default, Dec 29 2018, 19:04:46) [GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)] on darwin Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information. &gt;&gt;&gt; To exit the Python interpreter, enter and commit the command exit(); To exit the environment, use conda deactivate. To show existing environments and their locations, issue conda info --envs. 1.3.2 Pure Python distribution installation You can also install a pure Python distributiondirectly to your system from the official Python Downloads web page. To run Python, issue the python command in the console (there may be more interpreters installed on your machine and Python 3.5 might be run also using python3.5). After running the command, you should see something similar to the following: $ python Python 3.5.2 (v3.5.2:4def2a2901a5, Jun 26 2016, 10:47:25) [GCC 4.2.1 (Apple Inc. build 5666) (dot 3)] on darwin Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information. &gt;&gt;&gt; When using the distribution directly, all packages and settings are changed system-wide, which may lead to problems when maintaining multiple Python projects (e.g. problems in having installed different versions of a specific library). Similar to an Anaconda distribution, one can use virtualenv virtual environments. First, we need to install virtualenv via pip: $ pip3 install virtualenv $ virtualenv --version 16.6.1 To set up a virtual environment for a project, first create a project folder and set up a new environment in that folder. The latter will create Python executables within that folder and a copy of pip library that is used to install libraries local to the environment (parameter p is optional). $ cd itds_project $ virtualenv -p /usr/local/bin/python2 itds To activate the environment, run the script venv/bin/activate from the project folder and use project specific Python: $ source itds/bin/activate (itds)$ python Python 2.7.14 (default, Mar 9 2018, 23:57:12) [GCC 4.2.1 Compatible Apple LLVM 9.0.0 (clang-900.0.39.2)] on darwin Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information. &gt;&gt;&gt; After you finish working on your project, deactivate the current virtual environment: (itds)$ deactivate $ 1.4 Installing dependencies An Anaconda distribution provides its own package repository and Anaconda packages can be installed as follows: $ conda install nltk If a package is not available in the official repository, it may be available from some private or community-led channels, for example conda-forge: $ conda install -c conda-forge pyspark Many useful libraries are available online, mostly in the Python Package Index (PyPI) repository which can also be used directly in a conda environment. Well-built libraries consist of installation instructions and a setup.py file to install them. The common location for installing libraries is the folder %PYTHON_BASE%/lib, %PYTHON_BASE%/site-packages or %PYTHON_BASE%/Lib. Packages can be installed using the pip command. For example, to install the NLTK library, we issue the following command: $ pip install nltk In some cases packages will be prebuilt for a specific OS, because the installation of its dependencies can be tedious. In such cases, wheel packages can be provided and installed using the following command: $ pip install YOUR_DOWNLOADED_PACKAGE.whl 1.5 Jupyter notebooks The Jupyter Notebook is an open-source web application that allows you to create and share documents that contain live code, equations, visualizations and narrative text. Uses include: data cleaning and transformation, numerical simulation, statistical modeling, data visualization, machine learning, and much more. To add support for Anaconda environments to Jupyter, issue the command below. This will add Conda extensions to Jupyter. The feature will be installed for currently active environment. When running Jupyter, you will notice a Conda tab in the file browser, which will enable the listing and selection of existing Anaconda environments, overview of the installed packages in the environment, installing new packages from the available package list, checking for updates packages and updating packages in the environment. $ conda install nb_conda 1.5.1 Running a Jupyter notebook Prior to running a Jupyter notebook we first need to start a Jupyter web server by issuing a command: $ jupyter notebook By default the server is started in the current folder and accessible via the web interface: http://localhost:8888. The root web page shows a file browser where we can create a new Jupyter notebook or open an existing one: To get a similar view, save the provided Jupyter notebook into the folder where you run jupyter notebook command. Click the filename of the notebook and it will open in a new window: As you notice, a notebook consists of linearly ordered blocks of types Markdown, Code, Raw NBConvert and Heading. If you double click a text block you can edit it using Markdown syntax. To evaluate the block again and show rendered view, click the Run button or press Alt+Enter. The same holds for other types of blocks. Sometimes we just want to view or execute a notebook online. There exist multiple services offering this functionality, for example https://gke.mybinder.org/. 1.6 Short introduction to Python All the examples presented in this section are also provided in a Jupyter notebook. Basics Let’s first say hello: print(&quot;Hello Data science!&quot;) ## Hello Data science! Now we know how to print something to the output. Before we dive into details, let’s first check how we can easily output literal values and variables. Let’s have a variable name and age with specific values and form a form a final string to show to a user. name = &quot;Mark&quot; age = 42 # Basic old-style string concatenation print(&quot;Hi, I am &quot; + name + &quot; and I am &quot; + str(age) + &quot; years old.&quot;) # %-formatting print(&quot;Hi, I am %s and I am %d years old.&quot; % (name, age)) # Cleaner syntax using format function print(&quot;Hi, I am {} and I am {} years old.&quot;.format(name, age)) # Format function with extended features of parameter naming and output formatting print(&quot;Hi, I am {name} and I am {age:3d} years old.&quot;.format(age=age, name=name)) # Same features as format function with evaluations directly within curly braces print(f&quot;Hi, I am {name} and I am {age:3d} years old&quot;) # Another example f = 91 print(f&quot;{f:.2f} Fahrenheit is {(f - 32) * 5 / 9:.2f} Celsius&quot;) ## Hi, I am Mark and I am 42 years old. ## Hi, I am Mark and I am 42 years old. ## Hi, I am Mark and I am 42 years old. ## Hi, I am Mark and I am 42 years old. ## Hi, I am Mark and I am 42 years old ## 91.00 Fahrenheit is 32.78 Celsius The last example seems the most readable and we will use it from now on. It is also called f-string and you can read more about its features in the official documentation about literal string interpolation. 1.6.0.1 Variables and types Python defines whole numbers (int, long) and real numbers (float). Whole numbers are integers (\\(\\pm 2^{31}\\) or \\(\\pm 2^{63}\\)) and long numbers, limited by the memory size. Long is a number with a trailing L added at the end (Python 2, in Python 3, long is merged with int). Complex numbers are also supported using a trailing j to the imaginary part. Bool type is based on integer - value of 1 is True and value of 0 is False. For the boolean expressions, integer value of 1 will be interpreted as True and all other values as False. String values are represented as sequence of characters within \" or '. A constant None is defined to represent the nonexistence of a value. a = 2864 print(f&quot;Type of a is {type(a)}&quot;) b = 18+64j print(f&quot;Type of c is {type(b)}&quot;) c = False print(f&quot;Type of d is {type(c)}&quot;) d = &quot;I&#39;m loving it!&quot; print(f&quot;Type of e is {type(d)}&quot;) e = None print(f&quot;Type of f is {type(e)}&quot;) ## Type of a is &lt;class &#39;int&#39;&gt; ## Type of c is &lt;class &#39;complex&#39;&gt; ## Type of d is &lt;class &#39;bool&#39;&gt; ## Type of e is &lt;class &#39;str&#39;&gt; ## Type of f is &lt;class &#39;NoneType&#39;&gt; Numbers and basic operators Basic data manipulations: a = 3 b = 2.5 c = a + b print(f&quot;Addition: {c}&quot;) c = a * b print(f&quot;Multiplication: {c}&quot;) c = a / b print(f&quot;Division: {c}&quot;) c = True + 5 print(f&quot;Addition to Boolean: {c}&quot;) c = &quot;5&quot; * 5 print(f&quot;String multiplication: {c}&quot;) ## Addition: 5.5 ## Multiplication: 7.5 ## Division: 1.2 ## Addition to Boolean: 6 ## String multiplication: 55555 1.6.0.2 Strings, concatenation and formatting Basic strings manipulations: a = &quot;Data science&quot; b = &#39;a multi-disciplinary field&#39; # we can use double or single quotes c = a + &quot; &quot; + b print(f&quot;Concatenated string: &#39;{c}&#39;&quot;) first = c[:4] last = c[-5:] print(f&quot;First word: &#39;{first}&#39; and last word: &#39;{last}&#39;.&quot;) firstLower = first.lower() lastUpper = last.upper() print( (f&quot;First word lowercased: &#39;{firstLower}&#39;&quot; f&quot;and last word uppercased: &#39;{lastUpper}&#39;.&quot;) ) management = c.replace(&quot;science&quot;, &quot;management&quot;) print(f&quot;Substring replacement: &#39;{management}&#39;&quot;) ## Concatenated string: &#39;Data science a multi-disciplinary field&#39; ## First word: &#39;Data&#39; and last word: &#39;field&#39;. ## First word lowercased: &#39;data&#39;and last word uppercased: &#39;FIELD&#39;. ## Substring replacement: &#39;Data management a multi-disciplinary field&#39; Explore more about strings in the official Python 3 documentation for strings. # string package import string print(f&quot;Punctuation symbols: &#39;{string.punctuation}&#39;&quot;) ## Punctuation symbols: &#39;!&quot;#$%&amp;&#39;()*+,-./:;&lt;=&gt;?@[\\]^_`{|}~&#39; String manipulation is essential for parsing and providing machine readable outputs when needed. Below we show additional two examples of how to set print length of a variable using f-string method. For more sophisticated examples, see https://pyformat.info/ - f-string method is the new equivalent of the format method. number = 6/.7 text = &quot;dyslexia&quot; format0 = f&quot;Number: {round(number*100)/100.0}, Text: {&#39; &#39;*(15-len(text))} {text}&quot; print(format0) format1 = f&quot;Number: {number:5.2f}, Text: {text:&gt;15}&quot; print(format1) ## Number: 8.57, Text: dyslexia ## Number: 8.57, Text: dyslexia 1.6.0.3 Data stuctures: Lists, Tuples, Sets, Dictionaries Below we create some of the data structures available in Python. Explore more of their functionality in the official Python documentation. l = [1, 2, 3, &quot;a&quot;, 10] # List t = (1, 2, 3, &quot;a&quot;, 10) # Tuple (immutable) s = {&quot;a&quot;, &quot;b&quot;, &quot;c&quot;} # Set dict = { &quot;title&quot;: &quot;Introduction to Data Science&quot;, &quot;year&quot;: 1, &quot;semester&quot;: &quot;fall&quot;, &quot;classroom&quot;: &quot;P02&quot; } dict[&quot;classroom&quot;] = &quot;P03&quot; We often use inline functions to map, filter or calculate values on an iterable data structure. For example, to apply a function to all values (map), filter out unnecessary values or use all values in a calculation: # Python 3 import for reduce (not needed for Python 2) from functools import reduce l = [6, 8, 22, 4, 12] doubled = map(lambda x: x*2, l) print(f&quot;Doubled: {doubled}&quot;) filtered = filter(lambda x: x &gt; 10, l) print(f&quot;Filtered: {filtered}&quot;) sum = reduce(lambda x, y: x+y, l) print(f&quot;Sum value: {sum}&quot;) ## Doubled: &lt;map object at 0x103dd2f50&gt; ## Filtered: &lt;filter object at 0x103dd2cd0&gt; ## Sum value: 52 Functions filter, reduce and map create a generator since Python 3. Generators enable declaring functions that behaves like an iterator. To print all the values of the aobve generator objects, we need to evaluate them, for example transform them into a list. print(f&quot;Doubled: {list(doubled)}&quot;) print(f&quot;Filtered: {list(filtered)}&quot;) ## Doubled: [12, 16, 44, 8, 24] ## Filtered: [22, 12] In comparison to generators we propose to use list comprehension on iterable types, which is more readable and also faster. l = [6, 8, 22, 4, 12] newList = [x**2 for x in l if x &gt;= 5 and x &lt;= 10] print(f&quot;Squared values between 5 and 10: {newList}&quot;) Squared values between 5 and 10: [36, 64] Sometimes we would like to generate a repeated sequence of select (i.e. slice) only specific values from a string, bytes, tuple, list or range. Slice object represents the indices specified by range(start, stop, step): l = list(range(10)) print(f&quot;List: {l}&quot;) slice_indexes = slice(2,8,2) print(f&quot;Sliced list: {l[slice_indexes]}&quot;) List: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] Sliced list: [2, 4, 6] 1.6.1 Flow control Many operations can be written inline or using multiple lines. Let’s check how to use if statements and loops. a = 2 if a &gt; 1: print(&#39;a is greater than 1&#39;) elif a == 1: print(&#39;a is equal to 1&#39;) else: print(&#39;a is less than 1&#39;) ## a is greater than 1 # Inline if statement a = 2 print(&#39;a is greater than 1&#39; if a &gt; 1 else &#39;a is lower or equal to 2&#39;) ## a is greater than 1 Loops: for i in range(4, 6): print(i) ## 4 ## 5 people_list = [&#39;Ann&#39;, &#39;Bob&#39;, &#39;Charles&#39;] for person in people_list: print(person) ## Ann ## Bob ## Charles i = 1 while i &lt;= 3: print(i) i = i + 1 ## 1 ## 2 ## 3 1.6.2 Functions We organize (encapsulate) our code into logical units, so we can reuse them and reduce the amount of duplicate, boilerplate code. Below write the function greetMe that takes one argument (name) as input and prints some string. The function’s code will be executed when we call the function. def greetMe(name): print(f&quot;Hello my friend {name}!&quot;) greetMe(&quot;Janez&quot;) ## Hello my friend Janez! Sometimes our functions will have many arguments, some of which might be optional or have a default value. In the example below we add a argument with a default value. If there are multiple optional arguments we can set their values by naming them. def greet(name, title = &quot;Mr.&quot;): print(f&quot;Hello {title} {name}!&quot;) greet(&quot;Janez&quot;) greet(&quot;Mojca&quot;, &quot;Mrs.&quot;) greet(&quot;Mojca&quot;, title = &quot;Mrs.&quot;) ## Hello Mr. Janez! ## Hello Mrs. Mojca! ## Hello Mrs. Mojca! A function can also call itself and return a value. def sumUpTo(value): if value &gt; 0: return value + sumUpTo(value-1) else: return 0 print(f&quot;Sum of all positive integers up to 50 is: {sumUpTo(50)}&quot;) ## Sum of all positive integers up to 50 is: 1275 Functions can also return multiple values in a tuple. Tuple can be then also automatically unpacked into separate values: def calculateHealth(height_cm, weight_kg, age, gender = &#39;male&#39;): # Body mass index bmi = weight_kg/(height_cm/100)**2 # Basal metabolic rate (Revised Harris-Benedict Equation) bmr = 0 # Ideal body weight ibw = 0 if gender == &#39;male&#39;: bmr = 13.397*weight_kg + 4.799*height_cm - 5.677*age + 88.362 ibw = 50 + (0.91 * (height_cm - 152.4)) else: bmr = 9.247*weight_kg + 3.098*height_cm - 4.330*age + 447.593 ibw = 45.5 + (0.91 * (height_cm - 152.4)) return (bmi, bmr, ibw) janez_health = calculateHealth(184, 79, 42) (bmi, bmr, ibw) = calculateHealth(178, 66, 35, &#39;female&#39;) print(f&quot;Janez:\\n\\tBMI: {janez_health[0]}\\n\\tBMR: {janez_health[1]}\\n\\tIBW: {janez_health[2]}&quot;) print(f&quot;Mojca:\\n\\tBMI: {bmi}\\n\\tBMR: {bmr}\\n\\tIBW: {ibw}&quot;) Janez: BMI: 23.334120982986768 BMR: 1791.3070000000002 IBW: 78.756 Mojca: BMI: 20.830703194041156 BMR: 1457.7890000000002 IBW: 68.79599999999999 Python encapsulates variables within functions - they are not accessible outside the function. When we want variables to be accessible globally, we can use the global keyword. This can result in some difficulties to predict behaviour and interactions, so use with caution! def playWithVariables(value1, list1): global globVal globVal = 3 value1 = 10 list1.append(22) print(f&quot;Within function: {value1} and {list1} and {globVal}&quot;) value1 = 5 list1 = [3, 6, 9] print(f&quot;Before function: {value1} and {list1}&quot;) playWithVariables(value1, list1) print(f&quot;After function: {value1} and {list1} and {globVal}&quot;) ## Before function: 5 and [3, 6, 9] ## Within function: 10 and [3, 6, 9, 22] and 3 ## After function: 5 and [3, 6, 9, 22] and 3 In some cases we can also define functions that accept an arbitrary number of unnamed (args) and/or named (kwargs) arguments. def paramsWriter(*args, **kwargs): print(f&quot;Non-named arguments: &#39;{args}&#39;\\nNamed arguments: &#39;{kwargs}&#39;&quot;) paramsWriter(1, &quot;a&quot;, [1,5,6], studentIds = [234, 451, 842], maxScore = 100.0) ## Non-named arguments: &#39;(1, &#39;a&#39;, [1, 5, 6])&#39; ## Named arguments: &#39;{&#39;studentIds&#39;: [234, 451, 842], &#39;maxScore&#39;: 100.0}&#39; When naming functions, classes, objects, packages, etc. we need to be careful not to overwrite existing objects. Bugs such as this one can be difficult to find: def greeter(): print(&quot;Hello to everyone!&quot;) greeter() greeter = &quot;Mr. John Hopkins&quot; greeter() # Error - greeter is now string value Hello to everyone! Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt; TypeError: &#39;str&#39; object is not callable 1.6.3 Classes and objects Python is also object-oriented and therefore enables us to encapsulate data and functionality into classes. Instances of classes are objects. The class below consists of one class variable, one class method, three object methods and two object variables. All class-based variables are accessible using the class name directly without having instantiated the object. Object-based methods are accessible only through an instantiated object and can also directly modify object properties (self.*). All object methods accept self as an implicit parameter, which points to the current object. Below we show an example of object declaration and its use. A detailed explanation can be found in the official Python documentation. class Classroom: class_counter = 0 def num_classes(): return Classroom.class_counter def __init__(self, name): Classroom.class_counter += 1 self.name = &quot;Best of Data Science class &quot; + name self.students = [] def enroll(self, student): self.students.append(student) def __str__(self): return f&quot;Class: &#39;{self.name}&#39;, students: &#39;{self.students}&#39;&quot; class1 = Classroom(&quot;best of millenials&quot;) class2 = Classroom(&quot;old sports&quot;) print(f&quot;Num classes: {Classroom.class_counter}&quot;) print(f&quot;Num classes: {Classroom.num_classes()}&quot;) class2.enroll(&quot;Slavko Žitnik&quot;) class2.enroll(&quot;Erik Štrumbelj&quot;) class2.enroll(&quot;Tomaž Curk&quot;) print(class2) ## Num classes: 2 ## Num classes: 2 ## Class: &#39;Best of Data Science class old sports&#39;, students: &#39;Slavko Žitnik, Erik Štrumbelj, Tomaž Curk&#39; 1.6.4 Reading and writing files Sometimes the data is stored in a well-formatted file which we must read. Also, it is expected that we output results into a file, so let’s check how to read and write to files using Python. To open a file, call function open(FILENAME, MODE). The function returns handle to work with a file. We must select the mode type which can be one of: r - reading only, w - writing to a file (previous content will be deleted), x - creating a new file (function fails of a file already exists), a - appending to a file, t - opening a file in text mode, b - opening a file in binary mode and + - opening a file for reading and writind (updating). Writing content to a file: file = open(&quot;itds.txt&quot;,&quot;w+&quot;) for i in range(10): file.write(f&quot;This is line {i}.\\r\\n&quot;) file.close() Reading content (output omitted): #Open the file and read the contents file = open(&quot;itds.txt&quot;, &quot;r&quot;) # Reading the whole content into a variable contents = file.read() # Reading file into a list of lines file.seek(0) #Move to the beginning as we already readt the whole file lines = file.readlines() # Reading line by line file.seek(0) for line in file: print(line) It is also useful to read and write JSON data directly. Python can automatically write and read JSON files for built-in objects: import json json_obj = {&#39;name&#39;: &#39;Janez&#39;, &#39;age&#39;: &#39;Novak&#39;, &#39;marks&#39;: [{&#39;OPB&#39;: 8, &#39;ITDS&#39;: 6, &#39;WIER&#39;: 10}]} # Write json to a file json.dump(json_obj, open(&#39;json_output.json&#39;, &#39;w&#39;)) # Read json to a variable from file janez = json.load(open(&#39;json_output.json&#39;, &#39;r&#39;)) print(janez) ## {&#39;name&#39;: &#39;Janez&#39;, &#39;age&#39;: &#39;Novak&#39;, &#39;marks&#39;: [{&#39;OPB&#39;: 8, &#39;ITDS&#39;: 6, &#39;WIER&#39;: 10}]} 1.6.5 Python IDE’s and code editors An IDE (Integrated Development Environment) is software dedicated to software development. As the name implies, IDEs integrate several tools specifically designed for software development. These tools usually include: An editor designed to handle code with features such as syntax highlighting and auto-completion. Build, execution, and debugging tools. Some form of source control support. IDEs are generally large and take time to download and install. You may also need advanced knowledge to use them properly. In contrast, a dedicated code editor can be as simple as a text editor with syntax highlighting and code formatting capabilities. Most good code editors can execute code and control a debugger. The very best ones interact with source control systems as well. Compared to an IDE, a good dedicated code editor is usually smaller and quicker, but often less feature rich. Below we list some popular Python IDEs/code editors that are available for major operating systems (Windows, Linux and Mac OS): IDLE - the default code editor that installs together with the Python distribution. It includes a Python shell window (interactive interpreter), auto-completion, syntax highlighting, smart indentation and a basic integrated debugger. We do not recommend it for larger projects. Sublime Text, Atom, Visual Studio Code - highly customizable code editors with rich features of an IDE. They support installation of additional extensions and also provide intelligent code completion, linting for potential errors, debugging, unit testing and so on. These editors are becoming quite popular among Python and web developers. PyCharm - an IDE for professional developers. There are two versions available: a free Community version and a paid Professional version which is free for students only. PyCharm provides all major features of a good IDE: code completion, code inspections, error-highlighting and fixes, debugging, version control system and code refactoring, etc.. 1.7 Python ecosystem for Data Science The Python ecosystem of libraries, frameworks and tools is large and ever-growing. Python can be used for web scraping, machine learning, general scientific computing and many other computing and scripting uses. We list some of the most widely used libraries in the field of data science. NumPy - NumPy is the fundamental package for scientific computing with Python. The tools that we commonly use are: a powerful N-dimensional array object, sophisticated (broadcasting) functions, tools for integrating C/C++ and Fortran code and most importantly, linear algebra, Fourier transform and random number capabilities. NumPy can also be used as an efficient multi-dimensional container of generic data, where arbitrary data-types can be defined. Matplotlib - A 2D plotting library which produces publication quality figures in a variety of hardcopy formats and interactive environments across platforms. Matplotlib can be used in Python scripts, Python and IPython shells, Jupyter notebooks and web application servers. We can generate plots, histograms, power spectra, bar charts, errorcharts, scatterplots, etc., with just a few lines of code. It provides a MATLAB-like interface, particularly when combined with IPython. It gives users full control of line styles, font properties, axes properties, etc. SciPy - “Sigh Pie”\" is a Python-based ecosystem of open-source software for mathematics, science and engineering. In particular, it connects the following core packages: NumPy, SciPy library (fundamentals for scientific computing), Matplotlib, IPython, Sympy (symbolic mathematics) and Pandas. scikit-learn - a Machine Learning (ML) library in Python. It provides simple and efficient tools for data analysis. It offers a framework and many algorithms for classification, regression, clustering, dimensionality reduction, model selection and preprocessing. The library is open source and build on NumPy, SciPy and matplotlib. Pandas - Python Data Analysis Library (pandas) is an open source, BSD-licensed library providing high-performance, easy-to-use data structures and data analysis tools for the Python programming language. Some of the library highlights are a fast and efficient DataFrame object for data manipulation with integrated indexing, tools for reading and writing data between in-memory data structures and different formats, intelligent data alignment and integrated handling of missing data, flexible reshaping and pivoting of data sets, high performance merging and joining of data sets, an intuitive way of working with high-dimensional data in a lower-dimensional data structure, time series-functionality. TensorFlow - TensorFlow is an end-to-end open source platform for machine learning in the field of deep learning. It has a comprehensive and flexible ecosystem of tools, libraries and community resources that lets researchers push the state-of-the-art in ML and developers to easily build and deploy ML powered applications. Keras - Compared with TensorFlow, Keras is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, CNTK or Theano backends. It was developed with a focus on enabling fast experimentation, being able to go from idea to result with the least possible delay. Keras allows for easy and fast prototyping (through user friendliness, modularity and extensibility), supports both convolutional networks and recurrent networks, as well as combinations of the two and runs seamlessly on CPU and GPU. 1.8 Further reading and references Here is a list of more comprehensive guides to Python programming: Fluent Python: Clear, Concise, and Effective Programming Official Python Tutorials - Tutorial accompanying official Python documentation. See this resource for latest features. Beginning Python by Magnus Hetland - The book is written for beginners but last chapters are useful also for more experienced programmers. Non-Programmer’s Tutorial for Python - a well organized Wikibook. Think Python - similar book to the previous one, but a bit more intermediate. [Wikibook: Programming Python] - similar to the above two with more advanced topics. How to Think Like a Computer Scientist - well organized sections for self-paced learning. Python za programerje by Janez Demšar - Well known Slovene book, written by the professor at our Faculty. You can find electronic versions online, but printed version is accessible to buy at publishing house FRI (at the entrance). 1.9 Learning outcomes Data science students should work towards obtaining the knowledge and the skills that enable them to: Use the Python programming language for simple programming tasks, data manipulation and file I/0. Identify the Python IDE(s) that best fit their requirements. Find suitable Python packages for the task at hand and use them. Recognize when Python is and when it is not a suitable language to use. 1.10 Practice problems Install Anaconda Python, run the provided Jupyter notebook within a new conda environment and then export all the installed dependencies into an environment.yml file (see reference). Check the file, remove not needed data (location, library versions, libraries in lower dependency trees), create a new environment based on the exported file and run the notebook again (it should work without the need to install additiona packages manually). Try different Python IDEs and form a personal opinion of their advantages and disadvantaged. Download, explore and run some scripts from the Keras examples repository. Download the CMU Seminar Announcements dataset and uncompress it. The dataset consists of multiple files, whereas each file represents a seminar anncouncement. Write a program that reads every file and tries to extract the speaker, title, start-time, end-time and the location of the seminar. Help yourself with regular expressions and libraries mentioned above. Store all the extracted data into a Pandas data frame and lastly, export all the data into a single CSV file. In addition, compute your success rate in extraction of specific fields based on the manual tags from the documents. "],
["reproducibility.html", "Chapter 2 Reproducible research 2.1 Scientific inquiry 2.2 From principles to practice 2.3 Reproducibility tools in R 2.4 Reproducibility tools in Python 2.5 Further reading and references 2.6 Learning outcomes 2.7 Practice problems", " Chapter 2 Reproducible research 2.1 Scientific inquiry The term data science contains the word science. It is our view that data science is more or less synonimous with with modern empirical science and as such it should adhere to the standards for scientific inquiry. Ideally, scientific inquiry would always be objective and correct. However, these are unattainable ideals! Science will always be subjective at least in the sense of what we investigate and how we choose to investigate it. And scientists like all people, even with the best intentions and practices, make mistakes. Instead, the realistic standard is lower, albeit still surprisingly difficult to attain - research should be reproducible. We will borrow two definitions from the American Statistical Association: Reproducibility: A study is reproducible if you can take the original data and the computer code used to analyze the data and reproduce all of the numerical findings from the study. Replicability: This is the act of repeating an entire study, independently of the original investigator without the use of original data (but generally using the same methods). In other words, reproducibilty refers to providing a complete and unambiguous description of the entire process from the original raw data to the final numerical results. Reproducibility does not concern itself with the correctness of the results or the process. Our research might even have major bugs in the code or completely flawed methodology, but if it is reproducible, those bugs and those flaws are transparent and will be found by those trying to reproduce or replicate our results. That is, as long as our research is reproducible, scientific critique and progress can be made! However, if our research is not reproducible it is of much less value! Reproducibility will be our minimum standard for scientific enquiry and is also a prerequisite for replicability. Replicability, on the other hand, does concern itself with the correctness of the original research. If we independently repeat a study, we expect to get the same results or at least within some margin of error due to sampling and other sources of variability. Sometimes we might replicate our own study to validate it, but in most cases, replication of our studies is done by others. 2.2 From principles to practice Perfect reproducibility is also unattainable, because not all factors can be feasibly accounted for (time, location, hardware used, etc.). Instead, we will strive for a high but practically feasible level of reproducibility. We will also introduce the practical dimension of how easy to reproduce research is. For example, if we precisely describe all data manipulations in a text document, the data manipulation part of our research will be reproducible. However, it will be much more difficult to reproduce than, for example, if we provided well-documented code that did the same manipulations. The modern standard for reproduciblity from data to results is to use computer code for everything. This includes even minimal changes to the original data (for example, renaming a column), generating and including plots and tables and even single-number data in the text of our report or paper. Computer code is, by definition, clear and unambiguous - there is no room for misinterpretation of what has been done. And, if we have everything in code, we completely avoid the possiblity of introducing errors or undetectable changes through manual manipulation of the data or copying the results Such code together with the original data and a precise description of the platform and software used (relevant hardware, operating system, packages, including versions) constitutes as highly and easily reproducible research. This process can be made much easier with the use of standard tools. In particular, integrated analyses and reports, such as Jupyter notebooks and R markdown and notebooks, which we introduce later in the chapter. 2.2.1 Preregistration - the future standard As we already discussed above, reproduciblity is concerned only with the completeness and unambiguity of the description of the study. All studies should be reproducible, but we expect that some studies will not replicate. Some due to the inherent risk of statistical procedures, some due to researchers’ mistakes and some due to accidental or deliberate manipulation of the researcher’s degrees of freedom or data dredging. These refer to the choices that we make during a study (some of which should strictly be made before the study). For example: the researcher chooses the method or statistical test that confirms the desired hypothesis, the researcher selects the hypotheses only after seeing the data, the researcher splits the data into subgroups that lead to desired results, and many others. In order to deal with this problem and enforce a higher standard of research (and subsequently fewer studies that do not replicate) preregistration is very slowly but surely becoming the norm for professional research. In essence, preregistration is the practice of registering all relevant aspects of a scientific study (how the data will be collected, hypotheses, methods used, etc.) before we begin with the study itself. Preregistration is made with some central authority, such as the Open science framework and many reputable scientific journals already promote and support preregistered research. 2.3 Reproducibility tools in R In this section we introduce four different approaches to integrating R code and text to produce dynamic reports, fully reproducible reports that can automatically be updated if the data change. All four approaches are integrated into RStudio. R Markdown R Markdown (knitr) is the most popular approach to dynamic reporting with R. It combines the lightweight Markdown language with R code chunks. R Markdown documents can be rendered as html, pdf, word, etc. Here is an example of a R markdown file (.Rmd extension) that features R code, inline R code and an image: --- output: html_document --- ```{r setup, include=FALSE} knitr::opts_chunk$set(echo = FALSE) ``` ## Data ```{r} set.seed(0) x &lt;- rnorm(100) y &lt;- rnorm(100) ``` ```{r fig.width=4, fig.height=4, fig.cap = paste(&quot;Relationship between high jump and long jump performance (n =&quot;, length(x), &quot;).&quot;)} plot(x,y) ``` We can also use inline R core: there are `r length(y)` observations in our data. R Markdown is very easy to use and can produce nice documents. However, it lacks the control of more specialized typesetting languages such as LaTeX. A good starting point for learning R Markdown is the free online book R Markdown: The Definitive Guide. Note that the textbook you are reading was also produced with R Markdown and the Bookdown extension. R Markdown can also be used with chunks from other programming languages, for example, Python, with the help of the reticulate package. R Notebook A R Notebook is identical to a R Markdown document, except that we replace the output to html_notebook: --- output: html_notebook --- ```{r setup, include=FALSE} knitr::opts_chunk$set(echo = FALSE) ``` ## Data ```{r} set.seed(0) x &lt;- rnorm(100) y &lt;- rnorm(100) ``` ```{r fig.width=4, fig.height=4, fig.cap = paste(&quot;Relationship between high jump and long jump performance (n =&quot;, length(x), &quot;).&quot;)} plot(x,y) ``` We can also use inline R core: there are `r length(y)` observations in our data. R Notebooks are more interactive - code chunk results are shown inline and are rendered in real time. That is, we can render individual chunks instead of the entire document. Sweave When we require more control over the typsetting, for example, when we are trying to produce an elegant pdf document or adhere to the typsetting standards of an academic journal, we might prefer Sweave. Sweave is an integration of LaTeX and R code - in essence, a LaTeX document with R code chunks. R code is executed when we compile the Sweave document with Sweave and LaTeX. Here is an example of a Sweave file (.Rnw extension) that demonstrates the use of R code, inline R code and an image: \\documentclass{article} \\begin{document} \\section*{Data} &lt;&lt;&gt;&gt;= set.seed(0) x &lt;- rnorm(100) y &lt;- rnorm(100) @ Look at the nice plot in Figure \\ref{fig1}. \\begin{figure}[htb] &lt;&lt;fig=T&gt;&gt;= plot(x,y) @ \\caption{Relationship between high jump and long jump performance (n = \\Sexpr{length(x)}).}\\label{fig1} \\end{figure} \\end{document} Sweave gives us all the typsetting control of LaTeX at the expense of more code and having to compile the LaTeX as well. A good starting point for Sweave is this compact Sweave manual. Shiny web apps and dashboards Shiny is an R package that makes it easy to build interactive applications with R. Shiny can be used to produce standalone web apps and dashboards or it can be embedded into R Markdown documents. Shiny is useful for rapid development of user-friendly interfaces to interactive analyses of our data. These Shiny tutorials are a good starting point for further study. 2.4 Reproducibility tools in Python Python offers a variety of similar tools to R. It provides Jupyter notebooks with multiple options to create reproducible Python code with dynamic visualizations, presentations and exports to different formats. We already presented installation of Jupyter notebooks in Chapter 1. The NBConvert tool allows you to convert a Jupyter .ipynb notebook document file into another format. The nbconvert documentation contains a complete description of this tool’s capabilities. It allows for: presentation of information in familiar formats, such as PDF. publishing of research using LaTeX and opens the door for embedding notebooks in papers. collaboration with others who may not use the notebook in their work. sharing contents with many people via the web using HTML. Overall, notebook conversion and the nbconvert tool give scientists and researchers the flexibility to deliver information in a timely way across different formats. Primarily, the nbconvert tool allows you to convert a Jupyter .ipynb notebook document file into another static format including HTML, LaTeX, PDF, Markdown, reStructuredText, and more. The nbconvert tool can also add productivity to your workflow when used to execute notebooks programmatically. If used as a Python library (import nbconvert), nbconvert adds notebook conversion within a project. For example, nbconvert is used to implement the “Download as” feature within the Jupyter Notebook web application. When used as a command line tool (jupyter nbconvert), users can conveniently convert just one or a batch of notebook files to another format. Jupyter Dashboards A jupyter dashboard is a jupyter notebook with the dashboards layout extension, where we can arrange your notebook outputs (text, plots, widgets, etc.) in a grid - or report-like layouts. The layouts are saved in the notebook document. When in a jupyter notebook, we should see the dashboard view extension to control the dashboard settings: Jupyter notebooks and dashboards are also widely used, support a large number of programming languages and they provide: Documentation and literate programming by combining rich-text narrative concepts &amp; machine-readable code. The notebeook itself is a data-structure with metadata that can be easily read and parsed. Exploration &amp; development: Intermediate steps are saved in a clean, well documented format Communication/Collaboration: sharing research with peers, collaborators, reviewers, public Publishing: It is simple and quick switch between the development &amp; publishing stage In a combination with jupyter widgets, dynamic dashboards can be created easily. Below we show an example of a general dashboard, where a user selects a location and weather data is extracted from the Web. Along with the data a location of a Weather station is also visualized. import ipywidgets as widgets from ipywidgets import interact import urllib.request from lxml import etree import folium w = widgets.Dropdown( options=[(&quot;Ljubljana&quot;, &quot;LJUBL-ANA_BEZIGRAD&quot;), (&quot;Celje&quot;, &quot;CELJE&quot;), (&quot;Kočevje&quot;, &quot;KOCEVJE&quot;), (&quot;Novo mesto&quot;, &quot;NOVO-MES&quot;)], value=&quot;LJUBL-ANA_BEZIGRAD&quot;, description=&#39;Location:&#39;, ) @interact(location=w) def f(location): url = f&quot;http://meteo.arso.gov.si/uploads/probase/www/observ/surface/text/en/observation_{location}_latest.xml&quot; xml = etree.XML(urllib.request.urlopen(url).read()) lat = etree.XPath(&quot;//domain_lat/text()&quot;)(xml)[0] lon= etree.XPath(&quot;//domain_lon/text()&quot;)(xml)[0] station_name = etree.XPath(&quot;//domain_title/text()&quot;)(xml)[0] last_updated = etree.XPath(&quot;//tsUpdated/text()&quot;)(xml)[0] weather = etree.XPath(&quot;//nn_shortText/text()&quot;)(xml)[0] temp = etree.XPath(&quot;//t/text()&quot;)(xml)[0] humidity = etree.XPath(&quot;//rh/text()&quot;)(xml)[0] print(f&quot;Location: {station_name}\\nLast updated: {last_updated}\\n\\n&quot;) print(f&quot;Weather info: {weather}\\nTemperature: {temp}°C\\nHumidity: {humidity}%&quot;) slovenia_map = folium.Map(location=[46, 14.5], zoom_start=9, tiles=&#39;Stamen Terrain&#39;) folium.Marker([46.0658,14.5172], icon=folium.Icon(color=&#39;green&#39;, icon=&#39;ok-sign&#39;)).add_to(slovenia_map) display(slovenia_map) There exist many possible options for graph visualizations (e.g. Plotly) and integrations of common libraries such as matplotlib. from matplotlib import pyplot as plt import numpy as np import math %matplotlib inline def showGraph(scale): x = np.arange(0, math.pi*scale, 0.05) y = np.sin(x) fig = plt.figure() ax = fig.add_axes([0, 0, 1, 1]) ax.plot(x, y) ax.set_xlabel(&quot;Angle&quot;) ax.set_ylabel(&quot;sine&quot;) interact(showGraph, scale = widgets.IntSlider(value=4, description=&#39;Scale&#39;, max=10, min=1)) 2.5 Further reading and references Science is in a replication and reproducibility crisis. In some fields more than half of published studes fail to replicate and most studies are not reproducible: Baker, M. (2016). 1,500 scientists lift the lid on reproducibility. Nature News, 533(7604), 452. Preregistration will eventually become the standard for publication of empirical research: Nosek, B. A., Ebersole, C. R., DeHaven, A. C., &amp; Mellor, D. T. (2018). The preregistration revolution. Proceedings of the National Academy of Sciences, 115(11), 2600-2606. 2.6 Learning outcomes Data science students should work towards obtaining the knowledge and the skills that enable them to: Produce reproducible data science analyses. Identify reproducibility flaws in own and other peoples’ research. Distinguish between reproducibility and replication. 2.7 Practice problems Create a short R Markdown, Sweave and/or Jupyter Notebook that loads some data, draws a plot, prints a table and contains some text with an example of inline use of code. Consider the following experiment of drawing \\(m &gt; 1\\) samples from a distribution (standard normal or uniform) and taking their average, repeating this process for \\(n &gt; 1\\) times and plotting the histogram of these \\(n\\) averages: n &lt;- 5000 m &lt;- 10 set.seed(0) dist &lt;- &quot;normal&quot; all &lt;- c() for (i in 1:n) { if (dist == &quot;normal&quot;) { x &lt;- rnorm(m) } else { x &lt;- runif(m) } all &lt;- c(mean(x), all) } hist(all, breaks = sqrt(n)) Create a Shiny App/Dashboard and/or Jupyter Dashboard that draws such a histogram and allows you to interactively change \\(n\\), \\(m\\) and which distribution is used (support standard normal, uniform and a third distribution of choice). Use the Dashboard to visually explore if the sample average tend to a normal distribution as \\(m\\) grows larger. "],
["scraping.html", "Chapter 3 Web scraping 3.1 Introduction to Web data extraction 3.2 Web wrapper 3.3 Further reading and references 3.4 Learning outcomes 3.5 Practice problems", " Chapter 3 Web scraping Today there are more than 3.7 billion Internet users, which almost 50% of the entire population (Internet World Stats 2017). Taking into account all the existing Internet-enabled devices, we can estimate that approximatelly 30 billion devices are connected to the network (Deitel, Deitel, and Deitel 2011). In this chapter we focus on Web data extraction (Web scraping) - automatically extracting data from websites and storing it in a structured format. However, the broader field of Wb information extraction also requires the knowledge of natural language processing techniques such as text pre-processing, information extraction (entity extraction, relationship extraction, coreference resolution), sentiment analysis, text categorization/classification and language models. Students that are interested in learning more about these techniques are encouraged to enrol in a Natural language processing course. For introduction to natural language techniques please see the Further reading references (Liu 2011) (Chapter 11), (Christopher, Prabhakar, and Hinrich 2008) (Chapters 12, 13, 15-17, 20), (Aggarwal and Zhai 2012) (Chapters 1-8, 12-14) or other specialized books on natural language processing. 3.1 Introduction to Web data extraction Web data extraction systems (Ferrara et al. 2014) are a broad class of software applications that focus on extracting data from Web sources. A Web data extraction system usually interacts with a Web source and extracts data stored in it: for instance, if the source is an HTML Web page, the extracted content could consist of elements in the page as well as the full-text of the page itself. Eventually, extracted data might be post-processed, converted to the most convenient structured format and stored for further usage. The design and implementation of Web data extraction systems has been discussed from different perspectives and it leverages on scientific methods from different disciplines including machine learning, logic and natural language processing. Web data extraction systems find extensive use in a wide range of applications including the analysis of text-based documents available to a company (like e-mails, support forums, technical and legal documentation, and so on), Business and Competitive Intelligence, crawling of Social Web platforms, Bioinformatics and so on. The importance of Web data extraction systems depends on the fact that a large (and steadily growing) amount of data is continuously produced, shared and consumed online: Web data extraction systems allow us to efficiently collect these data with limited human effort. The availability and analysis of collected data is vital to understanding complex social, scientific and economic phenomena which generate the data. For example, collecting digital traces produced by users of social Web platforms such as Facebook, YouTube or Flickr is the key step to understand, model and predict human behavior. In commercial fields, the Web provides a wealth of public information. A company can probe the Web to acquire and analyze information about the activity of its competitors. This process is known as Competitive Intelligence and it is crucial to quickly identify the opportunities provided by the market, to anticipate the decisions of the competitors as well as to learn from their faults and successes. The challenges of Web data extraction In its most general formulation, the problem of extracting data from the Web is hard because it is constrained by several requirements. The key challenges we can encounter in the design of a Web Data Extraction system can be summarized as follows: Web data extraction techniques implemented in a Web data extraction system often require human input. The first challenge consists of providing a high degree of automation by reducing human efforts as much as possible. Human feedback, however, may play an important role in raising the level of accuracy achieved by a Web data extraction system. A related challenge is to identify a reasonable tradeoff between building highly automated Web data extraction procedures and the requirement of achieving accurate performance. Web data extraction techniques should be able to process large volumes of data in relatively short time. This requirement is particularly stringent in the field of Business and Competitive Intelligence because a company needs to perform timely analysis of market conditions. Applications in the field of Social Web or, more in general, those dealing with personal data must provide solid privacy guarantees. Therefore, potential (even if unintentional) attempts to violate user privacy should be timely and adequately identified and counteracted. Approaches relying on machine learning often require a significantly large training set of manually labeled Web pages. In general, the task of labeling pages is time-expensive and error-prone and, therefore, in many cases we cannot assume the existence of labeled pages. Often, a Web data extraction tool has to routinely extract data from a Web Data source which can evolve over time. Web sources are continuously evolving and structural changes happen with no forewarning, and are thus unpredictable. In real-world scenarios such systems have to be maintained, because they are likely to eventually stop working correctly, in particular if they lack the flexibility to detect and face structural modifications of the target Web sources. The first attempts to extract data from the Web date back to the early 90s. In the early days, this discipline borrowed approaches and techniques from Information Extraction (IE) literature. In particular, two classes of strategies emerged: learning techniques and knowledge engineering techniques – also called learning-based and rule-based approaches, respectively. These classes share a common rationale: the former was thought to develop systems that require human expertise to define rules (for example, regular expressions) to successfully accomplish the data extraction. These approaches require specific domain expertise: users that design and implement the rules and train the system must have programming experience and a good knowledge of the domain in which the data extraction system will operate; they will also have the ability to envisage potential usage scenarios and tasks assigned to the system. On the other hand, also some approaches of the latter class involve strong familiarity with both the requirements and the functions of the platform, so human input is essential. 3.2 Web wrapper In the literature, any procedure that aims at extracting structured data from unstructured or semi-structured data sources is usually referred to as a wrapper. In the context of Web data extraction we provide the following definition: Web wrapper is a procedure, that might implement one or many different classes of algorithms, which seeks and finds data required by a human user, extracting them from unstructured (or semi-structured) Web sources, and transforming them into structured data, merging and unifying this information for further processing, in a semi-automated or fully automated way. Web wrappers are characterized by a 3-step life-cycle: Wrapper generation: the wrapper is defined and implemented according to one or more selected techniques. Wrapper execution: the wrapper runs and extracts data continuously. Wrapper maintenance: the structure of data sources may change and the wrapper should be adapted accordingly. The first two steps of a wrapper life-cycle, generation and execution, might be implemented manually, for example, by defining and executing regular expressions over the HTML documents. Alternatively, which is the aim of Web data extraction systems, wrappers might be defined and executed by using an inductive approach – a process commonly known as wrapper induction (Kushmerick, Weld, and Doorenbos 1997). Web wrapper induction is challenging because it requires high-level automation strategies. Induction methods try to uncover structures from an HTML document to form a robust wrapper. There exist also hybrid approaches that make it possible for users to generate and run wrappers semi-automatically by means of visual interfaces. The third and final step of a wrapper’s life-cycle is maintenance: Web pages change their structure continuously and without forewarning. This might affect the correct functioning of a Web wrapper, whose definition is usually tightly bound to the structure of the Web pages adopted. Defining automated strategies for wrapper maintenance is vital to the correctness of extracted data and the robustness of Web data extraction platforms. Wrapper maintenance is especially important for long-term extractions. For the purposes of acquiring data in a typical data-science project, wrappers are mostly run once or over a short period of time. In such cases it is less likely that a Web site would change, so automation is not a priority and is typicaly not implemented. 3.2.1 HTML DOM HTML is the predominant language for implementing Web pages and it is largely supported by the World Wide Web consortium. HTML pages can be regarded as a form of semi-structured data (even if less structured than other sources like XML documents) in which information follows a nested structure. HTML structure can be exploided in the design of suitable wrappers. While semi-structured information is often also available in non-HTML formats (for example, e-mail messages, code, system logs), extracting information of this type is the subject of more general information extraction and not the focus of this chapter. The backbone of a Web page is a Hypertext Markup Language (HTML) document which consists of HTML tags. According to the Document Object Model (DOM), every HTML tag is an object. Nested tags are called children of the enclosing tag. Generally we first parse a web page into a DOM tree representation. Then we specify extraction patterns as paths from the root of the DOM tree to the node containing the values to extract. Special languages such as XPath or XQuery support searching and querying elements of a DOM tree. Below we show a simple HTML Web page and its representation as a HTML DOM tree. &lt;html&gt; &lt;head&gt; &lt;title&gt;My Title&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;a href=&quot;&quot;&gt;My link&lt;/a&gt; &lt;h1&gt;My header&lt;/h1&gt; &lt;/body&gt; &lt;/html&gt; Figure 3.1: DOM tree representation of the above HTML Web page. The HTML DOM is an Object Model for HTML and it defines: HTML elements as objects. Properties for all HTML elements. Methods for all HTML elements. Events for all HTML elements. Interactive Web pages mostly consist of Javascript cod which is executed directly in the Web browser. The HTML DOM is also an API (Programming Interface) for JavaScript that allows it to dynamically: Add/change/remove HTML elements/attributes/events. Add/change/remove CSS styles. React to HTML events. 3.2.2 XPath One of the main advantage of the adoption of the Document Object Model for Web Content Extraction is the possibility of exploiting the tools for XML languages (and HTML is to all effects a dialect of the XML). In particular, the XML Path Language (XPath) provides with a powerful syntax to succintly address specific elements of an XML document. XPath was also defined by the World Wide Web consortium, as was DOM. Below we provide an XML example to explain how XPath can be used to address elements of a Web page. &lt;persons&gt; &lt;person&gt; &lt;name&gt;John&lt;/name&gt; &lt;height unit=”cm”&gt;191&lt;/height&gt; &lt;sport&gt;Running&lt;/sport&gt; &lt;sport&gt;Cycling&lt;/sport&gt; &lt;/person&gt; &lt;person&gt; &lt;name&gt;Mandy&lt;/name&gt; &lt;height&gt;140&lt;/height&gt; &lt;sport&gt;Swimming&lt;/sport&gt; &lt;/person&gt; &lt;/persons&gt; There exist two possible ways to use XPath: (A) to identify a single element in the document tree, or (B) to address multiple occurrences of elements. We show some XPath queries against the above XML document: /persons/person/name - Extract all elements in the provided path. The result is &lt;name&gt;John&lt;/name&gt; and &lt;name&gt;Mandy&lt;/name&gt;. /persons/person/name/text() - Get contents of all the elements that match the provided path. The result is John and Mandy. //person/height[@unit=\"cm\"]/text() - Extract contents of all height objects that have set attribute unit to cm and have their parent element named as person. Result is 191. //sport - Extract all elements that appear at any level of XML and are named sport. The result is &lt;sport&gt;Running&lt;/sport&gt;, &lt;sport&gt;Cycling&lt;/sport&gt; and &lt;sport&gt;Swimming&lt;/sport&gt;. //person[name=\"Mandy\"]/sport/text() - Extract contents of sport objects that are nested directly under person objects which contain a name object with value Mandy. The result is Swimming. The major weakness of XPath is its lack of flexibility: each XPath expression is strictly related to the structure of the Web page. However, this limitation has been partially mitigated with the introduction of relative path expressions. In general, even minor changes to the structure of a Web page might corrupt the correct functioning of an XPath expression defined on a previous version of the page. Still, due to the ease of use, many Web extraction libraries support the use of XPath in to addition of their own extraction API. Let us consider Web pages generated by a script (e.g. the information about a book in an e-commerce Web site). Now assume that the script undergoes some changes. We can expect that the tree structure of the HTML page generated by that script will change accordingly. To keep the Web data extraction process functional, we should update the expression every time the underlying page generation model changes. Such an operation would require a substantial human effort and would therefore be very costly. To this end, the concept of wrapper robustness was introduced. From the perspective of XPath the idea is to find, among all the XPath expressions capable of extracting the same information from a Web page, the one that is least influenced by potential changes in the structure of the page and such an expression identifies the more robust wrapper. In general, to make the entire Web data extraction process robust, we need tools allowing ufor measuring document similarity. Such a task can be accomplished by detecting structural variations in the DOM trees associated with the documents. Techniques called tree-matching strategies are a good candidate to detect similarities between two trees (Tai 1979). The discussion of these techniques is outside the scope of this chapter. 3.2.3 Modern Web sites and JS frameworks Modern Web sites still use HTML to render their data within Web browsers. Below we show an example of a Web page (left side) and extracted content from it (right side). When all pages follow the same structure, we can easily extract all the data. e-Commerce Web page - product description Extracted data in a structured format (JSON) However, Web sites are becoming more dynamic - loading data in the background, not refreshing the entire view, etc. These functionalities require dynamic code to be executed direcly on the client, that is, inside the Web browser. The main language that can be interpreted by a Web browser is Javascript. Although best practices instruct the programmers to support non-Javascript browsers, there are many Web pages that malfunction if the browser does not support Javascript. With the advent of Single page application (SPA) Web sites the content does not even partially load as the whole Web page is driven by the Javascript. Popular frameworks that enable SPA development are for example Angular, Vue.js or React. Below we show some examples of rendering Web pages when a browser runs with Javascript enabled or disabled: Javascript enabled Javascript disabled When we develop a Web extraction system we should first review how the target Web site is built and which frontend technologies are used Then we can also more efficiently use a library to implement a final Web wrapper to extract the desired data. When we need to execute Javascript, our extraction library needs to implement headless browser functionality. This functionality runs a hidden browser to construct a final HTML content which is then used for further manipulation. Libraries that support such functionality are for example: Selenium phantomJS HTMLUnit Running a headless browser and executing Javascript can be time consuming and prone to errors. So, whenever we can, we should rather use just an HTML parsing library. There exist many of such libraries and we mention just a few: HTML Cleaner HTML Parser JSoup (Java) or BeautifulSoup (Python) Jaunt API HTTP Client 3.2.4 Crawling, resources and policies Crawling is a process of automatic navigation through Web pages within defined Web sites. When we deal with continuous retrieval of content from a large amount of Web pages, there are many aspects we need to take care of. For example, we need to track which pages were already visited, we need to decide how to handle HTTP redirects or HTTP error codes in case of a delayed retry, we must follow the rules written in robots.txt for each domain or should follow general crawling ethics so that we not send too many request to a specific server and we need to track changes on Web pages to identify approximate change-rate, etc. Generally, a crawler architecture will consist of the following components (Figure 3.2): HTTP downloader and renderer: To retrieve and render a web page. Data extractor: Minimal functionalities to extract images and hyperlinks. Duplicate detector: To detect already parsed pages. URL frontier: A list of URLs waiting to be parsed. Datastore: To store the data and additional metadata used by the crawler. Figure 3.2: Web crawler architecture. As we already mentioned before, we need to understand all the specifics how Web pages are built and generated. To make sure that we correctly gather all the needed content placed into the DOM by Javascript, we should use headless browsers. Google’s crawler Googlebot implements this as a two-step process or expects to retrieve dynamically built web page from an HTTP server. A session on crawling modern web sites built using JS frameworks, link parsing and image indexing was a part of Google IO 2018 and we recommend it to get an impression of problems that we can encounter: A crawler needs to identify links, which can be encoded in several different ways. They can be explicilty given within href attributes, as onclick Javascript events (e.g. location.href or document.location), etc. Similarly, images can be generated in different formats, shown dynamically, etc. While in some circumstances it might be better to implement a custom crawler, a crawler package or suite is typically a better choice. Here are two examples: Apache Nutch, Scrapy. The Web consists also of other files that web pages point to, for example PDF files, Word/OpenOffice documents, Excel spreadsheets, presentations, etc. They may also include some relevant information. We recommend the following tools for parsing such content: Apache Tika toolkit detects and extracts metadata and text from over a thousand different file types (such as PPT, XLS, and PDF). All of these file types can be parsed through a single interface, making Tika useful for search engine indexing, content analysis, translation, and much more. Apache Poi focus on manipulating various file formats based upon the Office Open XML standards (OOXML) and Microsoft’s OLE 2 Compound Document format (OLE2). In short, you can read and write MS Excel files using Java. Apache PDFBox library is an open source Java tool for working with PDF documents. This project allows creation of new PDF documents, manipulation of existing documents and the ability to extract content from documents. It also includes several command-line utilities. 3.3 Further reading and references Practical Web Scraping for Data Science, Best Practices and Examples with Python (2018), Seppe vanden Broucke and Bart Baesens. Web Scraping with Python (2015), Ryan Mitchell Web Data Mining: Exploring Hyperlinks, Contents, and Usage Data. 2nd ed. (2011), Bing Liu XPath W3schools tutorial 3.4 Learning outcomes Data science students should work towards obtaining the knowledge and the skills that enable them to: Understand the architecture and different technologies used in the World Wide Web. Use or build a standalone Web crawler to gather data from the Web. Identify and automatically extract the Web page content of interest. 3.5 Practice problems Explore the Web and find some dynamic Web sites. Using your favourite browser inspect the structure of specific Web pages and try to load the same Web pages with Javascript disabled. Implement and run a crawler that will target https://fri.uni-lj.si/sl/o-fakulteti/osebje and extract the following information about every employee: name, email, phone number, title/job position, and the number of courses they teach and number of projects they have listed. Save the data in CSV format. An example project: Recently, gov.si Web site has been launched. Build a crawler that will retrieve semi-structured data from the site only. Also, you need to obey crawler ethics, take into account robots.txt, etc. Store rendered HTML data and also extracted data into a database. "]
]
