<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Chapter 13 Dealing with missing data | Introduction to data science</title>
  <meta name="description" content="Course notes">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="Chapter 13 Dealing with missing data | Introduction to data science" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://fri-datascience.github.io/course_itds/" />
  
  <meta property="og:description" content="Course notes" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 13 Dealing with missing data | Introduction to data science" />
  
  <meta name="twitter:description" content="Course notes" />
  

<meta name="author" content="Slavko Žitnik, Tomaž Curk, Erik Štrumbelj">


<meta name="date" content="2019-08-04">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="predictive-modelling-feature-selection.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Introduction to data science</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="" data-path="what-is-data-science-anyway.html"><a href="what-is-data-science-anyway.html"><i class="fa fa-check"></i>What is data science anyway?</a><ul>
<li class="chapter" data-level="0.1" data-path="what-is-data-science-anyway.html"><a href="what-is-data-science-anyway.html#label"><i class="fa fa-check"></i><b>0.1</b> The role of this course</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="python-programming-language.html"><a href="python-programming-language.html"><i class="fa fa-check"></i><b>1</b> Python programming language</a><ul>
<li class="chapter" data-level="1.1" data-path="python-programming-language.html"><a href="python-programming-language.html#basic-characteristics"><i class="fa fa-check"></i><b>1.1</b> Basic characteristics</a></li>
<li class="chapter" data-level="1.2" data-path="python-programming-language.html"><a href="python-programming-language.html#why-python"><i class="fa fa-check"></i><b>1.2</b> Why Python?</a></li>
<li class="chapter" data-level="1.3" data-path="python-programming-language.html"><a href="python-programming-language.html#setting-up-the-environment"><i class="fa fa-check"></i><b>1.3</b> Setting up the environment</a><ul>
<li class="chapter" data-level="1.3.1" data-path="python-programming-language.html"><a href="python-programming-language.html#anaconda-distribution-installation"><i class="fa fa-check"></i><b>1.3.1</b> Anaconda distribution installation</a></li>
<li class="chapter" data-level="1.3.2" data-path="python-programming-language.html"><a href="python-programming-language.html#pure-python-distribution-installation"><i class="fa fa-check"></i><b>1.3.2</b> Pure Python distribution installation</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="python-programming-language.html"><a href="python-programming-language.html#installing-dependencies"><i class="fa fa-check"></i><b>1.4</b> Installing dependencies</a></li>
<li class="chapter" data-level="1.5" data-path="python-programming-language.html"><a href="python-programming-language.html#jupyter-notebooks"><i class="fa fa-check"></i><b>1.5</b> Jupyter notebooks</a><ul>
<li class="chapter" data-level="1.5.1" data-path="python-programming-language.html"><a href="python-programming-language.html#running-a-jupyter-notebook"><i class="fa fa-check"></i><b>1.5.1</b> Running a Jupyter notebook</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="python-programming-language.html"><a href="python-programming-language.html#short-introduction-to-python"><i class="fa fa-check"></i><b>1.6</b> Short introduction to Python</a><ul>
<li class="chapter" data-level="" data-path="python-programming-language.html"><a href="python-programming-language.html#basics"><i class="fa fa-check"></i>Basics</a></li>
<li class="chapter" data-level="1.6.1" data-path="python-programming-language.html"><a href="python-programming-language.html#control-flow-operations"><i class="fa fa-check"></i><b>1.6.1</b> Control flow operations</a></li>
<li class="chapter" data-level="1.6.2" data-path="python-programming-language.html"><a href="python-programming-language.html#functions"><i class="fa fa-check"></i><b>1.6.2</b> Functions</a></li>
<li class="chapter" data-level="1.6.3" data-path="python-programming-language.html"><a href="python-programming-language.html#classes-and-objects"><i class="fa fa-check"></i><b>1.6.3</b> Classes and objects</a></li>
<li class="chapter" data-level="1.6.4" data-path="python-programming-language.html"><a href="python-programming-language.html#python-ides-and-code-editors"><i class="fa fa-check"></i><b>1.6.4</b> Python IDE’s and code editors</a></li>
</ul></li>
<li class="chapter" data-level="1.7" data-path="python-programming-language.html"><a href="python-programming-language.html#python-ecosystem-for-data-science"><i class="fa fa-check"></i><b>1.7</b> Python ecosystem for Data Science</a></li>
<li class="chapter" data-level="1.8" data-path="python-programming-language.html"><a href="python-programming-language.html#further-reading-and-references"><i class="fa fa-check"></i><b>1.8</b> Further reading and references</a></li>
<li class="chapter" data-level="1.9" data-path="python-programming-language.html"><a href="python-programming-language.html#learning-outcomes"><i class="fa fa-check"></i><b>1.9</b> Learning outcomes</a></li>
<li class="chapter" data-level="1.10" data-path="python-programming-language.html"><a href="python-programming-language.html#practice-problems"><i class="fa fa-check"></i><b>1.10</b> Practice problems</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="source-code-control.html"><a href="source-code-control.html"><i class="fa fa-check"></i><b>2</b> Source code control</a></li>
<li class="chapter" data-level="3" data-path="docker-container-platform.html"><a href="docker-container-platform.html"><i class="fa fa-check"></i><b>3</b> Docker container platform</a><ul>
<li class="chapter" data-level="3.1" data-path="docker-container-platform.html"><a href="docker-container-platform.html#why-docker"><i class="fa fa-check"></i><b>3.1</b> Why Docker?</a></li>
<li class="chapter" data-level="3.2" data-path="docker-container-platform.html"><a href="docker-container-platform.html#setting-up-the-environment-1"><i class="fa fa-check"></i><b>3.2</b> Setting up the environment</a></li>
<li class="chapter" data-level="3.3" data-path="docker-container-platform.html"><a href="docker-container-platform.html#short-introduction-to-docker"><i class="fa fa-check"></i><b>3.3</b> Short introduction to Docker</a><ul>
<li class="chapter" data-level="3.3.1" data-path="docker-container-platform.html"><a href="docker-container-platform.html#basics-1"><i class="fa fa-check"></i><b>3.3.1</b> Basics</a></li>
<li class="chapter" data-level="3.3.2" data-path="docker-container-platform.html"><a href="docker-container-platform.html#docker-application-example"><i class="fa fa-check"></i><b>3.3.2</b> Docker application example</a></li>
<li class="chapter" data-level="3.3.3" data-path="docker-container-platform.html"><a href="docker-container-platform.html#volumes"><i class="fa fa-check"></i><b>3.3.3</b> Volumes</a></li>
<li class="chapter" data-level="3.3.4" data-path="docker-container-platform.html"><a href="docker-container-platform.html#docker-application-example-with-multiple-services"><i class="fa fa-check"></i><b>3.3.4</b> Docker application example with multiple services</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="docker-container-platform.html"><a href="docker-container-platform.html#further-reading-and-references-1"><i class="fa fa-check"></i><b>3.4</b> Further reading and references</a></li>
<li class="chapter" data-level="3.5" data-path="docker-container-platform.html"><a href="docker-container-platform.html#learning-outcomes-1"><i class="fa fa-check"></i><b>3.5</b> Learning outcomes</a></li>
<li class="chapter" data-level="3.6" data-path="docker-container-platform.html"><a href="docker-container-platform.html#practice-problems-1"><i class="fa fa-check"></i><b>3.6</b> Practice problems</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="web-scraping.html"><a href="web-scraping.html"><i class="fa fa-check"></i><b>4</b> Web scraping</a></li>
<li class="chapter" data-level="5" data-path="dynamic-reports-and-reproducibility.html"><a href="dynamic-reports-and-reproducibility.html"><i class="fa fa-check"></i><b>5</b> Dynamic reports and reproducibility</a></li>
<li class="chapter" data-level="6" data-path="summarizing-data-the-basics.html"><a href="summarizing-data-the-basics.html"><i class="fa fa-check"></i><b>6</b> Summarizing data - the basics</a><ul>
<li class="chapter" data-level="6.1" data-path="summarizing-data-the-basics.html"><a href="summarizing-data-the-basics.html#descriptive-statistics-for-univariate-distributions"><i class="fa fa-check"></i><b>6.1</b> Descriptive statistics for univariate distributions</a><ul>
<li class="chapter" data-level="6.1.1" data-path="summarizing-data-the-basics.html"><a href="summarizing-data-the-basics.html#central-tendency"><i class="fa fa-check"></i><b>6.1.1</b> Central tendency</a></li>
<li class="chapter" data-level="6.1.2" data-path="summarizing-data-the-basics.html"><a href="summarizing-data-the-basics.html#dispersion"><i class="fa fa-check"></i><b>6.1.2</b> Dispersion</a></li>
<li class="chapter" data-level="6.1.3" data-path="summarizing-data-the-basics.html"><a href="summarizing-data-the-basics.html#skewness-and-kurtosis"><i class="fa fa-check"></i><b>6.1.3</b> Skewness and kurtosis</a></li>
<li class="chapter" data-level="6.1.4" data-path="summarizing-data-the-basics.html"><a href="summarizing-data-the-basics.html#nominal-variables"><i class="fa fa-check"></i><b>6.1.4</b> Nominal variables</a></li>
<li class="chapter" data-level="6.1.5" data-path="summarizing-data-the-basics.html"><a href="summarizing-data-the-basics.html#testing-the-shape-of-a-distribution"><i class="fa fa-check"></i><b>6.1.5</b> Testing the shape of a distribution</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="summarizing-data-the-basics.html"><a href="summarizing-data-the-basics.html#descriptive-statistics-for-bivariate-distributions"><i class="fa fa-check"></i><b>6.2</b> Descriptive statistics for bivariate distributions</a></li>
<li class="chapter" data-level="6.3" data-path="summarizing-data-the-basics.html"><a href="summarizing-data-the-basics.html#further-reading-and-references-2"><i class="fa fa-check"></i><b>6.3</b> Further reading and references</a></li>
<li class="chapter" data-level="6.4" data-path="summarizing-data-the-basics.html"><a href="summarizing-data-the-basics.html#learning-outcomes-2"><i class="fa fa-check"></i><b>6.4</b> Learning outcomes</a></li>
<li class="chapter" data-level="6.5" data-path="summarizing-data-the-basics.html"><a href="summarizing-data-the-basics.html#practice-problems-2"><i class="fa fa-check"></i><b>6.5</b> Practice problems</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="summarizing-data-visualization.html"><a href="summarizing-data-visualization.html"><i class="fa fa-check"></i><b>7</b> Summarizing data - visualization</a><ul>
<li class="chapter" data-level="7.1" data-path="summarizing-data-visualization.html"><a href="summarizing-data-visualization.html#histograms-and-density-plots"><i class="fa fa-check"></i><b>7.1</b> Histograms and density plots</a></li>
<li class="chapter" data-level="7.2" data-path="summarizing-data-visualization.html"><a href="summarizing-data-visualization.html#bar-plot"><i class="fa fa-check"></i><b>7.2</b> Bar plot</a></li>
<li class="chapter" data-level="7.3" data-path="summarizing-data-visualization.html"><a href="summarizing-data-visualization.html#pie-chart"><i class="fa fa-check"></i><b>7.3</b> Pie chart</a></li>
<li class="chapter" data-level="7.4" data-path="summarizing-data-visualization.html"><a href="summarizing-data-visualization.html#scatterplot"><i class="fa fa-check"></i><b>7.4</b> Scatterplot</a></li>
<li class="chapter" data-level="7.5" data-path="summarizing-data-visualization.html"><a href="summarizing-data-visualization.html#d-density-plot"><i class="fa fa-check"></i><b>7.5</b> 2D density plot</a></li>
<li class="chapter" data-level="7.6" data-path="summarizing-data-visualization.html"><a href="summarizing-data-visualization.html#boxplot"><i class="fa fa-check"></i><b>7.6</b> Boxplot</a></li>
<li class="chapter" data-level="7.7" data-path="summarizing-data-visualization.html"><a href="summarizing-data-visualization.html#violin-plot"><i class="fa fa-check"></i><b>7.7</b> Violin plot</a></li>
<li class="chapter" data-level="7.8" data-path="summarizing-data-visualization.html"><a href="summarizing-data-visualization.html#correlogram"><i class="fa fa-check"></i><b>7.8</b> Correlogram</a></li>
<li class="chapter" data-level="7.9" data-path="summarizing-data-visualization.html"><a href="summarizing-data-visualization.html#a-comprehensive-summary"><i class="fa fa-check"></i><b>7.9</b> A comprehensive summary</a></li>
<li class="chapter" data-level="7.10" data-path="summarizing-data-visualization.html"><a href="summarizing-data-visualization.html#further-reading-and-references-3"><i class="fa fa-check"></i><b>7.10</b> Further reading and references</a></li>
<li class="chapter" data-level="7.11" data-path="summarizing-data-visualization.html"><a href="summarizing-data-visualization.html#learning-outcomes-3"><i class="fa fa-check"></i><b>7.11</b> Learning outcomes</a></li>
<li class="chapter" data-level="7.12" data-path="summarizing-data-visualization.html"><a href="summarizing-data-visualization.html#practice-problems-3"><i class="fa fa-check"></i><b>7.12</b> Practice problems</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="summarizing-data-multivariate-data.html"><a href="summarizing-data-multivariate-data.html"><i class="fa fa-check"></i><b>8</b> Summarizing data - multivariate data</a><ul>
<li class="chapter" data-level="8.1" data-path="summarizing-data-multivariate-data.html"><a href="summarizing-data-multivariate-data.html#principal-component-analysis-pca"><i class="fa fa-check"></i><b>8.1</b> Principal Component Analysis (PCA)</a></li>
<li class="chapter" data-level="8.2" data-path="summarizing-data-multivariate-data.html"><a href="summarizing-data-multivariate-data.html#factor-analysis-fa"><i class="fa fa-check"></i><b>8.2</b> Factor analysis (FA)</a></li>
<li class="chapter" data-level="8.3" data-path="summarizing-data-multivariate-data.html"><a href="summarizing-data-multivariate-data.html#multi-dimensional-scaling-mds"><i class="fa fa-check"></i><b>8.3</b> Multi-dimensional scaling (MDS)</a></li>
<li class="chapter" data-level="8.4" data-path="summarizing-data-multivariate-data.html"><a href="summarizing-data-multivariate-data.html#t-distributed-stochastic-neighbor-embedding-t-sne"><i class="fa fa-check"></i><b>8.4</b> t-Distributed Stochastic Neighbor Embedding (t-SNE)</a></li>
<li class="chapter" data-level="8.5" data-path="summarizing-data-multivariate-data.html"><a href="summarizing-data-multivariate-data.html#clustering"><i class="fa fa-check"></i><b>8.5</b> Clustering</a><ul>
<li class="chapter" data-level="8.5.1" data-path="summarizing-data-multivariate-data.html"><a href="summarizing-data-multivariate-data.html#k-means-clustering"><i class="fa fa-check"></i><b>8.5.1</b> k-means clustering</a></li>
<li class="chapter" data-level="8.5.2" data-path="summarizing-data-multivariate-data.html"><a href="summarizing-data-multivariate-data.html#determining-the-number-of-clusters"><i class="fa fa-check"></i><b>8.5.2</b> Determining the number of clusters</a></li>
<li class="chapter" data-level="8.5.3" data-path="summarizing-data-multivariate-data.html"><a href="summarizing-data-multivariate-data.html#agglomerative-hierarchical-clustering"><i class="fa fa-check"></i><b>8.5.3</b> Agglomerative hierarchical clustering</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="summarizing-data-multivariate-data.html"><a href="summarizing-data-multivariate-data.html#further-reading-and-references-4"><i class="fa fa-check"></i><b>8.6</b> Further reading and references</a></li>
<li class="chapter" data-level="8.7" data-path="summarizing-data-multivariate-data.html"><a href="summarizing-data-multivariate-data.html#learning-outcomes-4"><i class="fa fa-check"></i><b>8.7</b> Learning outcomes</a></li>
<li class="chapter" data-level="8.8" data-path="summarizing-data-multivariate-data.html"><a href="summarizing-data-multivariate-data.html#practice-problems-4"><i class="fa fa-check"></i><b>8.8</b> Practice problems</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="relational-databases.html"><a href="relational-databases.html"><i class="fa fa-check"></i><b>9</b> Relational databases</a></li>
<li class="chapter" data-level="10" data-path="predictive-modelling-introduction.html"><a href="predictive-modelling-introduction.html"><i class="fa fa-check"></i><b>10</b> Predictive modelling - Introduction</a></li>
<li class="chapter" data-level="11" data-path="predictive-modelling-learning-paradigms.html"><a href="predictive-modelling-learning-paradigms.html"><i class="fa fa-check"></i><b>11</b> Predictive modelling - Learning paradigms</a></li>
<li class="chapter" data-level="12" data-path="predictive-modelling-feature-selection.html"><a href="predictive-modelling-feature-selection.html"><i class="fa fa-check"></i><b>12</b> Predictive modelling - Feature selection</a></li>
<li class="chapter" data-level="13" data-path="dealing-with-missing-data.html"><a href="dealing-with-missing-data.html"><i class="fa fa-check"></i><b>13</b> Dealing with missing data</a><ul>
<li class="chapter" data-level="13.1" data-path="dealing-with-missing-data.html"><a href="dealing-with-missing-data.html#the-severity-of-the-missing-data-problem"><i class="fa fa-check"></i><b>13.1</b> The severity of the missing data problem</a></li>
<li class="chapter" data-level="13.2" data-path="dealing-with-missing-data.html"><a href="dealing-with-missing-data.html#visually-exploring-missingness"><i class="fa fa-check"></i><b>13.2</b> Visually exploring missingness</a></li>
<li class="chapter" data-level="13.3" data-path="dealing-with-missing-data.html"><a href="dealing-with-missing-data.html#deletion-methods"><i class="fa fa-check"></i><b>13.3</b> Deletion methods</a><ul>
<li class="chapter" data-level="13.3.1" data-path="dealing-with-missing-data.html"><a href="dealing-with-missing-data.html#column-deletion"><i class="fa fa-check"></i><b>13.3.1</b> Column deletion</a></li>
<li class="chapter" data-level="13.3.2" data-path="dealing-with-missing-data.html"><a href="dealing-with-missing-data.html#row-deletion"><i class="fa fa-check"></i><b>13.3.2</b> Row deletion</a></li>
<li class="chapter" data-level="13.3.3" data-path="dealing-with-missing-data.html"><a href="dealing-with-missing-data.html#pairwise-deletion"><i class="fa fa-check"></i><b>13.3.3</b> Pairwise deletion</a></li>
</ul></li>
<li class="chapter" data-level="13.4" data-path="dealing-with-missing-data.html"><a href="dealing-with-missing-data.html#imputation-methods"><i class="fa fa-check"></i><b>13.4</b> Imputation methods</a><ul>
<li class="chapter" data-level="13.4.1" data-path="dealing-with-missing-data.html"><a href="dealing-with-missing-data.html#single-imputation-with-the-mean"><i class="fa fa-check"></i><b>13.4.1</b> Single imputation with the mean</a></li>
<li class="chapter" data-level="13.4.2" data-path="dealing-with-missing-data.html"><a href="dealing-with-missing-data.html#single-imputation-with-prediction"><i class="fa fa-check"></i><b>13.4.2</b> Single imputation with prediction</a></li>
<li class="chapter" data-level="13.4.3" data-path="dealing-with-missing-data.html"><a href="dealing-with-missing-data.html#multiple-imputation"><i class="fa fa-check"></i><b>13.4.3</b> Multiple imputation</a></li>
</ul></li>
<li class="chapter" data-level="13.5" data-path="dealing-with-missing-data.html"><a href="dealing-with-missing-data.html#summary"><i class="fa fa-check"></i><b>13.5</b> Summary</a></li>
<li class="chapter" data-level="13.6" data-path="dealing-with-missing-data.html"><a href="dealing-with-missing-data.html#further-reading-and-references-5"><i class="fa fa-check"></i><b>13.6</b> Further reading and references</a></li>
<li class="chapter" data-level="13.7" data-path="dealing-with-missing-data.html"><a href="dealing-with-missing-data.html#learning-outcomes-5"><i class="fa fa-check"></i><b>13.7</b> Learning outcomes</a></li>
<li class="chapter" data-level="13.8" data-path="dealing-with-missing-data.html"><a href="dealing-with-missing-data.html#practice-problems-5"><i class="fa fa-check"></i><b>13.8</b> Practice problems</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>      
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Introduction to data science</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="dealing-with-missing-data" class="section level1">
<h1><span class="header-section-number">Chapter 13</span> Dealing with missing data</h1>
<p>In all of our data analyses so far we implicitly assumed that we don’t have any missing values in our data. In practice, that is often not the case. While some statistical and machine learning methods work with missing data, many commonly used methods can’t, so it is important to learn how to deal with missing values. In this chapter we will discuss a few of the most common methods.</p>
<div id="the-severity-of-the-missing-data-problem" class="section level2">
<h2><span class="header-section-number">13.1</span> The severity of the missing data problem</h2>
<p>If we judge by the imputation or data removing methods that are most commonly used in practice, we might conclude that missing data is a relatively simple problem that is secondary to the inference, predictive modelling, etc. that are the primary goal of our analysis. Unfortunately, that is not the case. Dealing with missing values is very challenging, often in itself a modelling problem.</p>
<p>The choice of an appropriate method is inseparable from our understanding of or assumptions about the process that generated the missing values (the <em>missingness mechanism</em>). Based on the characteristics of this process we typically characterize the missing data problem as one of these three cases:</p>
<ol style="list-style-type: lower-alpha">
<li><p><strong>MCAR</strong> (Missing Completely At Random): Whether or not a value is missing is independent of both the observed values and the missing (unobserved) values. For example, if we had temperature measuring devices at different locations and they occassionally and random intervals stopped working. Or, in surveys, where respondents don’t respond with a certain probability, independent of the characteristics that we are surveying.</p></li>
<li><p><strong>MAR</strong> (Missing At Random): Whether or not a value is missing is independent of the missing (unobserved) values but depends on the observed values. That is, there is a pattern to how the values are missing, but we could fully explain that pattern given only the observed data. For example, if our temperature measuring devices stopped working more often in certain locations than in others. Or, in surveys, if women are less likely to report their weight than men.</p></li>
<li><p><strong>MNAR</strong> (Missing Not At Random): Whether or not a value is missing also depends on the missing (unobserved) values in a way that can’t be explained by the observed values. That is, there is a pattern to how the values are missing, but we wouldn’t be able to fully explain it without observing the values that are missing. For example, if our temperature measuring device had a tendency to stop working when the temperature is very low. Or, in surveys, if a person was less likely to report their salary if their salary was high.</p></li>
</ol>
<p>Every variable in our data might have a different missingness mechanism. So, how do we determine whether it is MCAR, MAR, or MNAR?</p>
<p>Showing with a reasonable degree of certainty that the mechanism is not MCAR is equivalent to showing that the missingness (whether or not a value is missing) can be predicted from observed values. That is, it is a prediction problem and it is sufficient to show <em>one way</em> that missingness can be predicted. On the other hand, it is infeasible to show that the mechanism is MCAR, because that would require us to show that there is <em>no way</em> of predicting missingness from observed values. We can, however, rule out certain kinds of dependency (for example, linear dependency).</p>
<p>For MNAR, the situation is even worse. In general, it is impossible to determine the relationship between missingness and the value that is missing, because we don’t know what is missing. That is, unless we are able to somehow measure the values that are missing, we won’t be able to determine whether or not the missingness regime is MNAR. Getting our hands on the missing values is rarely possible.</p>
<p>To summarize, we’ll often be able to show that our missingness regime is not MCAR and never that it is MCAR. Subsequently, we’ll often know that the missingness regime is at least MAR, but we’ll rarely be able to determine whether it is MAR or MNAR, unless we can get our hands on the missing data.</p>
<p>The discussion in this section is very relevant for the remainder of this chapter. It will help us understand the limitations of the methods for dealing with missing data and the consequences of making the wrong choice.</p>
<p>We will discuss two of the most common types of methods - deletion methods and imputation methods. Both types of methods can introduce a bias to our data. That is, they can result in data that are no longer a completely representative (simple random) sample from the process that generated the data and that is the focus of our analysis. Note that the extent of the bias depends the amount of missingness and the strength of the dependencies, so there is no general rule. We must deal with each situation individually</p>
</div>
<div id="visually-exploring-missingness" class="section level2">
<h2><span class="header-section-number">13.2</span> Visually exploring missingness</h2>
<p>Throughout the remainder of the chapter we will be using a dataset from</p>
<p><em>Pampaka, M., Hutcheson, G., &amp; Williams, J. (2016). Handling missing data: analysis of a challenging data set using multiple imputation. International Journal of Research &amp; Method in Education, 39(1), 19-37.</em></p>
<p>as an illustrative example:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">dat &lt;-<span class="st"> </span><span class="kw">read.csv</span>(<span class="st">&quot;./data/imputationDATASET.csv&quot;</span>)
<span class="kw">summary</span>(dat)</code></pre></div>
<pre><code>##      GCSEgrade      Course      Disposition      SelfEfficacy    
##  1_intC   :201   ASTrad:1014   Min.   :-6.840   Min.   :-4.5500  
##  2_higherC: 80   UoM   : 360   1st Qu.: 0.500   1st Qu.: 0.0000  
##  3_intB   :354                 Median : 2.830   Median : 0.5800  
##  4_higherB:353                 Mean   : 2.176   Mean   : 0.7243  
##  5_A      :294                 3rd Qu.: 4.320   3rd Qu.: 1.3500  
##  6_A*     : 92                 Max.   : 4.320   Max.   : 6.0500  
##                                                                  
##  dropOUTretrieved dropOUToriginal    Gender         Language     EMA     
##  no :780          no  :289        female:513   BILINGUAL:273   no  :617  
##  yes:594          yes :206        male  :860   ENGLISH  :985   yes :698  
##                   NA&#39;s:879        NA&#39;s  :  1   OTHER    : 70   NA&#39;s: 59  
##                                                NA&#39;s     : 46             
##                                                                          
##                                                                          
##                                                                          
##    Ethnicity     LPN          HEFCE                   uniFAM   
##  ASIAN  :251   NO  :907   Min.   :1.000   firstgeneration:497  
##  BLACK  :105   YES :302   1st Qu.:2.000   parents        :342  
##  CHINESE: 17   NA&#39;s:165   Median :3.000   siblings       :499  
##  OTHER  : 58              Mean   :3.843   NA&#39;s           : 36  
##  WHITE  :449              3rd Qu.:6.000                        
##  NA&#39;s   :494              Max.   :6.000                        
##                           NA&#39;s   :167</code></pre>
<p>In this analysis, the authors were interested in modelling dropout from mathematics courses (<em>dropOUToriginal</em>) which had a lot of missing values. The actual values were later retreived (<em>dropOUTretrieved</em>), but we will only use these to verify the quality of our methods.</p>
<p>The other variables include their previous GCSE qualifications results in mathematics (<em>GCSEgrade</em>), type of course (<em>Course</em>), their disposition to study mathematics at a higher level (<em>Disposition</em>), their self-efficacy rating (<em>SelfEfficacy</em>), gender, language, whether the student was holding an Educational Maintenance Allowance (<em>EMA</em>), ethnicity, whether the student was from Low Participation Neighbourhood (<em>LPN</em>), socio-economic status (<em>HEFCE</em>) and whether the student was not first generation at HE (<em>uniFAM</em>).</p>
<p>If our dataset does not have too many variables, a visual summary such as this one can be very effective at revealing the extend and patterns of missingness in our data:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(naniar)</code></pre></div>
<pre><code>## Warning: package &#39;naniar&#39; was built under R version 3.5.3</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">vis_miss</span>(dat, <span class="dt">warn_large_data =</span> F)</code></pre></div>
<p><img src="Missing-data_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<p>Another useful visualization is the frequency of different patterns of missingness:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(UpSetR)</code></pre></div>
<pre><code>## Warning: package &#39;UpSetR&#39; was built under R version 3.5.3</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">gg_miss_upset</span>(dat)</code></pre></div>
<p><img src="Missing-data_files/figure-html/unnamed-chunk-3-1.png" width="672" /> From the above plot we can see that dropout and ethnicity have the most missing values and that the other most common pattern is for both of them to be missing. If missingness of dropout and ethnicity were independent, we should expect about 64% of missing ethnicity rows to also have missing dropout. That is very likely not the case, as only about a half have missing dropout. That is, there is likely a pattern to missingness.</p>
</div>
<div id="deletion-methods" class="section level2">
<h2><span class="header-section-number">13.3</span> Deletion methods</h2>
<p>In this section we will cover deletion methods - dealing with missing values by deleting the columns and/or rows that have them.</p>
<div id="column-deletion" class="section level3">
<h3><span class="header-section-number">13.3.1</span> Column deletion</h3>
<p>The most simple way of dealing with missing values is to delete the column that holds the variable. That is, to remove that variable from all observations.</p>
<p>Such complete removal of potentially useful data is never the optimal choice in terms of available information. And it is not even an option if we are interested in doing inference with/about that variable. In our illustrative example we would, for example, be able to use only the GCSEgrade, Course, Disposition and SelfEfficacy variables to predict dropout.</p>
<p>However, if we have reason to believe that the variable is not practically important for our analysis and/or the fraction of missing values is so large, it might be more easier to just remove the variable. That is, in some cases, the effort of dealing with missing values might outweigh the potential benefits of the extra information.</p>
<p>By removing a variable we of course completely remove all information about the dependency of its (missing) values with other variables. So, regardless of what the missingness mechanism is, we will <em>not</em> introduce a bias to our analyses by removing a column.</p>
</div>
<div id="row-deletion" class="section level3">
<h3><span class="header-section-number">13.3.2</span> Row deletion</h3>
<p>The most common deletion approach is to delete all rows with missing values. If we are not interested in those rows directly and there are not many such rows than this is a completely viable alternative to imputation methods.</p>
<p>However, we must be aware that unless the missingness mechanism is MCAR, we will be introducing a bias into our data. Let’s estimate the dropout rate from the data that are available. That is, we remove all rows where we don’t know whether the student dropped out or not:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">stderr &lt;-<span class="st"> </span><span class="cf">function</span>(x) {
  N  &lt;-<span class="st"> </span><span class="kw">sum</span>(<span class="op">!</span><span class="kw">is.na</span>(x), <span class="dt">na.rm =</span> T)
  <span class="kw">sd</span>(x, <span class="dt">na.rm =</span> T) <span class="op">/</span><span class="st"> </span><span class="kw">sqrt</span>(N)
}

x  &lt;-<span class="st"> </span>dat<span class="op">$</span>dropOUToriginal <span class="op">==</span><span class="st"> &quot;yes&quot;</span>
mu &lt;-<span class="st"> </span><span class="kw">mean</span>(x, <span class="dt">na.rm =</span> T)
N  &lt;-<span class="st"> </span><span class="kw">sum</span>(<span class="op">!</span><span class="kw">is.na</span>(x), <span class="dt">na.rm =</span> T)
SE &lt;-<span class="st"> </span><span class="kw">stderr</span>(x)
<span class="kw">cat</span>(<span class="kw">sprintf</span>(<span class="st">&quot;%.2f +/- %.3f (n = %d)</span><span class="ch">\n</span><span class="st">&quot;</span>, mu, SE, N))</code></pre></div>
<pre><code>## 0.42 +/- 0.022 (n = 495)</code></pre>
<p>So, this suggest that we can be reasonably certain that the dropout rate is around 42%. However, this is only valid if the observed values are also a representative sample from our original data (we’re assuming that the original data are a representative sample from the population). That will always be the case if the missingness mechanism is MCAR - if missingness is completely random then row deletion will also be completely random.</p>
<p>As we stated at the begining, there is no way of proving that the mechanism is MCAR, however, we can do our best to show that it is not and then account for it. We’ll do the latter in the imputation methods section. Here, we just show how incorrect conclusions could be reached if we rely only on row deletion.</p>
<p>Observe that the relative frequency of missing values depends on ethnicity:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">x  &lt;-<span class="st"> </span><span class="kw">is.na</span>(dat<span class="op">$</span>dropOUToriginal)[dat<span class="op">$</span>Ethnicity <span class="op">==</span><span class="st"> &quot;WHITE&quot;</span>]
mu &lt;-<span class="st"> </span><span class="kw">mean</span>(x, <span class="dt">na.rm =</span> T)
N  &lt;-<span class="st"> </span><span class="kw">sum</span>(<span class="op">!</span><span class="kw">is.na</span>(x), <span class="dt">na.rm =</span> T)
SE &lt;-<span class="st"> </span><span class="kw">sd</span>(x, <span class="dt">na.rm =</span> T) <span class="op">/</span><span class="st"> </span><span class="kw">sqrt</span>(N)
<span class="kw">cat</span>(<span class="kw">sprintf</span>(<span class="st">&quot;==WHITE: %.2f +/- %.3f (n = %d)</span><span class="ch">\n</span><span class="st">&quot;</span>, mu, SE, N))</code></pre></div>
<pre><code>## ==WHITE: 0.61 +/- 0.023 (n = 449)</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">x  &lt;-<span class="st"> </span><span class="kw">is.na</span>(dat<span class="op">$</span>dropOUToriginal)[dat<span class="op">$</span>Ethnicity <span class="op">!=</span><span class="st"> &quot;WHITE&quot;</span>]
mu &lt;-<span class="st"> </span><span class="kw">mean</span>(x, <span class="dt">na.rm =</span> T)
N  &lt;-<span class="st"> </span><span class="kw">sum</span>(<span class="op">!</span><span class="kw">is.na</span>(x), <span class="dt">na.rm =</span> T)
SE &lt;-<span class="st"> </span><span class="kw">stderr</span>(x)
<span class="kw">cat</span>(<span class="kw">sprintf</span>(<span class="st">&quot;!=WHITE: %.2f +/- %.3f (n = %d)</span><span class="ch">\n</span><span class="st">&quot;</span>, mu, SE, N))</code></pre></div>
<pre><code>## !=WHITE: 0.70 +/- 0.022 (n = 431)</code></pre>
<p>So, we can be reasonably certain that the missingness mechanism is at least MAR! Now, if those of white ethnicity would be more (less) prone to dropping out, our estimate of 42% from above would underestimate (overestimate) true dropout rate! Before using row deletion, we should check if dropout rate depends on ethnicity:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">x  &lt;-<span class="st"> </span>(dat<span class="op">$</span>dropOUToriginal <span class="op">==</span><span class="st"> &quot;yes&quot;</span>)[dat<span class="op">$</span>Ethnicity <span class="op">==</span><span class="st"> &quot;WHITE&quot;</span>]
mu &lt;-<span class="st"> </span><span class="kw">mean</span>(x, <span class="dt">na.rm =</span> T)
N  &lt;-<span class="st"> </span><span class="kw">sum</span>(<span class="op">!</span><span class="kw">is.na</span>(x), <span class="dt">na.rm =</span> T)
SE &lt;-<span class="st"> </span><span class="kw">stderr</span>(x)
<span class="kw">cat</span>(<span class="kw">sprintf</span>(<span class="st">&quot;==WHITE: %.2f +/- %.3f (n = %d)</span><span class="ch">\n</span><span class="st">&quot;</span>, mu, SE, N))</code></pre></div>
<pre><code>## ==WHITE: 0.45 +/- 0.037 (n = 177)</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">x  &lt;-<span class="st"> </span>(dat<span class="op">$</span>dropOUToriginal <span class="op">==</span><span class="st"> &quot;yes&quot;</span>)[dat<span class="op">$</span>Ethnicity <span class="op">!=</span><span class="st"> &quot;WHITE&quot;</span>]
mu &lt;-<span class="st"> </span><span class="kw">mean</span>(x, <span class="dt">na.rm =</span> T)
N  &lt;-<span class="st"> </span><span class="kw">sum</span>(<span class="op">!</span><span class="kw">is.na</span>(x), <span class="dt">na.rm =</span> T)
SE &lt;-<span class="st"> </span><span class="kw">stderr</span>(x)
<span class="kw">cat</span>(<span class="kw">sprintf</span>(<span class="st">&quot;!=WHITE: %.2f +/- %.3f (n = %d)</span><span class="ch">\n</span><span class="st">&quot;</span>, mu, SE, N))</code></pre></div>
<pre><code>## !=WHITE: 0.43 +/- 0.044 (n = 130)</code></pre>
<p>In this case, there is no discernible difference. So, the bias that we might be introducing, is small. Of course, we should do this check for every variable that can reasonably be dependent on/off missigness of dropout!</p>
<p>If the missingness mechanism is MNAR, there is not much we can do. For example, it is not unreasonable to assume that people that dropped out might be less likely to report the dropout information. However, we could not be able to verify if that is the case unless we gathered some of the missing data. Our illustrative example is one of those rare exceptions - the authors gathered the true values hidden behind the missing values, so we can compare:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">x  &lt;-<span class="st"> </span>dat<span class="op">$</span>dropOUToriginal <span class="op">==</span><span class="st"> &quot;yes&quot;</span>
mu &lt;-<span class="st"> </span><span class="kw">mean</span>(x, <span class="dt">na.rm =</span> T)
N  &lt;-<span class="st"> </span><span class="kw">sum</span>(<span class="op">!</span><span class="kw">is.na</span>(x), <span class="dt">na.rm =</span> T)
SE &lt;-<span class="st"> </span><span class="kw">stderr</span>(x)
<span class="kw">cat</span>(<span class="kw">sprintf</span>(<span class="st">&quot;ORIGINAL: %.2f +/- %.3f (n = %d)</span><span class="ch">\n</span><span class="st">&quot;</span>, mu, SE, N))</code></pre></div>
<pre><code>## ORIGINAL: 0.42 +/- 0.022 (n = 495)</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">x  &lt;-<span class="st"> </span>dat[<span class="kw">is.na</span>(dat<span class="op">$</span>dropOUToriginal),]<span class="op">$</span>dropOUTretrieved <span class="op">==</span><span class="st"> &quot;yes&quot;</span> 
mu &lt;-<span class="st"> </span><span class="kw">mean</span>(x, <span class="dt">na.rm =</span> T)
N  &lt;-<span class="st"> </span><span class="kw">sum</span>(<span class="op">!</span><span class="kw">is.na</span>(x), <span class="dt">na.rm =</span> T)
SE &lt;-<span class="st"> </span><span class="kw">stderr</span>(x)
<span class="kw">cat</span>(<span class="kw">sprintf</span>(<span class="st">&quot; MISSING: %.2f +/- %.3f (n = %d)</span><span class="ch">\n</span><span class="st">&quot;</span>, mu, SE, N))</code></pre></div>
<pre><code>##  MISSING: 0.44 +/- 0.017 (n = 879)</code></pre>
<p>There is no discernible difference, so, for practical purposes, we’ll conclude that the missingness mechanism is MAR and not MNAR.</p>
<p>To summarize this section: row deletion can be useful, but if we are deleting many rows, we should always check if the missingness mechanism is at least MAR and check if we need to account for dependencies between other variables and missingness.</p>
</div>
<div id="pairwise-deletion" class="section level3">
<h3><span class="header-section-number">13.3.3</span> Pairwise deletion</h3>
<p>Pairwise deletion is a special case of row deletion where we delete only the rows with missing values in the variables of interest for a particular part of the analysis. For example, suppose that we are interested in whether or not there is a dependency between dropout, language and ethnicity. First, lets investigate this on the subset of the three columns where we also drop all rows with missing values. We’ll use a Chi-squared test (see Basic summarization chapter for details) to test for dependency between the categorical variables:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">tmp &lt;-<span class="st"> </span>dat[,<span class="kw">c</span>(<span class="dv">6</span>,<span class="dv">8</span>,<span class="dv">10</span>)]
tmp &lt;-<span class="st"> </span>tmp[<span class="kw">complete.cases</span>(tmp),]
<span class="kw">cat</span>(<span class="kw">sprintf</span>(<span class="st">&quot;Rows before = %d, rows after deletion = %d</span><span class="ch">\n</span><span class="st">&quot;</span>, <span class="kw">nrow</span>(dat), <span class="kw">nrow</span>(tmp)))</code></pre></div>
<pre><code>## Rows before = 1374, rows after deletion = 291</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">chisq.test</span>(<span class="kw">table</span>(tmp<span class="op">$</span>dropOUToriginal, tmp<span class="op">$</span>Language), <span class="dt">simulate.p.value =</span> T)</code></pre></div>
<pre><code>## 
##  Pearson&#39;s Chi-squared test with simulated p-value (based on 2000
##  replicates)
## 
## data:  table(tmp$dropOUToriginal, tmp$Language)
## X-squared = 4.6808, df = NA, p-value = 0.09895</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">chisq.test</span>(<span class="kw">table</span>(tmp<span class="op">$</span>dropOUToriginal, tmp<span class="op">$</span>Ethnicity), <span class="dt">simulate.p.value =</span> T)</code></pre></div>
<pre><code>## 
##  Pearson&#39;s Chi-squared test with simulated p-value (based on 2000
##  replicates)
## 
## data:  table(tmp$dropOUToriginal, tmp$Ethnicity)
## X-squared = 1.4154, df = NA, p-value = 0.8461</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">chisq.test</span>(<span class="kw">table</span>(tmp<span class="op">$</span>Ethnicity, tmp<span class="op">$</span>Language), <span class="dt">simulate.p.value =</span> T)</code></pre></div>
<pre><code>## 
##  Pearson&#39;s Chi-squared test with simulated p-value (based on 2000
##  replicates)
## 
## data:  table(tmp$Ethnicity, tmp$Language)
## X-squared = 123.1, df = NA, p-value = 0.0004998</code></pre>
<p>So, if we dropped all rows with missing values in any of the three columns, we’d be left with 291 observations. If we instead remove for each pair only the rows that are missing one of those values, we get 474, 307 and 841 observations, respectively. If we had used a 5% risk level, we’d in fact reject the null hypothesis with pairwise deletion but not with row deletion:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">tmp &lt;-<span class="st"> </span>dat[,<span class="kw">c</span>(<span class="dv">6</span>,<span class="dv">8</span>)]
tmp &lt;-<span class="st"> </span>tmp[<span class="kw">complete.cases</span>(tmp),]
<span class="kw">cat</span>(<span class="kw">sprintf</span>(<span class="st">&quot;Rows before = %d, rows after deletion = %d</span><span class="ch">\n</span><span class="st">&quot;</span>, <span class="kw">nrow</span>(dat), <span class="kw">nrow</span>(tmp)))</code></pre></div>
<pre><code>## Rows before = 1374, rows after deletion = 474</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">chisq.test</span>(<span class="kw">table</span>(tmp<span class="op">$</span>dropOUToriginal, tmp<span class="op">$</span>Language), <span class="dt">simulate.p.value =</span> T)</code></pre></div>
<pre><code>## 
##  Pearson&#39;s Chi-squared test with simulated p-value (based on 2000
##  replicates)
## 
## data:  table(tmp$dropOUToriginal, tmp$Language)
## X-squared = 5.8535, df = NA, p-value = 0.05547</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">tmp &lt;-<span class="st"> </span>dat[,<span class="kw">c</span>(<span class="dv">6</span>,<span class="dv">10</span>)]
tmp &lt;-<span class="st"> </span>tmp[<span class="kw">complete.cases</span>(tmp),]
<span class="kw">cat</span>(<span class="kw">sprintf</span>(<span class="st">&quot;Rows before = %d, rows after deletion = %d</span><span class="ch">\n</span><span class="st">&quot;</span>, <span class="kw">nrow</span>(dat), <span class="kw">nrow</span>(tmp)))</code></pre></div>
<pre><code>## Rows before = 1374, rows after deletion = 307</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">chisq.test</span>(<span class="kw">table</span>(tmp<span class="op">$</span>dropOUToriginal, tmp<span class="op">$</span>Ethnicity), <span class="dt">simulate.p.value =</span> T)</code></pre></div>
<pre><code>## 
##  Pearson&#39;s Chi-squared test with simulated p-value (based on 2000
##  replicates)
## 
## data:  table(tmp$dropOUToriginal, tmp$Ethnicity)
## X-squared = 0.72928, df = NA, p-value = 0.953</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">tmp &lt;-<span class="st"> </span>dat[,<span class="kw">c</span>(<span class="dv">8</span>,<span class="dv">10</span>)]
tmp &lt;-<span class="st"> </span>tmp[<span class="kw">complete.cases</span>(tmp),]
<span class="kw">cat</span>(<span class="kw">sprintf</span>(<span class="st">&quot;Rows before = %d, rows after deletion = %d</span><span class="ch">\n</span><span class="st">&quot;</span>, <span class="kw">nrow</span>(dat), <span class="kw">nrow</span>(tmp)))</code></pre></div>
<pre><code>## Rows before = 1374, rows after deletion = 841</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">chisq.test</span>(<span class="kw">table</span>(tmp<span class="op">$</span>Ethnicity, tmp<span class="op">$</span>Language), <span class="dt">simulate.p.value =</span> T)</code></pre></div>
<pre><code>## 
##  Pearson&#39;s Chi-squared test with simulated p-value (based on 2000
##  replicates)
## 
## data:  table(tmp$Ethnicity, tmp$Language)
## X-squared = 285.5, df = NA, p-value = 0.0004998</code></pre>
<p>So, pairwise deletion allows us to use more data. Note, however, that all that we’ve discussed for row deletion also applies to pairwise deletion. And with pairwise deletion, we have to deal with each sub-analysis separately.</p>
</div>
</div>
<div id="imputation-methods" class="section level2">
<h2><span class="header-section-number">13.4</span> Imputation methods</h2>
<p>The alternative to deleting rows or columns with missing values is to replace missing data with a value. When a single value is used, we refer to it as single imputation, when multiple values (a distribution) is used to replace a missing value, we refer to it as multiple imputation. In this section we’ll cover some of the most common variants of both single and multiple imputation.</p>
<div id="single-imputation-with-the-mean" class="section level3">
<h3><span class="header-section-number">13.4.1</span> Single imputation with the mean</h3>
<p>Single imputation with the mean is a simple procedure of replacing all missing values with the mean of the observed values.</p>
<p>Imputation with the mean has the advantage of not introducing a bias automatically - changing all missing values to the mean will not change the sample mean across all variable values. However, similar to deletion methods, if the missingness mechanism is not MCAR, we risk introducing a bias.</p>
<p>Replacing missing values with the median or mode (with categorical variables mode is the only reasonable central tendency) can be more representative of the underlying distribution, but we must be aware that it automatically introduces a bias. That is, we are inserting values that are not the mean and therefore by definition changing the expectation and introducing a bias. Note that for categorical variables we can also treat missing values as a new separate category and procede with inference and prediction - if we specify the model correctly, such data will be sufficient to account for bias, even if the missingness mechanism is MAR.</p>
<p>A major disadvantage of single imputation is that it typically reduces the variance of the variable. Subsequently, it also reduces any covariance of that variable with other variables. Observe how the variability of the HEFCE variable and its correlation with SelfEfficacy reduce after mean imputation:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">x  &lt;-<span class="st"> </span>dat<span class="op">$</span>HEFCE
mu &lt;-<span class="st"> </span><span class="kw">mean</span>(x, <span class="dt">na.rm =</span> T)
sd &lt;-<span class="st"> </span><span class="kw">sd</span>(x, <span class="dt">na.rm =</span> T)
pr &lt;-<span class="st"> </span><span class="kw">cor</span>(x, dat<span class="op">$</span>SelfEfficacy, <span class="dt">use =</span> <span class="st">&quot;complete.obs&quot;</span>)
<span class="kw">cat</span>(<span class="kw">sprintf</span>(<span class="st">&quot;row deletion   : mu = %.3f  sd = %.3f  cor = %.3f </span><span class="ch">\n</span><span class="st">&quot;</span>,  mu, sd, pr))</code></pre></div>
<pre><code>## row deletion   : mu = 3.843  sd = 2.011  cor = -0.064</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">x  &lt;-<span class="st"> </span>dat<span class="op">$</span>HEFCE
x[<span class="kw">is.na</span>(x)] &lt;-<span class="st"> </span><span class="kw">mean</span>(x, <span class="dt">na.rm =</span> T)
mu &lt;-<span class="st"> </span><span class="kw">mean</span>(x, <span class="dt">na.rm =</span> T)
sd &lt;-<span class="st"> </span><span class="kw">sd</span>(x, <span class="dt">na.rm =</span> T)
pr &lt;-<span class="st"> </span><span class="kw">cor</span>(x, dat<span class="op">$</span>SelfEfficacy, <span class="dt">use =</span> <span class="st">&quot;complete.obs&quot;</span>)
<span class="kw">cat</span>(<span class="kw">sprintf</span>(<span class="st">&quot;mean imputation: mu = %.3f  sd = %.3f  cor = %.3f </span><span class="ch">\n</span><span class="st">&quot;</span>,  mu, sd, pr))</code></pre></div>
<pre><code>## mean imputation: mu = 3.843  sd = 1.885  cor = -0.060</code></pre>
<p>Such underestimation of variability would also transfer to underestimating uncertainty in our predictions or parameter estimates, which is a serious problem if the goal is to understand uncertainty and not just point estimates.</p>
</div>
<div id="single-imputation-with-prediction" class="section level3">
<h3><span class="header-section-number">13.4.2</span> Single imputation with prediction</h3>
<p>A generalization of imputation with the mean is to predict the missing value using all other available data. This translates to a fully fledged predictive modelling problem with all the complexity of model selection, etc.</p>
<p>We demonstrate the approach by using logistic regression to predict missing dropout values from all variables without missing values:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">tmp &lt;-<span class="st"> </span>dat[,<span class="kw">c</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">4</span>, <span class="dv">6</span>)]
mod &lt;-<span class="st"> </span><span class="kw">glm</span>(dropOUToriginal <span class="op">~</span><span class="st"> </span>., tmp, <span class="dt">family =</span> <span class="st">&quot;binomial&quot;</span>)  <span class="co"># prediction model</span>
idx &lt;-<span class="st"> </span><span class="kw">is.na</span>(dat<span class="op">$</span>dropOUToriginal)

pre &lt;-<span class="st"> </span><span class="kw">ifelse</span>(<span class="kw">predict</span>(mod, <span class="dt">newdata =</span> tmp, <span class="dt">type =</span> <span class="st">&quot;response&quot;</span>) <span class="op">&gt;</span><span class="st"> </span><span class="fl">0.5</span>, <span class="st">&quot;yes&quot;</span>, <span class="st">&quot;no&quot;</span>) <span class="co"># predicted values</span>
tmp<span class="op">$</span>dropOUToriginal &lt;-<span class="st"> </span><span class="kw">ifelse</span>(idx, pre, dat<span class="op">$</span>dropOUToriginal)

<span class="co"># accuracy on missing values, compared to relative freq.</span>
y &lt;-<span class="st"> </span>tmp<span class="op">$</span>dropOUToriginal[idx] <span class="op">==</span><span class="st"> </span>dat<span class="op">$</span>dropOUTretrieved[idx]

<span class="kw">cat</span>(<span class="kw">sprintf</span>(<span class="st">&quot;%.2f +/- %.3f</span><span class="ch">\n</span><span class="st">&quot;</span>, <span class="kw">mean</span>(y), <span class="kw">stderr</span>(y)))</code></pre></div>
<pre><code>## 0.65 +/- 0.016</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">round</span>(<span class="kw">mean</span>(dat<span class="op">$</span>dropOUTretrieved[idx] <span class="op">==</span><span class="st"> &quot;no&quot;</span>), <span class="dv">2</span>)</code></pre></div>
<pre><code>## [1] 0.56</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># </span></code></pre></div>
<p>The predictions are not very accurate, but still above the relative frequency of the mode of dropout (<em>no</em>), which is 0.56.</p>
<p>We could take the extra step of imputing the remaining variables with mean/mode and using them in the model as well:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mode &lt;-<span class="st"> </span><span class="cf">function</span>(x) {
  ux &lt;-<span class="st"> </span><span class="kw">unique</span>(x)
  ux[<span class="kw">which.max</span>(<span class="kw">tabulate</span>(<span class="kw">match</span>(x, ux)))]
}

impute &lt;-<span class="st"> </span><span class="cf">function</span>(x) {
  <span class="cf">if</span> (<span class="kw">is.numeric</span>(x)) {
    <span class="kw">return</span> (<span class="kw">ifelse</span>(<span class="kw">is.na</span>(x), <span class="kw">mean</span>(x, <span class="dt">na.rm =</span> T), x))
  } <span class="cf">else</span> {
    <span class="kw">return</span>(<span class="kw">ifelse</span>(<span class="kw">is.na</span>(x), <span class="kw">mode</span>(x[<span class="op">!</span><span class="kw">is.na</span>(x)]), x))
  }
}
tmp &lt;-<span class="st"> </span>dat[,<span class="op">-</span><span class="dv">5</span>]
<span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">ncol</span>(tmp)) {
  <span class="cf">if</span> (<span class="kw">names</span>(tmp)[i] <span class="op">!=</span><span class="st"> &quot;dropOUToriginal&quot;</span>) tmp[,i] &lt;-<span class="st"> </span><span class="kw">impute</span>(tmp[,i])
}

mod &lt;-<span class="st"> </span><span class="kw">glm</span>(dropOUToriginal <span class="op">~</span><span class="st"> </span>., tmp, <span class="dt">family =</span> <span class="st">&quot;binomial&quot;</span>)  <span class="co"># prediction model</span>
idx &lt;-<span class="st"> </span><span class="kw">is.na</span>(dat<span class="op">$</span>dropOUToriginal)

pre &lt;-<span class="st"> </span><span class="kw">ifelse</span>(<span class="kw">predict</span>(mod, <span class="dt">newdata =</span> tmp, <span class="dt">type =</span> <span class="st">&quot;response&quot;</span>) <span class="op">&gt;</span><span class="st"> </span><span class="fl">0.5</span>, <span class="st">&quot;yes&quot;</span>, <span class="st">&quot;no&quot;</span>) <span class="co"># predicted values</span>
tmp<span class="op">$</span>dropOUToriginal &lt;-<span class="st"> </span><span class="kw">ifelse</span>(idx, pre, dat<span class="op">$</span>dropOUToriginal)

<span class="co"># accuracy on missing values, compared to relative freq.</span>
y &lt;-<span class="st"> </span>tmp<span class="op">$</span>dropOUToriginal[idx] <span class="op">==</span><span class="st"> </span>dat<span class="op">$</span>dropOUTretrieved[idx]
<span class="kw">cat</span>(<span class="kw">sprintf</span>(<span class="st">&quot;%.2f +/- %.3f</span><span class="ch">\n</span><span class="st">&quot;</span>, <span class="kw">mean</span>(y), <span class="kw">stderr</span>(y)))</code></pre></div>
<pre><code>## 0.67 +/- 0.016</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># </span></code></pre></div>
<p>Using the other variables did not lead to an improvement in accuracy. Either there is nothing more we can extract from the given variables or we would need to pick a better model or engineer better input variables.</p>
<p>The advantage of a predictive approach over imputation with mean is that, if we manage to capture the relationships in the data, imputation with a predictive model can account for bias caused by a MAR missingness regime! However, as with any single imputation method, we will still underestimate variability. In order to fully capture variability, we must use a distribution or multiple values - multiple imputation.</p>
</div>
<div id="multiple-imputation" class="section level3">
<h3><span class="header-section-number">13.4.3</span> Multiple imputation</h3>
<p>The imputation with prediction example from the previous section illustrates how imputation is in itself a prediction problem. Even more, it is potentially a set of many prediction problems, because we have to account for every possible combination of missing values when predicting the missing value of one variable. In the previous example we circumvented that problem by using imputation with mean, but that is not an ideal solution as it underestimates variability. To solve the problem in general, we would have to specify a generative model for all the variables. That is, to specify the data generation process in a way that can be used to generate values for missing data regardless of the pattern of missingness for that observation. This also solves the variability underestimation problem of single imputation - if we have a distribution over all possible missing values, we have a full characterization of the uncertainty or we can generate many datasets which as a whole capture that uncertainty (hence the term <em>multiple</em> imputation)</p>
<p>Specifying a full generative model is the most difficult of all modelling problems. A common approach is to use a multivariate normal distribution, because under that modelling assumption all the conditional distributions are also multivariate normal and it is relatively easy to derive their parameters. In R, we can find it implemented in the <a href="https://cran.r-project.org/web/packages/Amelia/index.html">amelia</a> package. However, this approach can be used out-of-the-box only if all of our data are numerical.</p>
<p>A popular alternative is multiple imputation with chained equations (MICE):</p>
<ol style="list-style-type: decimal">
<li><p>First, we specify the type of model we want to use for each type of variable - packages that implement MICE often come with pre-specified models, such as <em>if numerical, use linear regression</em>, <em>if categorical, use logistic regression</em>, etc.</p></li>
<li><p>Initially we replace all missing values with some simple procedure, such as mean imputation.</p></li>
<li><p>We then cycle through all the variables and for each variable use its model from (1) to predict the values of its missing values from all the other variables. After we complete this step, all data that were initially missing values have received new values. We never change the observed values.</p></li>
<li><p>We repeat (3) for several iterations. We do this to <em>forget</em> where we started so that the imputed values reflect the relationships in the data and not the initial, possibly very poor imputation.</p></li>
</ol>
<p>This generates one imputed dataset. We repeate steps (2-4) several times to generate multiple imputed datasets. Any further analysis we do should be based on all those datasets, because they capture the variability in the missing data - our uncertainty about what the missing data values might be.</p>
<p>Those familiar with Gibbs sampling, a popular inference algorithm in Bayesian statistics, will quickly recognize that MICE is based on the same ideas. A full generative model can be specified by specifying just the full conditional distributions (a distribution of a variable with all other variables known) and we can use Markov Chain Monte Carlo (in this case Gibbs sampling) to generate samples from the posterior of that model.</p>
<p>Implementations of MICE typically do all the work for us. We only need to specify the number of imputed datasets we need. We’ll use the R package <a href="https://cran.r-project.org/web/packages/mice/index.html">mice</a> to create 50 imputed datasets for our data. We then use logistic regression to predict dropout for each of those datasets and combine the predictions of all the models into a final prediction of dropout. First, we impute the predictor variables:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">tmp &lt;-<span class="st"> </span>dat[,<span class="op">-</span><span class="kw">c</span>(<span class="dv">5</span>,<span class="dv">6</span>)]
tgt &lt;-<span class="st"> </span>dat<span class="op">$</span>dropOUToriginal

<span class="co"># multiple imputation</span>

<span class="kw">library</span>(mice)</code></pre></div>
<pre><code>## Warning: package &#39;mice&#39; was built under R version 3.5.3</code></pre>
<pre><code>## Loading required package: lattice</code></pre>
<pre><code>## 
## Attaching package: &#39;lattice&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:UpSetR&#39;:
## 
##     histogram</code></pre>
<pre><code>## 
## Attaching package: &#39;mice&#39;</code></pre>
<pre><code>## The following objects are masked from &#39;package:base&#39;:
## 
##     cbind, rbind</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co">#imputed_tmp &lt;- mice(tmp, m = 50, seed = 0)</span>
<span class="co">#saveRDS(imputed_tmp, &quot;./data/imputed_tmp.rds&quot;)</span>
imputed_tmp &lt;-<span class="st"> </span><span class="kw">readRDS</span>(<span class="st">&quot;./data/imputed_tmp.rds&quot;</span>) <span class="co"># we load the precomputed data to save time</span></code></pre></div>
<p>We can inspect which types of models were used:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">print</span>(imputed_tmp<span class="op">$</span>method)</code></pre></div>
<pre><code>##    GCSEgrade       Course  Disposition SelfEfficacy       Gender 
##           &quot;&quot;           &quot;&quot;           &quot;&quot;           &quot;&quot;     &quot;logreg&quot; 
##     Language          EMA    Ethnicity          LPN        HEFCE 
##    &quot;polyreg&quot;     &quot;logreg&quot;    &quot;polyreg&quot;     &quot;logreg&quot;        &quot;pmm&quot; 
##       uniFAM 
##    &quot;polyreg&quot;</code></pre>
<p>Now we iterate through all the imputed datasets, predict outcome and pool the predictions:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">pre &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, <span class="kw">length</span>(x))
<span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">50</span>) {
  df &lt;-<span class="st"> </span><span class="kw">complete</span>(imputed_tmp, <span class="dv">1</span>)
  df<span class="op">$</span>tgt &lt;-<span class="st"> </span>tgt <span class="co"># add dropout</span>
  mod &lt;-<span class="st"> </span><span class="kw">glm</span>(tgt <span class="op">~</span><span class="st"> </span>., df, <span class="dt">family =</span> <span class="st">&quot;binomial&quot;</span>)
  idx &lt;-<span class="st"> </span><span class="kw">is.na</span>(dat<span class="op">$</span>dropOUToriginal)
  pre &lt;-<span class="st"> </span>pre <span class="op">+</span><span class="st"> </span><span class="kw">predict</span>(mod, <span class="dt">newdata =</span> df, <span class="dt">type =</span> <span class="st">&quot;response&quot;</span>) <span class="op">/</span><span class="st"> </span><span class="dv">50</span>
}

pre &lt;-<span class="st"> </span><span class="kw">ifelse</span>(pre <span class="op">&gt;</span><span class="st"> </span><span class="fl">0.5</span>, <span class="st">&quot;yes&quot;</span>, <span class="st">&quot;no&quot;</span>) <span class="co"># predicted values</span>
tmp<span class="op">$</span>tgt &lt;-<span class="st"> </span><span class="kw">ifelse</span>(idx, pre, dat<span class="op">$</span>dropOUToriginal)

<span class="co"># accuracy on missing values, compared to relative freq.</span>
y &lt;-<span class="st"> </span>tmp<span class="op">$</span>tgt[idx] <span class="op">==</span><span class="st"> </span>dat<span class="op">$</span>dropOUTretrieved[idx]
<span class="kw">cat</span>(<span class="kw">sprintf</span>(<span class="st">&quot;%.2f +/- %.3f</span><span class="ch">\n</span><span class="st">&quot;</span>, <span class="kw">mean</span>(y), <span class="kw">stderr</span>(y)))</code></pre></div>
<pre><code>## 0.65 +/- 0.016</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># </span></code></pre></div>
<p>In this case MICE doesn’t seem to have helped. In general, MICE is a very robust procedure and it performs really well in many different situations. The main issue with MICE is that it is computationally very intensive.</p>
</div>
</div>
<div id="summary" class="section level2">
<h2><span class="header-section-number">13.5</span> Summary</h2>
<p>If we are to take away one thing from this chapter it should be that dealing with missing data is not an easy problem. Data will rarely be missing in a <em>nice</em> (completely random) way, so if we want to resort to more simple removal or imputation techniques, we must put reasonable effort into determining whether or not the dependencies between observed data and missingness are a cause for concern. If strong dependencies exist and/or there is a lot of data missing, the missing data problem becomes a prediction problem (or a series of prediction problems). We should also be aware of the possibility that missingness depends on the missing value in a way that can’t be explained by observed variables. This can also cause bias in our analyses and we will not be able to detect it unless we get our hands on some of the missing values.</p>
<p>Note that in this chapter we focused on standard tabular data. There are many other types of data, such as time-series data, spatial data, images, sound, graphs, etc. The basic principles remain unchanged. We must be aware of the missingness mechanism and introducing bias. We can deal with missing values either by removing observations/variables or by imputing them. However, different, sometimes additional models and techniques will be appropriate. For example, temporal and spatial data lend themselves to interpolation of missing values.</p>
</div>
<div id="further-reading-and-references-5" class="section level2">
<h2><span class="header-section-number">13.6</span> Further reading and references</h2>
<ul>
<li><p>A gentle introduction from a practitioners perspective: Blankers, M., Koeter, M. W., &amp; Schippers, G. M. (2010). Missing data approaches in eHealth research: simulation study and a tutorial for nonmathematically inclined researchers. Journal of medical Internet research, 12(5), e54.</p></li>
<li><p>A great book on basic and some advance techniques: Allison, P. D. (2001). Missing data (Vol. 136). Sage publications.</p></li>
<li><p>Understanding multiple imputation with chained equations (in R): Buuren, S. V., &amp; Groothuis-Oudshoorn, K. (2010). mice: Multivariate imputation by chained equations in R. Journal of statistical software, 1-68.</p></li>
</ul>
</div>
<div id="learning-outcomes-5" class="section level2">
<h2><span class="header-section-number">13.7</span> Learning outcomes</h2>
<p>Data science students should work towards obtaining the knowledge and the skills that enable them to:</p>
<ul>
<li>Reproduce the techniques demonstrated in this chapter using their language/tool of choice.</li>
<li>Analyze the severity of their missing data problem.</li>
<li>Recognize when a technique is appropriate and what are its limitations.</li>
</ul>
</div>
<div id="practice-problems-5" class="section level2">
<h2><span class="header-section-number">13.8</span> Practice problems</h2>
<ol style="list-style-type: decimal">
<li>We prepared a subset of the <a href="https://www.kaggle.com/ajinkyablaze/football-manager-data/downloads/football-manager-data.zip/2">Football Manager Players</a> dataset that contains 1000 randomly selected 19-year old players, their playing position, height, and 10 other attributes (<em>football-manager-complete.rds</em>). We then introduced missing values to this data based on various missingness mechanisms (<em>football-manager-missing.rds</em>) and in a way that could also have a reasonable practical explanation. Your task is to:</li>
</ol>
<ol style="list-style-type: lower-alpha">
<li>Identify and, as much as it is possible, characterize missingness mechanisms and patterns in the data.</li>
<li>Use information from (a) to impute missing values in all numerical variables (all variables except <em>PositionsDesc</em>, which can be ignored throughout this problem).</li>
<li>Estimate the mean of all numerical variables.</li>
<li>Only once you have completed your analysis, use <em>football-manager-complete.rds</em> to compare your estimated means with column averages on the complete dataset. Discuss which mechanisms you correctly detected and characterized. Discuss the discrepancies between your estimates and actual column means - was there anything you could have done better?</li>
</ol>
<ol start="2" style="list-style-type: decimal">
<li>In the same setting as (1) try to predict players’ playing position (<em>PositionsDesc</em>) using <em>football-manager-missing.rds</em>. once you have completed your analysis, use <em>football-manager-complete.rds</em> to evaluate your model on the observations that had missing playing position. Discuss what you could have done better.</li>
</ol>

<div id="refs" class="references">
<div>
<p>Noyes, Katherine. 2008. “Docker: A ’Shipping Container’ for Linux Code.” <a href="https://www.linux.com/news/docker-shipping-container-linux-code" class="uri">https://www.linux.com/news/docker-shipping-container-linux-code</a>.</p>
</div>
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="predictive-modelling-feature-selection.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
