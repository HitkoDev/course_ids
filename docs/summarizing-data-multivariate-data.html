<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Chapter 8 Summarizing data - multivariate data | Introduction to data science</title>
  <meta name="description" content="Course notes">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="Chapter 8 Summarizing data - multivariate data | Introduction to data science" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://fri-datascience.github.io/course_itds/" />
  
  <meta property="og:description" content="Course notes" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 8 Summarizing data - multivariate data | Introduction to data science" />
  
  <meta name="twitter:description" content="Course notes" />
  

<meta name="author" content="Slavko Žitnik, Tomaž Curk, Erik Štrumbelj">


<meta name="date" content="2019-07-18">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="dynamic-reports-and-reproducibility.html">
<link rel="next" href="relational-databases.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Introduction to data science</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="python-programming-language.html"><a href="python-programming-language.html"><i class="fa fa-check"></i><b>1</b> Python programming language</a><ul>
<li class="chapter" data-level="1.1" data-path="python-programming-language.html"><a href="python-programming-language.html#basic-characteristics"><i class="fa fa-check"></i><b>1.1</b> Basic characteristics</a></li>
<li class="chapter" data-level="1.2" data-path="python-programming-language.html"><a href="python-programming-language.html#why-python"><i class="fa fa-check"></i><b>1.2</b> Why Python?</a></li>
<li class="chapter" data-level="1.3" data-path="python-programming-language.html"><a href="python-programming-language.html#setting-up-the-environment"><i class="fa fa-check"></i><b>1.3</b> Setting up the environment</a><ul>
<li class="chapter" data-level="1.3.1" data-path="python-programming-language.html"><a href="python-programming-language.html#anaconda-distribution-installation"><i class="fa fa-check"></i><b>1.3.1</b> Anaconda distribution installation</a></li>
<li class="chapter" data-level="1.3.2" data-path="python-programming-language.html"><a href="python-programming-language.html#pure-python-distribution-installation"><i class="fa fa-check"></i><b>1.3.2</b> Pure Python distribution installation</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="python-programming-language.html"><a href="python-programming-language.html#installing-dependencies"><i class="fa fa-check"></i><b>1.4</b> Installing dependencies</a></li>
<li class="chapter" data-level="1.5" data-path="python-programming-language.html"><a href="python-programming-language.html#jupyter-notebooks"><i class="fa fa-check"></i><b>1.5</b> Jupyter notebooks</a><ul>
<li class="chapter" data-level="1.5.1" data-path="python-programming-language.html"><a href="python-programming-language.html#running-a-jupyter-notebook"><i class="fa fa-check"></i><b>1.5.1</b> Running a Jupyter notebook</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="python-programming-language.html"><a href="python-programming-language.html#short-introduction-to-python"><i class="fa fa-check"></i><b>1.6</b> Short introduction to Python</a><ul>
<li class="chapter" data-level="" data-path="python-programming-language.html"><a href="python-programming-language.html#basics"><i class="fa fa-check"></i>Basics</a></li>
<li class="chapter" data-level="1.6.1" data-path="python-programming-language.html"><a href="python-programming-language.html#control-flow-operations"><i class="fa fa-check"></i><b>1.6.1</b> Control flow operations</a></li>
<li class="chapter" data-level="1.6.2" data-path="python-programming-language.html"><a href="python-programming-language.html#functions"><i class="fa fa-check"></i><b>1.6.2</b> Functions</a></li>
<li class="chapter" data-level="1.6.3" data-path="python-programming-language.html"><a href="python-programming-language.html#classes-and-objects"><i class="fa fa-check"></i><b>1.6.3</b> Classes and objects</a></li>
<li class="chapter" data-level="1.6.4" data-path="python-programming-language.html"><a href="python-programming-language.html#python-ides-and-code-editors"><i class="fa fa-check"></i><b>1.6.4</b> Python IDE’s and code editors</a></li>
</ul></li>
<li class="chapter" data-level="1.7" data-path="python-programming-language.html"><a href="python-programming-language.html#python-ecosystem-for-data-science"><i class="fa fa-check"></i><b>1.7</b> Python ecosystem for Data Science</a></li>
<li class="chapter" data-level="1.8" data-path="python-programming-language.html"><a href="python-programming-language.html#further-reading-and-references"><i class="fa fa-check"></i><b>1.8</b> Further reading and references</a></li>
<li class="chapter" data-level="1.9" data-path="python-programming-language.html"><a href="python-programming-language.html#learning-outcomes"><i class="fa fa-check"></i><b>1.9</b> Learning outcomes</a></li>
<li class="chapter" data-level="1.10" data-path="python-programming-language.html"><a href="python-programming-language.html#practice-problems"><i class="fa fa-check"></i><b>1.10</b> Practice problems</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="source-code-control.html"><a href="source-code-control.html"><i class="fa fa-check"></i><b>2</b> Source code control</a></li>
<li class="chapter" data-level="3" data-path="docker-container-platform.html"><a href="docker-container-platform.html"><i class="fa fa-check"></i><b>3</b> Docker container platform</a><ul>
<li class="chapter" data-level="3.1" data-path="docker-container-platform.html"><a href="docker-container-platform.html#why-docker"><i class="fa fa-check"></i><b>3.1</b> Why Docker?</a></li>
<li class="chapter" data-level="3.2" data-path="docker-container-platform.html"><a href="docker-container-platform.html#setting-up-the-environment-1"><i class="fa fa-check"></i><b>3.2</b> Setting up the environment</a></li>
<li class="chapter" data-level="3.3" data-path="docker-container-platform.html"><a href="docker-container-platform.html#short-introduction-to-docker"><i class="fa fa-check"></i><b>3.3</b> Short introduction to Docker</a><ul>
<li class="chapter" data-level="3.3.1" data-path="docker-container-platform.html"><a href="docker-container-platform.html#basics-1"><i class="fa fa-check"></i><b>3.3.1</b> Basics</a></li>
<li class="chapter" data-level="3.3.2" data-path="docker-container-platform.html"><a href="docker-container-platform.html#docker-application-example"><i class="fa fa-check"></i><b>3.3.2</b> Docker application example</a></li>
<li class="chapter" data-level="3.3.3" data-path="docker-container-platform.html"><a href="docker-container-platform.html#volumes"><i class="fa fa-check"></i><b>3.3.3</b> Volumes</a></li>
<li class="chapter" data-level="3.3.4" data-path="docker-container-platform.html"><a href="docker-container-platform.html#docker-application-example-with-multiple-services"><i class="fa fa-check"></i><b>3.3.4</b> Docker application example with multiple services</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="docker-container-platform.html"><a href="docker-container-platform.html#further-reading-and-references-1"><i class="fa fa-check"></i><b>3.4</b> Further reading and references</a></li>
<li class="chapter" data-level="3.5" data-path="docker-container-platform.html"><a href="docker-container-platform.html#learning-outcomes-1"><i class="fa fa-check"></i><b>3.5</b> Learning outcomes</a></li>
<li class="chapter" data-level="3.6" data-path="docker-container-platform.html"><a href="docker-container-platform.html#practice-problems-1"><i class="fa fa-check"></i><b>3.6</b> Practice problems</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="web-scraping.html"><a href="web-scraping.html"><i class="fa fa-check"></i><b>4</b> Web scraping</a></li>
<li class="chapter" data-level="5" data-path="summarizing-data-the-basics.html"><a href="summarizing-data-the-basics.html"><i class="fa fa-check"></i><b>5</b> Summarizing data - the basics</a><ul>
<li class="chapter" data-level="5.1" data-path="summarizing-data-the-basics.html"><a href="summarizing-data-the-basics.html#descriptive-statistics-for-univariate-distributions"><i class="fa fa-check"></i><b>5.1</b> Descriptive statistics for univariate distributions</a><ul>
<li class="chapter" data-level="5.1.1" data-path="summarizing-data-the-basics.html"><a href="summarizing-data-the-basics.html#central-tendency"><i class="fa fa-check"></i><b>5.1.1</b> Central tendency</a></li>
<li class="chapter" data-level="5.1.2" data-path="summarizing-data-the-basics.html"><a href="summarizing-data-the-basics.html#dispersion"><i class="fa fa-check"></i><b>5.1.2</b> Dispersion</a></li>
<li class="chapter" data-level="5.1.3" data-path="summarizing-data-the-basics.html"><a href="summarizing-data-the-basics.html#skewness-and-kurtosis"><i class="fa fa-check"></i><b>5.1.3</b> Skewness and kurtosis</a></li>
<li class="chapter" data-level="5.1.4" data-path="summarizing-data-the-basics.html"><a href="summarizing-data-the-basics.html#nominal-variables"><i class="fa fa-check"></i><b>5.1.4</b> Nominal variables</a></li>
<li class="chapter" data-level="5.1.5" data-path="summarizing-data-the-basics.html"><a href="summarizing-data-the-basics.html#testing-the-shape-of-a-distribution"><i class="fa fa-check"></i><b>5.1.5</b> Testing the shape of a distribution</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="summarizing-data-the-basics.html"><a href="summarizing-data-the-basics.html#descriptive-statistics-for-bivariate-distributions"><i class="fa fa-check"></i><b>5.2</b> Descriptive statistics for bivariate distributions</a></li>
<li class="chapter" data-level="5.3" data-path="summarizing-data-the-basics.html"><a href="summarizing-data-the-basics.html#further-reading-and-references-2"><i class="fa fa-check"></i><b>5.3</b> Further reading and references</a></li>
<li class="chapter" data-level="5.4" data-path="summarizing-data-the-basics.html"><a href="summarizing-data-the-basics.html#learning-outcomes-2"><i class="fa fa-check"></i><b>5.4</b> Learning outcomes</a></li>
<li class="chapter" data-level="5.5" data-path="summarizing-data-the-basics.html"><a href="summarizing-data-the-basics.html#practice-problems-2"><i class="fa fa-check"></i><b>5.5</b> Practice problems</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="summarizing-data-visualization.html"><a href="summarizing-data-visualization.html"><i class="fa fa-check"></i><b>6</b> Summarizing data - visualization</a><ul>
<li class="chapter" data-level="6.1" data-path="summarizing-data-visualization.html"><a href="summarizing-data-visualization.html#histograms-and-density-plots"><i class="fa fa-check"></i><b>6.1</b> Histograms and density plots</a></li>
<li class="chapter" data-level="6.2" data-path="summarizing-data-visualization.html"><a href="summarizing-data-visualization.html#bar-plot"><i class="fa fa-check"></i><b>6.2</b> Bar plot</a></li>
<li class="chapter" data-level="6.3" data-path="summarizing-data-visualization.html"><a href="summarizing-data-visualization.html#pie-chart"><i class="fa fa-check"></i><b>6.3</b> Pie chart</a></li>
<li class="chapter" data-level="6.4" data-path="summarizing-data-visualization.html"><a href="summarizing-data-visualization.html#scatterplot"><i class="fa fa-check"></i><b>6.4</b> Scatterplot</a></li>
<li class="chapter" data-level="6.5" data-path="summarizing-data-visualization.html"><a href="summarizing-data-visualization.html#d-density-plot"><i class="fa fa-check"></i><b>6.5</b> 2D density plot</a></li>
<li class="chapter" data-level="6.6" data-path="summarizing-data-visualization.html"><a href="summarizing-data-visualization.html#boxplot"><i class="fa fa-check"></i><b>6.6</b> Boxplot</a></li>
<li class="chapter" data-level="6.7" data-path="summarizing-data-visualization.html"><a href="summarizing-data-visualization.html#violin-plot"><i class="fa fa-check"></i><b>6.7</b> Violin plot</a></li>
<li class="chapter" data-level="6.8" data-path="summarizing-data-visualization.html"><a href="summarizing-data-visualization.html#correlogram"><i class="fa fa-check"></i><b>6.8</b> Correlogram</a></li>
<li class="chapter" data-level="6.9" data-path="summarizing-data-visualization.html"><a href="summarizing-data-visualization.html#a-comprehensive-summary"><i class="fa fa-check"></i><b>6.9</b> A comprehensive summary</a></li>
<li class="chapter" data-level="6.10" data-path="summarizing-data-visualization.html"><a href="summarizing-data-visualization.html#further-reading-and-references-3"><i class="fa fa-check"></i><b>6.10</b> Further reading and references</a></li>
<li class="chapter" data-level="6.11" data-path="summarizing-data-visualization.html"><a href="summarizing-data-visualization.html#learning-outcomes-3"><i class="fa fa-check"></i><b>6.11</b> Learning outcomes</a></li>
<li class="chapter" data-level="6.12" data-path="summarizing-data-visualization.html"><a href="summarizing-data-visualization.html#practice-problems-3"><i class="fa fa-check"></i><b>6.12</b> Practice problems</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="dynamic-reports-and-reproducibility.html"><a href="dynamic-reports-and-reproducibility.html"><i class="fa fa-check"></i><b>7</b> Dynamic reports and reproducibility</a></li>
<li class="chapter" data-level="8" data-path="summarizing-data-multivariate-data.html"><a href="summarizing-data-multivariate-data.html"><i class="fa fa-check"></i><b>8</b> Summarizing data - multivariate data</a><ul>
<li class="chapter" data-level="8.1" data-path="summarizing-data-multivariate-data.html"><a href="summarizing-data-multivariate-data.html#principal-component-analysis-pca"><i class="fa fa-check"></i><b>8.1</b> Principal Component Analysis (PCA)</a></li>
<li class="chapter" data-level="8.2" data-path="summarizing-data-multivariate-data.html"><a href="summarizing-data-multivariate-data.html#multi-dimensional-scaling-mds"><i class="fa fa-check"></i><b>8.2</b> Multi-dimensional scaling (MDS)</a></li>
<li class="chapter" data-level="8.3" data-path="summarizing-data-multivariate-data.html"><a href="summarizing-data-multivariate-data.html#t-distributed-stochastic-neighbor-embedding-t-sne"><i class="fa fa-check"></i><b>8.3</b> t-Distributed Stochastic Neighbor Embedding (t-SNE)</a></li>
<li class="chapter" data-level="8.4" data-path="summarizing-data-multivariate-data.html"><a href="summarizing-data-multivariate-data.html#clustering"><i class="fa fa-check"></i><b>8.4</b> Clustering</a><ul>
<li class="chapter" data-level="8.4.1" data-path="summarizing-data-multivariate-data.html"><a href="summarizing-data-multivariate-data.html#k-means-clustering"><i class="fa fa-check"></i><b>8.4.1</b> k-means clustering</a></li>
<li class="chapter" data-level="8.4.2" data-path="summarizing-data-multivariate-data.html"><a href="summarizing-data-multivariate-data.html#determining-the-number-of-clusters"><i class="fa fa-check"></i><b>8.4.2</b> Determining the number of clusters</a></li>
<li class="chapter" data-level="8.4.3" data-path="summarizing-data-multivariate-data.html"><a href="summarizing-data-multivariate-data.html#agglomerative-hierarchical-clustering"><i class="fa fa-check"></i><b>8.4.3</b> Agglomerative hierarchical clustering</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="summarizing-data-multivariate-data.html"><a href="summarizing-data-multivariate-data.html#further-reading-and-references-4"><i class="fa fa-check"></i><b>8.5</b> Further reading and references</a></li>
<li class="chapter" data-level="8.6" data-path="summarizing-data-multivariate-data.html"><a href="summarizing-data-multivariate-data.html#learning-outcomes-4"><i class="fa fa-check"></i><b>8.6</b> Learning outcomes</a></li>
<li class="chapter" data-level="8.7" data-path="summarizing-data-multivariate-data.html"><a href="summarizing-data-multivariate-data.html#practice-problems-4"><i class="fa fa-check"></i><b>8.7</b> Practice problems</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="relational-databases.html"><a href="relational-databases.html"><i class="fa fa-check"></i><b>9</b> Relational databases</a></li>
<li class="chapter" data-level="10" data-path="predictive-modelling.html"><a href="predictive-modelling.html"><i class="fa fa-check"></i><b>10</b> Predictive modelling</a></li>
<li class="chapter" data-level="11" data-path="large-datasets-and-non-relational-databases.html"><a href="large-datasets-and-non-relational-databases.html"><i class="fa fa-check"></i><b>11</b> Large datasets and non relational databases</a></li>
<li class="chapter" data-level="12" data-path="parallel-and-distributed-computing.html"><a href="parallel-and-distributed-computing.html"><i class="fa fa-check"></i><b>12</b> Parallel and distributed computing</a></li>
<li class="chapter" data-level="13" data-path="other-important-topics.html"><a href="other-important-topics.html"><i class="fa fa-check"></i><b>13</b> Other important topics</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>      
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Introduction to data science</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="summarizing-data---multivariate-data" class="section level1">
<h1><span class="header-section-number">Chapter 8</span> Summarizing data - multivariate data</h1>
<p>Data summarization - the art of conveying the same information in less time/space or conveying more information in the same amount of time/space. Data summarization is typically numerical or visual (or combined) and is a key skill in data analysis as we use it to provide insights both to others and to ourselves. Data summarization is also an important component of exploratory data analysis. In this chapter we will focus on the some advanced techniques for multivariate data.</p>
<p>We will be using R and ggplot2, but the contents of this chapter are meant to be tool-agnostic. Readers should use the programming language and tools that they are most comfortable with. However, do not sacrifice expresiveness or profesionallism for the sake of convenience - if your current toolbox limits you in any way, learn new tools!</p>
<p>As we already mentioned, we humans prefer low-dimensional representations of information, which makes sense, because we spend most of our time in a 3-dimensional world. The natural way (and the only way) of dealing with high-dimensional data is therefore to map it to fewer dimensions. Often, a lower-dimensional representation can offer useful information and sometimes it can even be done without substantial loss of information. Dimensionality reduction is often also used as a preprocessing step before prediction or inference - fewer dimensions reduce computation times and simplify interpretation of input variables’ importance.</p>
<p>Here, we will discuss some of the most common techniques.</p>
<div id="principal-component-analysis-pca" class="section level2">
<h2><span class="header-section-number">8.1</span> Principal Component Analysis (PCA)</h2>
<p>PCA is typicaly the first method we use and often the only method, due to its simplicity, interpretability, and speed. PCA is based on an orthogonal transformation of possibly correlated variables into new linearly uncorrelated variables which we call principal components. The first principal component accounts for as much of the variability as possible and each next component in turn has the highest possible variance, conditional to being orthogonal to all the previous components.</p>
<p>The proportion of variance explained serves as an indicator of the importance of that principal component. If some principal components explain only a small part of the variability, we can discard them and therefore reduce the dimensionality of the representation. Because PCA produces orthogonal (uncorrelated) variables, we can use it as a preprocessing step before linear modelling, as it will, if we can interpret the principal components, simplify the interpretation of the linear model.</p>
<p>Note that PCA is sensitive to the relative scales of the variables. That is, scaling a variable would increase its variance and make it a priority for the principal components to include. Before applying PCA we should scale the variables according to their practical scale or, if we have no preference, standardize them so that they have equal relative importance.</p>
<p>We demonstrate PCA on a dataset of decathlon results. We hypothesize that decathlon results might be explained by a smaller set of dimensions that correspond to the athlete’s strength, explosiveness, and stamina. We’ll use the decathlon dataset that can be found in the <em>FactoMineR</em> R package. First, we load the data:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">dat &lt;-<span class="st"> </span><span class="kw">read.csv</span>(<span class="st">&quot;./data/decathlon.csv&quot;</span>)
dat &lt;-<span class="st"> </span>dat[,<span class="dv">2</span><span class="op">:</span><span class="dv">11</span>]
<span class="kw">print</span>(<span class="kw">summary</span>(dat))</code></pre></div>
<pre><code>##      X100m         Long.jump       Shot.put       High.jump    
##  Min.   :10.44   Min.   :6.61   Min.   :12.68   Min.   :1.850  
##  1st Qu.:10.85   1st Qu.:7.03   1st Qu.:13.88   1st Qu.:1.920  
##  Median :10.98   Median :7.30   Median :14.57   Median :1.950  
##  Mean   :11.00   Mean   :7.26   Mean   :14.48   Mean   :1.977  
##  3rd Qu.:11.14   3rd Qu.:7.48   3rd Qu.:14.97   3rd Qu.:2.040  
##  Max.   :11.64   Max.   :7.96   Max.   :16.36   Max.   :2.150  
##      X400m        X110m.hurdle       Discus        Pole.vault   
##  Min.   :46.81   Min.   :13.97   Min.   :37.92   Min.   :4.200  
##  1st Qu.:48.93   1st Qu.:14.21   1st Qu.:41.90   1st Qu.:4.500  
##  Median :49.40   Median :14.48   Median :44.41   Median :4.800  
##  Mean   :49.62   Mean   :14.61   Mean   :44.33   Mean   :4.762  
##  3rd Qu.:50.30   3rd Qu.:14.98   3rd Qu.:46.07   3rd Qu.:4.920  
##  Max.   :53.20   Max.   :15.67   Max.   :51.65   Max.   :5.400  
##     Javeline         X1500m     
##  Min.   :50.31   Min.   :262.1  
##  1st Qu.:55.27   1st Qu.:271.0  
##  Median :58.36   Median :278.1  
##  Mean   :58.32   Mean   :279.0  
##  3rd Qu.:60.89   3rd Qu.:285.1  
##  Max.   :70.52   Max.   :317.0</code></pre>
<p>Next, we prepare the data by standardizing the columns (we don’t want 1500m running to be more important just because it has a larger scale). We also take the negative value of the running events - we want all the variables to be of the type “larger is better” to simplify interpretation.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">dat[,<span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">4</span>,<span class="dv">5</span>,<span class="dv">6</span>,<span class="dv">10</span>)] &lt;-<span class="st"> </span><span class="op">-</span>dat[,<span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">4</span>,<span class="dv">5</span>,<span class="dv">6</span>,<span class="dv">10</span>)]
dat &lt;-<span class="st"> </span><span class="kw">scale</span>(dat)</code></pre></div>
<p>Now we are ready to do PCA:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">res &lt;-<span class="st"> </span><span class="kw">prcomp</span>(dat)
prop_explained &lt;-<span class="st"> </span>res<span class="op">$</span>sdev<span class="op">^</span><span class="dv">2</span> <span class="op">/</span><span class="st"> </span><span class="kw">sum</span>(res<span class="op">$</span>sdev<span class="op">^</span><span class="dv">2</span>)
<span class="kw">data.frame</span>(prop_explained, <span class="kw">cumsum</span>(prop_explained))</code></pre></div>
<pre><code>##    prop_explained cumsum.prop_explained.
## 1      0.32719055              0.3271906
## 2      0.17371310              0.5009037
## 3      0.14049167              0.6413953
## 4      0.10568504              0.7470804
## 5      0.06847735              0.8155577
## 6      0.05992687              0.8754846
## 7      0.04512353              0.9206081
## 8      0.03968766              0.9602958
## 9      0.02148149              0.9817773
## 10     0.01822275              1.0000000</code></pre>
<p>We can see that the first three principal components explain half of the variability in the data. And if we keep half of the principal components, we lose only about 13% of the variability. We could now argue that the latent dimensionality of this data is lower than the original 10 dimensions.</p>
<p>Of course, in order to produce a meaningful summary of the data, we must provide an explanation of what these principal components represent:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">round</span>(res<span class="op">$</span>rotation,<span class="dv">2</span>)</code></pre></div>
<pre><code>##                PC1   PC2   PC3   PC4   PC5   PC6   PC7   PC8   PC9  PC10
## X100m         0.43 -0.14  0.16 -0.04  0.37 -0.30  0.38 -0.46 -0.10  0.42
## Long.jump     0.41 -0.26  0.15 -0.10 -0.04 -0.31 -0.63 -0.02  0.48 -0.08
## Shot.put      0.34  0.45 -0.02 -0.19 -0.13  0.31  0.31 -0.31  0.43 -0.39
## High.jump    -0.32 -0.27  0.22 -0.13  0.67  0.47 -0.09 -0.13  0.24 -0.11
## X400m         0.38 -0.43 -0.11  0.03 -0.11  0.33 -0.12 -0.21 -0.55 -0.41
## X110m.hurdle  0.41 -0.17  0.08  0.28  0.20  0.10  0.36  0.71  0.15 -0.09
## Discus        0.31  0.46  0.04  0.25  0.13  0.45 -0.43  0.04 -0.15  0.45
## Pole.vault    0.03 -0.14  0.58 -0.54 -0.40  0.26  0.10  0.18 -0.08  0.28
## Javeline      0.15  0.24 -0.33 -0.69  0.37 -0.16 -0.11  0.30 -0.25 -0.09
## X1500m        0.03 -0.36 -0.66 -0.16 -0.19  0.30  0.08 -0.01  0.31  0.43</code></pre>
<p>As we described in the beginning, each principal component is a linear combination of the original variables. Because we standardized the variables, the corresponding coefficients serve as an indicator of importance. For example, pole vaulting and 1500m running have little importance in the first principal component, while these two disciplines have the highest weight in the fourth principal components.</p>
<p>The meaning of principal components can more easily be discerend by plotting pairs of components, their relationship with the original variables, and individual observations. Typically, we first plot the first two principal components - components which explain most of the variance:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">biplot</span>(res)</code></pre></div>
<p><img src="Summarizing-data-03_files/figure-html/unnamed-chunk-5-1.png" width="672" /> This reveals two major axes in the data - (1) athletes that have high PC1 and PC2 are better at javeline, discus, and shotput (events that require strength) and worse at high jump (events where having a lot of muscle mass is detrimental) and (2) atletes that have high PC1 but low PC2 are better at all the shorter running events and long jump (events that depend on running speed). Pole vaulting and 1500m running results are not explained well by the first two principal components.</p>
</div>
<div id="multi-dimensional-scaling-mds" class="section level2">
<h2><span class="header-section-number">8.2</span> Multi-dimensional scaling (MDS)</h2>
<p>MDS is a dimensionality reduction technique that is based on the idea of representing the original data in a lower (typically 2-dimensional) space in a way that best preserves the distances between observations in the original space. The results of MDS are of course sensitive to the distance (metric) that we choose and, similar to PCA, scaling of the variables. There are many variants of MDS: classic MDS uses Euclidean distance and a generalization of that to an arbitrary metric is called metric MDS. There are also non-metric variants of MDS that allow for monotonic transformations of distance.</p>
<p>Unlike PCA, MDS does not assume that the high-dimensional structure of the input data can be reduced in a linear fashion and is not sensitive to outliers. That is, MDS can reduce dimensionality in a more robust way than PCA and can be used to detect outliers. Note that MDS, unlike PCA, can be used on data where only the relative distances are known.</p>
<p>We’ll be using non-metric MDS and we’ll first apply it to the decathlon data from the PCA example:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(MASS)
d &lt;-<span class="st"> </span><span class="kw">dist</span>(dat) <span class="co"># compute Euclidean distances between observations</span>
res &lt;-<span class="st"> </span><span class="kw">isoMDS</span>(d, <span class="dt">trace =</span> F)<span class="op">$</span>points
<span class="kw">plot</span>(res, <span class="dt">xlab =</span> <span class="st">&quot;MDS1&quot;</span>, <span class="dt">ylab =</span> <span class="st">&quot;MDS2&quot;</span>, <span class="dt">col =</span> <span class="st">&quot;white&quot;</span>)
<span class="kw">text</span>(res, <span class="dt">labels =</span> <span class="dv">1</span><span class="op">:</span><span class="kw">nrow</span>(dat))</code></pre></div>
<p><img src="Summarizing-data-03_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<p>In order to understand MDS1 and MDS2 dimensions, we would have to look at the characteristics of the athletes on the left-hand or right-hand side (higher and lower). Fortunately, we do not have to go through the effort, because we can readily compare this result to the result of PCA and we will quickly determine that they are very similar (up to rotation).</p>
<p>To better illustrate where PCA might fail but MDS would give reasonable results, we’ll use a 3D ball.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(scatterplot3d)
<span class="kw">library</span>(MASS)
dataset &lt;-<span class="st"> </span><span class="kw">readRDS</span>(<span class="st">&quot;./data/dataset.rds&quot;</span>)<span class="op">$</span>ball
dataset &lt;-<span class="st"> </span>dataset[<span class="kw">sample</span>(<span class="dv">1</span><span class="op">:</span><span class="kw">nrow</span>(dataset),<span class="dv">1000</span>, <span class="dt">rep =</span> F),]
<span class="kw">scatterplot3d</span>(dataset[,<span class="dv">1</span><span class="op">:</span><span class="dv">3</span>], <span class="dt">color =</span> dataset[,<span class="dv">4</span>], <span class="dt">pch =</span> <span class="dv">16</span>)</code></pre></div>
<p><img src="Summarizing-data-03_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<p>All three dimensions are almost identical, so PCA can only produce principal components that are a rotation of the ball. That is, the projection on the first two components does not add any value over just visualizing the first two original dimensions:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># PCA</span>
res &lt;-<span class="st"> </span><span class="kw">prcomp</span>(dataset[,<span class="dv">1</span><span class="op">:</span><span class="dv">3</span>])
rot &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(dataset[,<span class="dv">1</span><span class="op">:</span><span class="dv">3</span>]) <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(res<span class="op">$</span>rotation)
<span class="kw">plot</span>(rot[,<span class="dv">1</span><span class="op">:</span><span class="dv">2</span>], <span class="dt">col =</span> dataset[,<span class="dv">4</span>], <span class="dt">pch =</span> <span class="dv">16</span>, <span class="dt">xlab =</span> <span class="st">&quot;PCA1&quot;</span>, <span class="dt">ylab =</span> <span class="st">&quot;PCA2&quot;</span>)</code></pre></div>
<p><img src="Summarizing-data-03_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<p>On the other hand, MDS works with distances (topology) and groups points that are on similar layers of the ball closer together, producing a more useful 2D projection:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># MDS</span>
d &lt;-<span class="st"> </span><span class="kw">dist</span>(dataset[,<span class="dv">1</span><span class="op">:</span><span class="dv">3</span>]) <span class="co"># compute Euclidean distances between observations</span>
res &lt;-<span class="st"> </span><span class="kw">isoMDS</span>(d, <span class="dt">trace =</span> F)<span class="op">$</span>points
<span class="kw">plot</span>(res, <span class="dt">xlab =</span> <span class="st">&quot;MDS1&quot;</span>, <span class="dt">ylab =</span> <span class="st">&quot;MDS2&quot;</span>, <span class="dt">col =</span> dataset[,<span class="dv">4</span>], <span class="dt">pch =</span> <span class="dv">16</span>)</code></pre></div>
<p><img src="Summarizing-data-03_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
</div>
<div id="t-distributed-stochastic-neighbor-embedding-t-sne" class="section level2">
<h2><span class="header-section-number">8.3</span> t-Distributed Stochastic Neighbor Embedding (t-SNE)</h2>
<p>t-SNE is an advanced state-of-the-art non-linear technique for dimensionality reduction and visualization of high-dimensional data.</p>
<p>The key idea of t-SNE is to minimize the divergence between a distribution that measures pairwise similarities of the input objects and a distribution that measures pairwise similarities of the corresponding low-dimensional points in the embedding. Note that t-SNE is mainly a data exploration and visualization technique - the input features are no longer identifiable in the embedding, so inference can not be done only with t-SNE output.</p>
<p>The expressivenes of t-SNE makes it very useful for visualizing complex datasets that require non-linear transformations. On the other hand, as with all complex methods, there is the added computational complexity, additional parameters that need to be tuned, and fewer guarantees regarding convergence to a sensible solution. Due to its stochastic nature, t-SNE output can vary on the same data or, if we set the seed, we can get substantially different results for different seeds.</p>
<p>We illustrate t-SNE on data known as the Swiss-roll:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(scatterplot3d)
dataset &lt;-<span class="st"> </span><span class="kw">readRDS</span>(<span class="st">&quot;./data/dataset.rds&quot;</span>)<span class="op">$</span>roll
dataset &lt;-<span class="st"> </span>dataset[<span class="kw">sample</span>(<span class="dv">1</span><span class="op">:</span><span class="kw">nrow</span>(dataset),<span class="dv">1000</span>, <span class="dt">rep =</span> F),]
<span class="kw">scatterplot3d</span>(dataset[,<span class="dv">1</span><span class="op">:</span><span class="dv">3</span>], <span class="dt">color =</span> dataset[,<span class="dv">4</span>], <span class="dt">pch =</span> <span class="dv">16</span>)</code></pre></div>
<p><img src="Summarizing-data-03_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<p>MDS and PCA preserve the x and y dimensions of the data:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># MDS</span>
d &lt;-<span class="st"> </span><span class="kw">dist</span>(dataset[,<span class="dv">1</span><span class="op">:</span><span class="dv">3</span>]) <span class="co"># compute Euclidean distances between observations</span>
res &lt;-<span class="st"> </span><span class="kw">isoMDS</span>(d, <span class="dt">trace =</span> F)<span class="op">$</span>points
<span class="kw">plot</span>(res, <span class="dt">xlab =</span> <span class="st">&quot;MDS1&quot;</span>, <span class="dt">ylab =</span> <span class="st">&quot;MDS2&quot;</span>, <span class="dt">col =</span> dataset[,<span class="dv">4</span>], <span class="dt">pch =</span> <span class="dv">16</span>)</code></pre></div>
<p><img src="Summarizing-data-03_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># PCA</span>
res &lt;-<span class="st"> </span><span class="kw">prcomp</span>(dataset[,<span class="dv">1</span><span class="op">:</span><span class="dv">3</span>])
rot &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(dataset[,<span class="dv">1</span><span class="op">:</span><span class="dv">3</span>]) <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(res<span class="op">$</span>rotation)
<span class="kw">plot</span>(rot[,<span class="dv">1</span><span class="op">:</span><span class="dv">2</span>], <span class="dt">col =</span> dataset[,<span class="dv">4</span>], <span class="dt">pch =</span> <span class="dv">16</span>)</code></pre></div>
<p><img src="Summarizing-data-03_files/figure-html/unnamed-chunk-11-2.png" width="672" /></p>
<p>t-SNE on the other hand projects the data manifold into 2 dimensions, which produces an arguably more useful visualization of the characteristics of the data:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(Rtsne)</code></pre></div>
<pre><code>## Warning: package &#39;Rtsne&#39; was built under R version 3.5.3</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">321</span>)
res &lt;-<span class="st"> </span><span class="kw">Rtsne</span>(dataset[,<span class="dv">1</span><span class="op">:</span><span class="dv">3</span>], <span class="dt">perplexity =</span> <span class="dv">50</span>)
<span class="kw">plot</span>(res<span class="op">$</span>Y, <span class="dt">col =</span> dataset[,<span class="dv">4</span>], <span class="dt">xlab =</span> <span class="st">&quot;t-SNE1&quot;</span>, <span class="dt">ylab =</span> <span class="st">&quot;t-SNE2&quot;</span>)</code></pre></div>
<p><img src="Summarizing-data-03_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
</div>
<div id="clustering" class="section level2">
<h2><span class="header-section-number">8.4</span> Clustering</h2>
<p>Another common summarizatin technique is to group the observations into a finite number of groups or <strong>clusters</strong>. There are numerous clustering techniques, but they all have the same underlying general idea - we want to cluster the points in such a way that similar observations are in the same cluster and dissimilar observations are in different clusters. Instead of hard assignments to clusters, we can also probabilistically assign observations to clusters or assign some other type of assignment weight. This is known as soft clustering and we’ll not be discussing it in this text.</p>
<div id="k-means-clustering" class="section level3">
<h3><span class="header-section-number">8.4.1</span> k-means clustering</h3>
<p>One of the most common and useful clustering methods is k-means clustering. It is based on the assumption that the data were generated by k multivariate normal distributions, each representing one cluster. An observation is assigned to the cluster that it was most likely to have been generated from - and, because we are assuming normal distributions, this reduces to assigning the observation to the mean (centroid) that is closest in terms of Euclidean distance.</p>
<p>We’ll illustrate k-means clustering on the <em>swiss</em> dataset from R - data about 47 provinces of Switzerland in about 1888:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(swiss)</code></pre></div>
<pre><code>##    Fertility      Agriculture     Examination      Education    
##  Min.   :35.00   Min.   : 1.20   Min.   : 3.00   Min.   : 1.00  
##  1st Qu.:64.70   1st Qu.:35.90   1st Qu.:12.00   1st Qu.: 6.00  
##  Median :70.40   Median :54.10   Median :16.00   Median : 8.00  
##  Mean   :70.14   Mean   :50.66   Mean   :16.49   Mean   :10.98  
##  3rd Qu.:78.45   3rd Qu.:67.65   3rd Qu.:22.00   3rd Qu.:12.00  
##  Max.   :92.50   Max.   :89.70   Max.   :37.00   Max.   :53.00  
##     Catholic       Infant.Mortality
##  Min.   :  2.150   Min.   :10.80   
##  1st Qu.:  5.195   1st Qu.:18.15   
##  Median : 15.140   Median :20.00   
##  Mean   : 41.144   Mean   :19.94   
##  3rd Qu.: 93.125   3rd Qu.:21.70   
##  Max.   :100.000   Max.   :26.60</code></pre>
<p>We’ll suppose that there are 2 clusters in the data (automatic determination of the number of clusters will be discussed later in this chapter):</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">res &lt;-<span class="st"> </span><span class="kw">kmeans</span>(swiss, <span class="dt">centers =</span> <span class="dv">2</span>)
<span class="kw">print</span>(res<span class="op">$</span>cluster)</code></pre></div>
<pre><code>##   Courtelary     Delemont Franches-Mnt      Moutier   Neuveville 
##            1            2            2            1            1 
##   Porrentruy        Broye        Glane      Gruyere       Sarine 
##            2            2            2            2            2 
##      Veveyse        Aigle      Aubonne     Avenches     Cossonay 
##            2            1            1            1            1 
##    Echallens     Grandson     Lausanne    La Vallee       Lavaux 
##            1            1            1            1            1 
##       Morges       Moudon        Nyone         Orbe         Oron 
##            1            1            1            1            1 
##      Payerne Paysd&#39;enhaut        Rolle        Vevey      Yverdon 
##            1            1            1            1            1 
##      Conthey    Entremont       Herens     Martigwy      Monthey 
##            2            2            2            2            2 
##   St Maurice       Sierre         Sion       Boudry La Chauxdfnd 
##            2            2            2            1            1 
##     Le Locle    Neuchatel   Val de Ruz ValdeTravers V. De Geneve 
##            1            1            1            1            1 
##  Rive Droite  Rive Gauche 
##            1            1</code></pre>
<p>Unless we understand the meaning of the observations (in this case, province names), cluster assignments carry little information. To gain more insight, we can plot the clusters. This is typically done with some dimensionality reduction method that projects the data into 2 dimensions and preserves as much information as possible - the methods we’ve been discussing so far in this chapter. We’ll plot the clusters in the space determined by the first two principal components:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(factoextra)</code></pre></div>
<pre><code>## Warning: package &#39;factoextra&#39; was built under R version 3.5.3</code></pre>
<pre><code>## Loading required package: ggplot2</code></pre>
<pre><code>## Welcome! Related Books: `Practical Guide To Cluster Analysis in R` at https://goo.gl/13EFCZ</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">pca &lt;-<span class="st"> </span><span class="kw">prcomp</span>(swiss, <span class="dt">scale=</span><span class="ot">TRUE</span>)

<span class="kw">fviz_pca_biplot</span>(pca, <span class="dt">label=</span><span class="st">&quot;var&quot;</span>, <span class="dt">habillage=</span><span class="kw">as.factor</span>(res<span class="op">$</span>cluster)) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">color=</span><span class="ot">NULL</span>) <span class="op">+</span><span class="st"> </span><span class="kw">theme</span>(<span class="dt">text =</span> <span class="kw">element_text</span>(<span class="dt">size =</span> <span class="dv">15</span>), 
      <span class="dt">panel.background =</span> <span class="kw">element_blank</span>(), 
      <span class="dt">panel.grid.major =</span> <span class="kw">element_blank</span>(), 
      <span class="dt">panel.grid.minor =</span> <span class="kw">element_blank</span>(), 
      <span class="dt">axis.line =</span> <span class="kw">element_line</span>(<span class="dt">colour =</span> <span class="st">&quot;black&quot;</span>),
      <span class="dt">legend.position=</span><span class="st">&quot;none&quot;</span>)</code></pre></div>
<p><img src="Summarizing-data-03_files/figure-html/unnamed-chunk-15-1.png" width="672" /></p>
<p>Now we can interpret the two clusters as predominately Catholic and predominately Protestant provinces. PCA also shows some other characteristics assicuated with the clusters (blue is associated with higher fertility and more agriculture, while red is associated with higher education).</p>
<p>K-means clustering is simple and fast. It is, however, sensitive to outliers and, if the typical iterative algorithm is used, it might not converge to the same solution every time (solution might be sensitive to choice of initial centroids). K-means clustering also performs poorly in cases where the modelling assumption of k multivariate normal distributions does not hold.</p>
</div>
<div id="determining-the-number-of-clusters" class="section level3">
<h3><span class="header-section-number">8.4.2</span> Determining the number of clusters</h3>
<p>A very simple but often good enough way of determining the most appropriate number of clusters is the so-called <em>elbow method</em>. The idea is to find the number of clusters after which, if we furter increase the number of clusters, does no longer substantially decrease within-cluster variability. Within-cluster variability is decreasing in the number of clusters - more clusters will always lead to clusters being more similar, up to the point of assigning every point to its own cluster, which leads to 0 within-cluster variability. However, if adding a cluster decreases the within-cluster variability by a relatively small amount, that indicates that we might just be splitting already very homogeneous clusters.</p>
<p>We demonstrate this technique by plotting the within-cluster variability for different numbers of clusters (also called a <em>scree plot</em>):</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">wss  &lt;-<span class="st"> </span><span class="dv">0</span>
maxk &lt;-<span class="st"> </span><span class="dv">10</span>
<span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>maxk) {
  km &lt;-<span class="st"> </span><span class="kw">kmeans</span>(swiss, <span class="dt">centers =</span> i)
 
  wss[i] &lt;-<span class="st"> </span>km<span class="op">$</span>tot.withinss
}

<span class="co"># Plot total within sum of squares vs. number of clusters</span>
<span class="kw">plot</span>(<span class="dv">1</span><span class="op">:</span>maxk, wss, <span class="dt">type =</span> <span class="st">&quot;b&quot;</span>, 
     <span class="dt">xlab =</span> <span class="st">&quot;Number of Clusters&quot;</span>, 
     <span class="dt">ylab =</span> <span class="st">&quot;Within groups sum of squares&quot;</span>)</code></pre></div>
<p><img src="Summarizing-data-03_files/figure-html/unnamed-chunk-16-1.png" width="672" /> We see that after 2 clusters, the within-cluster variability decreases slowly. Therefore, according to this technique, k = 2 is a good choice for the number of clusters. Sometimes there will be no distinct elbow in the scree plot and we will not be able to use this technique effectively.</p>
<p>Another popular technique is to compute the Silhouette index. The silhouette intex measures how similar an observation is to its own cluster compared to other clusters ). It ranges from ???1 to +1, where a high value indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters. If most objects have a high value, then the clustering configuration is appropriate. If many points have a low or negative value, then the clustering configuration may have too many or too few clusters.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(cluster)
si  &lt;-<span class="st"> </span><span class="dv">0</span>
maxk &lt;-<span class="st"> </span><span class="dv">10</span>
<span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">2</span><span class="op">:</span>maxk) {
  km &lt;-<span class="st"> </span><span class="kw">kmeans</span>(swiss, <span class="dt">centers =</span> i)
 
  si[i] &lt;-<span class="st"> </span><span class="kw">mean</span>(<span class="kw">silhouette</span>(km<span class="op">$</span>cluster, <span class="kw">dist</span>(swiss))[,<span class="dv">3</span>]) <span class="co"># mean Silhouette</span>
}

si</code></pre></div>
<pre><code>##  [1] 0.0000000 0.6284002 0.5368920 0.4406436 0.4341318 0.4089358 0.4146117
##  [8] 0.3922444 0.3738533 0.3278620</code></pre>
<p>In this case the Silhouette technique leads to the same conclusion - k = 2 is the best choice for the number of clusters.</p>
<p>Alternatively, we can use clustering methods such as Affinity propagation and Mean shift clustering where the number of clusters is determined automatically, however, such methods introduce other parameters, such as kernel bandwidth.</p>
</div>
<div id="agglomerative-hierarchical-clustering" class="section level3">
<h3><span class="header-section-number">8.4.3</span> Agglomerative hierarchical clustering</h3>
<p>Hierarchical clustering is an umbrella term for a host of clustering methods that build a hierarchy of clusters. Here we’ll talk about agglomerative clustering - hierarchical clustering that starts with each observation in its own cluster and procedes by joining the most similar clusters, until only one cluster remains. The steps of this procedure form a hierarchical cluster structure. Divisive hierarchical clustering approaches instead start with one cluster and procede by splitting the clusters.</p>
<p>There are also several different criteria for determining the two most similar clusters A and B: smallest minimum distance between a point from A and a point from B (single) linkage), smallest maximum distance (complete linkage), average distance, and many other, each with its own advantages and disadvantages.</p>
<p>Here, we’ll apply hierarchical agglomerative clustering with joining clusters according to average distance. Of course, hierarchical clustering works for any distance (metric). We choose Euclidean distance. Here are the results for the swiss dataset:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">res_hk &lt;-<span class="st"> </span><span class="kw">hclust</span>(<span class="kw">dist</span>(swiss), <span class="dt">method =</span> <span class="st">&quot;average&quot;</span>)
<span class="kw">plot</span>(res_hk)</code></pre></div>
<p><img src="Summarizing-data-03_files/figure-html/unnamed-chunk-18-1.png" width="672" /></p>
<p>The above dendrogram visualizes the entire hierarchical clustering structure. We can see how a hierarchical clustering contains not just one, but a clustering for every number of clusters between 1 and the number of points.</p>
<p>Comparing with k-means results for 2 clusters, we can see that the two methods return identical clusters:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">hk2 &lt;-<span class="st"> </span><span class="kw">cutree</span>(res_hk, <span class="dt">k =</span> <span class="dv">2</span>)

res &lt;-<span class="st"> </span><span class="kw">kmeans</span>(swiss, <span class="dt">centers =</span> <span class="dv">2</span>)
km2 &lt;-<span class="st"> </span>res<span class="op">$</span>cluster

<span class="kw">rbind</span>(hk2, km2)</code></pre></div>
<pre><code>##     Courtelary Delemont Franches-Mnt Moutier Neuveville Porrentruy Broye
## hk2          1        2            2       1          1          2     2
## km2          1        2            2       1          1          2     2
##     Glane Gruyere Sarine Veveyse Aigle Aubonne Avenches Cossonay Echallens
## hk2     2       2      2       2     1       1        1        1         1
## km2     2       2      2       2     1       1        1        1         1
##     Grandson Lausanne La Vallee Lavaux Morges Moudon Nyone Orbe Oron
## hk2        1        1         1      1      1      1     1    1    1
## km2        1        1         1      1      1      1     1    1    1
##     Payerne Paysd&#39;enhaut Rolle Vevey Yverdon Conthey Entremont Herens
## hk2       1            1     1     1       1       2         2      2
## km2       1            1     1     1       1       2         2      2
##     Martigwy Monthey St Maurice Sierre Sion Boudry La Chauxdfnd Le Locle
## hk2        2       2          2      2    2      1            1        1
## km2        2       2          2      2    2      1            1        1
##     Neuchatel Val de Ruz ValdeTravers V. De Geneve Rive Droite Rive Gauche
## hk2         1          1            1            1           1           1
## km2         1          1            1            1           1           1</code></pre>
<p>However, for 3 clusters, the results are, at first glance, completely different:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">hk2 &lt;-<span class="st"> </span><span class="kw">cutree</span>(res_hk, <span class="dt">k =</span> <span class="dv">3</span>)

res &lt;-<span class="st"> </span><span class="kw">kmeans</span>(swiss, <span class="dt">centers =</span> <span class="dv">3</span>)
km2 &lt;-<span class="st"> </span>res<span class="op">$</span>cluster

<span class="kw">rbind</span>(hk2, km2)</code></pre></div>
<pre><code>##     Courtelary Delemont Franches-Mnt Moutier Neuveville Porrentruy Broye
## hk2          1        2            2       1          1          2     2
## km2          3        1            1       3          3          1     1
##     Glane Gruyere Sarine Veveyse Aigle Aubonne Avenches Cossonay Echallens
## hk2     2       2      2       2     1       1        1        1         1
## km2     1       1      1       1     3       3        3        3         3
##     Grandson Lausanne La Vallee Lavaux Morges Moudon Nyone Orbe Oron
## hk2        1        1         1      1      1      1     1    1    1
## km2        3        3         3      3      3      3     3    3    3
##     Payerne Paysd&#39;enhaut Rolle Vevey Yverdon Conthey Entremont Herens
## hk2       1            1     1     1       1       2         2      2
## km2       3            3     3     3       3       2         2      2
##     Martigwy Monthey St Maurice Sierre Sion Boudry La Chauxdfnd Le Locle
## hk2        2       2          2      2    2      1            1        1
## km2        2       1          2      2    1      3            3        3
##     Neuchatel Val de Ruz ValdeTravers V. De Geneve Rive Droite Rive Gauche
## hk2         1          1            1            3           3           3
## km2         3          3            3            3           3           3</code></pre>
<p>When comparing different clusterings of the same data, we must keep in mind that a clustering is invariant to relabelling. That is, the cluster label or number has no meaning - a clusters meaning is determined by the observations in that cluster. There exist special summaries for cluster similarity. One of the most commonly used is the <em>Rand index</em>. It is defined as the ratio of concordant pairs in all pairs of points. A pair of points is concordant if the points are either in the same cluster in both clusterings or in different clusters in both clusterings.</p>
<p>Typicaly, we would use an exisitng implementation, but here we implement the basic Rand Index ourselves:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">concordant &lt;-<span class="st"> </span><span class="dv">0</span>
<span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">2</span><span class="op">:</span><span class="kw">length</span>(hk2)) {
  <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>(i<span class="op">-</span><span class="dv">1</span>)) {
    <span class="cf">if</span> ((hk2[i] <span class="op">==</span><span class="st"> </span>hk2[j]) <span class="op">==</span><span class="st"> </span>(km2[i] <span class="op">==</span><span class="st"> </span>km2[j])) concordant &lt;-<span class="st"> </span>concordant <span class="op">+</span><span class="st"> </span><span class="dv">1</span>
  }
}
rand_idx &lt;-<span class="st"> </span>concordant <span class="op">/</span><span class="st"> </span><span class="kw">choose</span>(<span class="kw">length</span>(hk2), <span class="dv">2</span>)
<span class="kw">round</span>(rand_idx, <span class="dv">2</span>)</code></pre></div>
<pre><code>## [1] 0.87</code></pre>
<p>By definition, Rand index is between 0 and 1.</p>
</div>
</div>
<div id="further-reading-and-references-4" class="section level2">
<h2><span class="header-section-number">8.5</span> Further reading and references</h2>
</div>
<div id="learning-outcomes-4" class="section level2">
<h2><span class="header-section-number">8.6</span> Learning outcomes</h2>
<p>Data science students should work towards obtaining the knowledge and the skills that enable them to:</p>
<ul>
<li>Reproduce the techniques demonstrated in this chapter using their language/tool of choice.</li>
<li>Recognize when a technique is appropriate and when it is not.</li>
<li>Apply data summarization techiques to obtain insights from data.</li>
</ul>
</div>
<div id="practice-problems-4" class="section level2">
<h2><span class="header-section-number">8.7</span> Practice problems</h2>
<p>TODO: Basically, take a rich enough dataset and demonstrate most if not all summarization techniques shown here. For each summary, add an interpretation of the insights it provides.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="dynamic-reports-and-reproducibility.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="relational-databases.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
