[
["index.html", "Introduction to data science Preface", " Introduction to data science Slavko Žitnik, Tomaž Curk, Erik Štrumbelj 2019-07-29 Preface These are the course notes for the Introduction to data science course of the Data Science Master’s at University of Ljubljana, Faculty of computer and information science. "],
["introduction.html", "Introduction", " Introduction "],
["python-programming-language.html", "Chapter 1 Python programming language 1.1 Basic characteristics 1.2 Why Python? 1.3 Setting up the environment 1.4 Installing dependencies 1.5 Jupyter notebooks 1.6 Short introduction to Python 1.7 Python ecosystem for Data Science 1.8 Further reading and references 1.9 Learning outcomes 1.10 Practice problems", " Chapter 1 Python programming language 1.1 Basic characteristics Python is an interpreted, dynamically typed, object-oriented programming language. Advantages: Simple. Easy to learn. Extensive packages. Cross-platform. Free and open source. Large user base. Disadvantages: Being interpreted results in slower execution. Dynamic typing can lead to errors that only show up at runtime. Higher memory consumption, less suitable for memory-intensive tasks. 1.2 Why Python? The Python language is one of the two most popular languages for data science (the other being R). The two main reasons are: The advantages of Python fit the typical data science workflow and its disadvantages are not that deterimental to the data science workflow. Python has a large ecosystem of packages, libraries and tools for data science, some of which are discussed later in this chapter. Often libraries and software developed in other languages provide Python API or bindings. The typical data science workflow consists of acquiring and manipulating data and applying standard machine learning or statistical methods. In essence, the data flows through different methods. The emphasis is on obtaining results - extracting meaningful information from data. The advantages of Python are extremely beneficial to such a workflow: Being simple, easy to learn and free, it is accessible to a larger user base, including users with little or no prior programming experience. Being an interpreted language (and straightforward piecewise execution through read-eval-print loop shells or REPL) makes Python very flexible - multiple alternatives can be explored and quick decisions made on how to procede, depending on intermediary results. The disadvantages of Python are of minor consequence: The data science workflow is not time-critical - even an order-of-magnitude slowdown typically makes little difference. Code maintainability is less important - data science scripts are often short and discarded after use. Specific platforms development or enterprise-level applications development are also not part of the typical data science workflow. Data science products used in such applications are normally rewritten as final models in a production-ready code. 1.3 Setting up the environment Before using Python, we need to select and install a desired Python distribution. We can choose to install a pure Python distribution or an Anaconda Python distribution. Some advantages of using an Anaconda distribution are: Anaconda makes it easy for the user to install the Python version of choice. Anaconda will also resolve issues with administrator privileges if a user does not have administrative rights for his system. Anaconda Accelerate can provide the user with high performance computing and several other components. Anaconda removes bottlenecks involved in installing the right packages while taking into considerations their compatibility with various other packages as might be encountered while using the traditional package manager (pip). There is no risk of breaking required system libraries. There are also many open source packages available for Anaconda, which are not within the pip repository. We encourage you to use the Anaconda Python distribution. 1.3.1 Anaconda distribution installation Install the desired Anaconda Python distribution. A useful way of managing multiple Python projects is to use Conda environments. An environment enables you to use a specific version of Python along with specific dependencies completely separately on a single system. To create and use an environment with a name itds, issue the following command: $ conda create -n itds $ conda activate itds At the beginning of a line in the console you can see currently active environment. To run Python within this evironment, issue the python command in the console. You should see something similar to the following: (itds)$ python Python 3.6.8 |Anaconda, Inc.| (default, Dec 29 2018, 19:04:46) [GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)] on darwin Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information. &gt;&gt;&gt; To exit the Python interpreter, enter and commit the command exit(); To exit the environment, use conda deactivate. To show existing environments and their locations, issue conda info --envs. 1.3.2 Pure Python distribution installation You can also install a pure Python distributiondirectly to your system from the official Python Downloads web page. To run Python, issue the python command in the console (there may be more interpreters installed on your machine and Python 3.5 might be run also using python3.5). After running the command, you should see something similar to the following: $ python Python 3.5.2 (v3.5.2:4def2a2901a5, Jun 26 2016, 10:47:25) [GCC 4.2.1 (Apple Inc. build 5666) (dot 3)] on darwin Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information. &gt;&gt;&gt; When using the distribution directly, all packages and settings are changed system-wide, which may lead to problems when maintaining multiple Python projects (e.g. problems in having installed different versions of a specific library). Similar to an Anaconda distribution, one can use virtualenv virtual environments. First, we need to install virtualenv via pip: $ pip3 install virtualenv $ virtualenv --version 16.6.1 To set up a virtual environment for a project, first create a project folder and set up a new environment in that folder. The latter will create Python executables within that folder and a copy of pip library that is used to install libraries local to the environment (parameter p is optional). $ cd itds_project $ virtualenv -p /usr/local/bin/python2 itds To activate the environment, run the script venv/bin/activate from the project folder and use project specific Python: $ source itds/bin/activate (itds)$ python Python 2.7.14 (default, Mar 9 2018, 23:57:12) [GCC 4.2.1 Compatible Apple LLVM 9.0.0 (clang-900.0.39.2)] on darwin Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information. &gt;&gt;&gt; After you finish working on your project, deactivate the current virtual environment: (itds)$ deactivate $ 1.4 Installing dependencies Many useful libraries are available online, mostly in the Python Package Index (PyPI) repository. Well-built libraries consist of installation instructions and a setup.py file to install them. The common location for installing libraries is the folder %PYTHON_BASE%/lib, %PYTHON_BASE%/site-packages or %PYTHON_BASE%/Lib. Packages can be installed using the pip command. For example, to install the NLTK library, we issue the following command: $ pip install nltk In some cases packages will be prebuilt for a specific OS, because the installation of its dependencies can be tedious. In such cases, wheel packages can be provided and installed using the following command: $ pip install YOUR_DOWNLOADED_PACKAGE.whl An Anaconda distribution provides its own package repository and in addition to pip, Anaconda packages can be installed as follows: $ conda install nltk If a package is not available in the official repository, it may be available from some private or community-led channels, for example conda-forge: $ conda install -c conda-forge pyspark 1.5 Jupyter notebooks The Jupyter Notebook is an open-source web application that allows you to create and share documents that contain live code, equations, visualizations and narrative text. Uses include: data cleaning and transformation, numerical simulation, statistical modeling, data visualization, machine learning, and much more. To add support for Anaconda environments to Jupyter, issue the command below. This will add Conda extensions to Jupyter. The feature will be installed for currently active environment, so you can also install it to the base environment and always run Jupyter from it. When running Jupyter, you will notice a Conda tab in the file browser, which will enable the listing and selection of existing Anaconda environments, overview of the installed packages in the environment, installing new packages from the available package list, checking for updates packages and updating packages in the environment. $ conda install nb_conda 1.5.1 Running a Jupyter notebook Prior to running a Jupyter notebook we first need to start a Jupyter web server by issuing a command: $ jupyter notebook By default the server is started in the current folder and accessible via the web interface: http://localhost:8888. The root web page shows a file browser where we can create a new Jupyter notebook or open an existing one: To get a similar view, save the provided Jupyter notebook into the folder where you ran jupyter notebook command. Click the filename of the notebook and it will open in a new window: As you notice, a notebook consists of linearly ordered block of types Markdown, Code, Raw NBConvert and Heading. If you double click a text block you can edit it using Markdown syntax. To evaluate the block again and show rendered view, click the Run button or press Alt+Enter. The same holds for other types of blocks. Sometimes we just want to view or execute a notebook online. There exist multiple services offering this functionality, for example https://gke.mybinder.org/. 1.6 Short introduction to Python All the examples presented in this section are also provided in a Jupyter notebook. Basics Let’s first say hello: print(&quot;Hello Data science!&quot;) ## Hello Data science! 1.6.0.1 Variables and types Python defines whole numbers (int, long) and real numbers (float). Whole numbers are integers (\\(\\pm 2^{31}\\) or \\(\\pm 2^{63}\\)) and long numbers, limited by the memory size. Long is a number with a trailing L added at the end (Python 2, in Python 3, long is merged with int). Complex numbers are also supported using a trailing j to the imaginary part. Bool type is based on integer - value of 1 as True or anything else as False. String values are represented as sequence of characters within &quot; or '. A constant None is defined to represent the nonexistence of a value. a = 2864 print(&quot;Type of a is {}&quot;.format(type(a))) ## Type of a is &lt;type &#39;int&#39;&gt; b = 2864L print(&quot;Type of b is {}&quot;.format(type(b))) ## Type of b is &lt;type &#39;long&#39;&gt; c = 18+64j print(&quot;Type of c is {}&quot;.format(type(c))) ## Type of c is &lt;type &#39;complex&#39;&gt; d = False print(&quot;Type of d is {}&quot;.format(type(d))) ## Type of d is &lt;type &#39;bool&#39;&gt; e = &quot;I&#39;m loving it!&quot; print(&quot;Type of e is {}&quot;.format(type(e))) ## Type of e is &lt;type &#39;str&#39;&gt; f = None print(&quot;Type of f is {}&quot;.format(type(f))) ## Type of f is &lt;type &#39;NoneType&#39;&gt; Numbers and basic operators Basic data manipulations: a = 3 b = 2.5 c = a + b print(&quot;Addition: {}&quot;.format(c)) ## Addition: 5.5 c = a * b print(&quot;Multiplication: {}&quot;.format(c)) ## Multiplication: 7.5 c = a / b print(&quot;Division: {}&quot;.format(c)) ## Division: 1.2 c = True + 5 print(&quot;Addition to Boolean: {}&quot;.format(c)) ## Addition to Boolean: 6 c = &quot;5&quot; * 5 print(&quot;String multiplication: {}&quot;.format(c)) ## String multiplication: 55555 1.6.0.2 Strings, concatenation and formatting Basic strings manipulations: a = &quot;Data science&quot; b = &#39;a multi-disciplinary field&#39; # we can use double or single quotes c = a + &quot; &quot; + b print(&quot;Concatenated string: {}&quot;.format(c)) ## Concatenated string: Data science a multi-disciplinary field first = c[:4] last = c[-5:] print(&quot;First word: &#39;{}&#39; and last word: &#39;{}&#39;.&quot;.format(first, last)) ## First word: &#39;Data&#39; and last word: &#39;field&#39;. firstLower = first.lower() lastUpper = last.upper() print(&quot;First word lowercased: &#39;{}&#39; and last word uppercased: &#39;{}&#39;.&quot;.\\ format(firstLower, lastUpper)) ## First word lowercased: &#39;data&#39; and last word uppercased: &#39;FIELD&#39;. management = c.replace(&quot;science&quot;, &quot;management&quot;) print(&quot;Substring replacement: &#39;{}&#39;&quot;.format(management)) ## Substring replacement: &#39;Data management a multi-disciplinary field&#39; Explore more about strings in the official Python 3 documentation for strings. # string package import string print(&quot;Punctuation symbols: &#39;{}&#39;&quot;.format(string.punctuation)) ## Punctuation symbols: &#39;!&quot;#$%&amp;&#39;()*+,-./:;&lt;=&gt;?@[\\]^_`{|}~&#39; String manipulation is essential for parsing and providing machine readable outputs when needed. For more sophisticated examples, see https://pyformat.info/. number = 6/.7 text = &quot;dyslexia&quot; format0 = &quot;Number: &quot; + str(round(number*100)/100.0) + &quot;, Text: &quot; + \\ &quot; &quot;*(15-len(text)) + text print(format0) ## Number: 8.57, Text: dyslexia format1 = &quot;Number: {:5.2f}, Text: {:&gt;15}&quot;.format(number, text) print(format1) ## Number: 8.57, Text: dyslexia format2 = &quot;Number: %5.2f, Text: %15s&quot; % (number, text) print(format2) ## Number: 8.57, Text: dyslexia 1.6.0.3 Data stuctures: Lists, Tuples, Sets, Dictionaries Below we create some of the data structures available in Python. Explore more of their functionality in the official Python documentation. l = [1, 2, 3, &quot;a&quot;, 10] # List t = (1, 2, 3, &quot;a&quot;, 10) # Tuple (immutable) s = {&quot;a&quot;, &quot;b&quot;, &quot;c&quot;} # Set dict = { &quot;title&quot;: &quot;Introduction to Data Science&quot;, &quot;year&quot;: 1, &quot;semester&quot;: &quot;fall&quot;, &quot;classroom&quot;: &quot;P02&quot; } dict[&quot;classroom&quot;] = &quot;P03&quot; We often use inline functions to map, filter or calculate values on an iterable data structure. For example, to apply a function to all values (map), filter out unnecessary values or use all values in a calculation: #from functools import reduce # Python 3 import for reduce (not needed for Python 2) l = [6, 8, 22, 4, 12] doubled = map(lambda x: x*2, l) print(&quot;Doubled: {}&quot;.format(doubled)) ## Doubled: [12, 16, 44, 8, 24] filtered = filter(lambda x: x &gt; 10, l) print(&quot;Filtered: {}&quot;.format(filtered)) ## Filtered: [22, 12] sum = reduce(lambda x, y: x+y, l) print(&quot;Sum value: {}&quot;.format(sum)) ## Sum value: 52 l = [6, 8, 22, 4, 12] newList = [x**2 for x in l if x &gt;= 5 and x &lt;= 10] print(&quot;Squared values between 5 and 10: {}&quot;.format(newList)) ## Squared values between 5 and 10: [36, 64] 1.6.1 Control flow operations Many operations can be written inline or using multiple lines. Let’s check how to use if statements and loops. a = 2 if a &gt; 1: print(&#39;a is greater than 1&#39;) elif a == 1: print(&#39;a is equal to 1&#39;) else: print(&#39;a is less than 1&#39;) ## a is greater than 1 # Inline if statement a = 2 print(&#39;a is greater than 1&#39; if a &gt; 1 else &#39;a is lower or equal to 2&#39;) ## a is greater than 1 Loops: for i in range(4, 6): print(i) ## 4 ## 5 people_list = [&#39;Ann&#39;, &#39;Bob&#39;, &#39;Charles&#39;] for person in people_list: print(person) ## Ann ## Bob ## Charles i = 1 while i &lt;= 3: print(i) i = i + 1 ## 1 ## 2 ## 3 1.6.2 Functions We organize (encapsulate) our code into logical units, so we can reuse them and reduce the amount of duplicate, boilerplate code. Below write the function greetMe that takes one argument (name) as input and prints some string. The function’s code will be executed when we call the function. def greetMe(name): print(&quot;Hello my friend {}!&quot;.format(name)) greetMe(&quot;Janez&quot;) ## Hello my friend Janez! Sometimes our functions will have many arguments, some of which might be optional or have a default value. In the example below we add a argument with a default value. If there are multiple optional arguments we can set their values by naming them. def greet(name, title = &quot;Mr.&quot;): print(&quot;Hello {} {}!&quot;.format(title, name)) greet(&quot;Janez&quot;) ## Hello Mr. Janez! greet(&quot;Mojca&quot;, &quot;Mrs.&quot;) ## Hello Mrs. Mojca! greet(&quot;Mojca&quot;, title = &quot;Mrs.&quot;) ## Hello Mrs. Mojca! A function can also call itself and return a value. def sumUpTo(value): if value &gt; 0: return value + sumUpTo(value-1) else: return 0 print(&quot;Sum of all positive integers up to 50 is: {}&quot;.format(sumUpTo(50))) ## Sum of all positive integers up to 50 is: 1275 Python encapsulates variables within functions - they are not accessible outside the function. When we want variables to be accessible globally, we can use the global keyword. This can result in some difficult to predict behaviour and interactions, so use with caution! def playWithVariables(value1, list1): global globVal globVal = 3 value1 = 10 list1.append(22) print(&quot;Within function: {} and {} and {}&quot;.format(value1, list1, globVal)) value1 = 5 list1 = [3, 6, 9] print(&quot;Before function: {} and {}&quot;.format(value1, list1)) playWithVariables(value1, list1) print(&quot;After function: {} and {} and {}&quot;.format(value1, list1, globVal)) ## Before function: 5 and [3, 6, 9] ## Within function: 10 and [3, 6, 9, 22] and 3 ## After function: 5 and [3, 6, 9, 22] and 3 In some cases we can also define functions that accept an arbitrary number of unnamed (args) and/or named (kwargs) arguments. def paramsWriter(*args, **kwargs): print(&quot;Non-named arguments: {}\\nNamed arguments: {}&quot;.format(args, kwargs)) paramsWriter(1, &quot;a&quot;, [1,5,6], studentIds = [234, 451, 842], maxScore = 100.0) ## Non-named arguments: (1, &#39;a&#39;, [1, 5, 6]) ## Named arguments: {&#39;maxScore&#39;: 100.0, &#39;studentIds&#39;: [234, 451, 842]} When naming functions, classes, objects, packages, etc. we need to be careful not to overwrite existing objects. Bugs such as this one can be difficult to find: def greeter(): print(&quot;Hello to everyone!&quot;) greeter() greeter = &quot;Mr. John Hopkins&quot; greeter() # Error - greeter is now string value Hello to everyone! Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt; TypeError: &#39;str&#39; object is not callable 1.6.3 Classes and objects Python is also object-oriented and therefore enables us to encapsulate data and functionality into classes. Instances of classes are objects. The class below consists of one class variable, one class method, three object methods and two object variables. All class-based variables are accessible using the class name directly without having instantiated the object. Object-based methods are accessible only through an instantiated object and can also directly modify object properties (self.*). All object methods accept self as an implicit parameter, which points to the current object. elow we show an example of object declaration and its use. A detailed explanation can be found in the official Python documentation. class Classroom: classCounter = 0 def numClasses(): return Classroom.classCounter def __init__(self, name): Classroom.classCounter += 1 self.name = &quot;Best of Data Science class &quot; + name self.students = [] def enroll(self, student): self.students.append(student) def __str__(self): return &quot;Class: &#39;{}&#39;, students: &#39;{}&#39;&quot;.format(self.name, &quot;, &quot;.join(self.students)) class1 = Classroom(&quot;best of millenials&quot;) class2 = Classroom(&quot;old sports&quot;) print(&quot;Num classes: {}&quot;.format(Classroom.classCounter)) print(&quot;Num classes: {}&quot;.format(Classroom.numClasses())) class2.enroll(&quot;Slavko Žitnik&quot;) class2.enroll(&quot;Erik Štrumbelj&quot;) class2.enroll(&quot;Tomaž Curk&quot;) print(class2) ## Num classes: 2 ## Num classes: 2 ## Class: &#39;Best of Data Science class old sports&#39;, students: &#39;Slavko Žitnik, Erik Štrumbelj, Tomaž Curk&#39; 1.6.4 Python IDE’s and code editors An IDE (Integrated Development Environment) is software dedicated to software development. As the name implies, IDEs integrate several tools specifically designed for software development. These tools usually include: An editor designed to handle code with features such as syntax highlighting and auto-completion. Build, execution, and debugging tools. Some form of source control support. IDEs are generally large and take time to download and install. You may also need advanced knowledge to use them properly. In contrast, a dedicated code editor can be as simple as a text editor with syntax highlighting and code formatting capabilities. Most good code editors can execute code and control a debugger. The very best ones interact with source control systems as well. Compared to an IDE, a good dedicated code editor is usually smaller and quicker, but often less feature rich. Below we list some popular Python IDEs/code editors that are available for major operating systems (Windows, Linux and Mac OS): IDLE - the default code editor that installs together with the Python distribution. It includes a Python shell window (interactive interpreter), auto-completion, syntax highlighting, smart indentation and a basic integrated debugger. We do not recommend it for larger projects. Sublime Text, Atom, Visual Studio Code - highly customizable code editors with rich features of an IDE. They support installation of additional extensions and also provide intelligent code completion, linting for potential errors, debugging, unit testing and so on. These editors are becoming quite popular among Python and web developers. PyCharm - an IDE for professional developers. There are two versions available: a free Community version and a paid Professional version which is free for students only. PyCharm provides all major features of a good IDE: code completion, code inspections, error-highlighting and fixes, debugging, version control system and code refactoring, etc.. 1.7 Python ecosystem for Data Science The Python ecosystem of libraries, frameworks and tools is large and ever-growing. Python can be used for web scraping, machine learning, general scientific computing and many other computing and scripting uses. We list some of the most widely used libraries in the field of data science. NumPy - NumPy is the fundamental package for scientific computing with Python. The tools that we commonly use are: a powerful N-dimensional array object, sophisticated (broadcasting) functions, tools for integrating C/C++ and Fortran code and most importantly, linear algebra, Fourier transform and random number capabilities. NumPy can also be used as an efficient multi-dimensional container of generic data, where arbitrary data-types can be defined. Matplotlib - A 2D plotting library which produces publication quality figures in a variety of hardcopy formats and interactive environments across platforms. Matplotlib can be used in Python scripts, Python and IPython shells, Jupyter notebooks and web application servers. We can generate plots, histograms, power spectra, bar charts, errorcharts, scatterplots, etc., with just a few lines of code. It provides a MATLAB-like interface, particularly when combined with IPython. It gives users full control of line styles, font properties, axes properties, etc. SciPy - “Sigh Pie”&quot; is a Python-based ecosystem of open-source software for mathematics, science and engineering. In particular, it connects the following core packages: NumPy, SciPy library (fundamentals for scientific computing), Matplotlib, IPython, Sympy (symbolic mathematics) and Pandas. scikit-learn - a Machine Learning (ML) library in Python. It provides simple and efficient tools for data analysis. It offers a framework and many algorithms for classification, regression, clustering, dimensionality reduction, model selection and preprocessing. The library is open source and build on NumPy, SciPy and matplotlib. Pandas - Python Data Analysis Library (pandas) is an open source, BSD-licensed library providing high-performance, easy-to-use data structures and data analysis tools for the Python programming language. Some of the library highlights are a fast and efficient DataFrame object for data manipulation with integrated indexing, tools for reading and writing data between in-memory data structures and different formats, intelligent data alignment and integrated handling of missing data, flexible reshaping and pivoting of data sets, high performance merging and joining of data sets, an intuitive way of working with high-dimensional data in a lower-dimensional data structure, time series-functionality. TensorFlow - TensorFlow is an end-to-end open source platform for machine learning in the field of deep learning. It has a comprehensive and flexible ecosystem of tools, libraries and community resources that lets researchers push the state-of-the-art in ML and developers to easily build and deploy ML powered applications. Keras - Compared with TensorFlow, Keras is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, CNTK or Theano backends. It was developed with a focus on enabling fast experimentation, being able to go from idea to result with the least possible delay. Keras allows for easy and fast prototyping (through user friendliness, modularity and extensibility), supports both convolutional networks and recurrent networks, as well as combinations of the two and runs seamlessly on CPU and GPU. 1.8 Further reading and references Here is a list of more comprehensive guides to Python programming: Official Python Tutorials - Tutorial accompanying official Python documentation. See this resource for latest features. Beginning Python by Magnus Hetland - The book is written for beginners but last chapters are useful also for more experienced programmers. Non-Programmer’s Tutorial for Python - a well organized Wikibook. Think Python - similar book to the previous one, but a bit more intermediate. [Wikibook: Programming Python] - similar to the above two with more advanced topics. How to Think Like a Computer Scientist - well organized sections for self-paced learning. Python za programerje by Janez Demšar - Well known Slovene book, written by the professor at our Faculty. You can find electronic versions online, but printed version is accessible to buy at publishing house FRI (at the entrance). 1.9 Learning outcomes Data science students should work towards obtaining the knowledge and the skills that enable them to: Use the Python programming language for simple programming tasks, data manipulation and file I/0. Identify the Python IDE(s) that best fit their requirements. Find suitable Python packages for the task at hand and use them. Recognize when Python is and when it is not a suitable language to use. 1.10 Practice problems Install Anaconda Python, run the provided Jupyter notebook within a new conda environment and then export all the installed dependencies into an environment.yml file (see reference). Check the file, remove not needed data (location, library versions, libraries in lower dependency trees), create a new environment based on the exported file and run the notebook again (it should work without the need to install additiona packages manually). Try different Python IDEs and form a personal opinion of their advantages and disadvantaged. Download, explore and run some scripts from the Keras examples repository. Download the CMU Seminar Announcements dataset and uncompress it. The dataset consists of multiple files, whereas each file represents a seminar anncouncement. Write a program that reads every file and tries to extract the speaker, title, start-time, end-time and the location of the seminar. Help yourself with regular expressions and libraries mentioned above. Store all the extracted data into a Pandas data frame and lastly, export all the data into a single CSV file. In addition, compute your success rate in extraction of specific fields based on the manual tags from the documents. "],
["source-code-control.html", "Chapter 2 Source code control", " Chapter 2 Source code control Now let’s talk details. "],
["docker-container-platform.html", "Chapter 3 Docker container platform 3.1 Why Docker? 3.2 Setting up the environment 3.3 Short introduction to Docker 3.4 Further reading and references 3.5 Learning outcomes 3.6 Practice problems", " Chapter 3 Docker container platform For extreme portability and scalability of our applications there exist tools that can package together the entire required application environment, such as library dependencies or language runtime. Docker is a set of tools that use operating system-level virtualization to develop and deliver software in packages called containers. The software that hosts the containers is called Docker Engine and was first started in 2013 by Docker, Inc. Docker products are available both in Community and Enterprise editions. Docker containers are isolated from each other and they bundle their own software, libraries and configuration files. If our application consists of multiple services, containers can communicate with each other through internal or external network configuations. All containers are run by a single host operating system kernel and are thus more lightweight than virtual machines. They are instantiated from Docker images that specify their precise contents. We often create images by combining and modifying existing standard images downloaded from public repositories, also known as image registries. An instance of an image (a container) thus contains configured networking, storage, logging, etc. Furthermore, Docker defines an abstraction for these machine-specific settings and the exact same Docker container can run without any changes on many different machines with many different configurations. There exist other tools similar to Docker, which may not be that well known or are used for different scenarios, such as Singularity, Nanobox or LXC. Another approach of bundling applications into separate environment is using virtual machines, where each virtual machine contains its own operating system. Most common virtualization technologies are VMWare products or Virtualbox. A technology in-between both worlds is Vagrant, which enables a user to script the entire environment but when running the app, Vagrant creates a separate virtual machine (e.g. using Virtualbox) and runs it. 3.1 Why Docker? Author of a Linux.com article (Noyes 2008) described Docker as follows: Docker is a tool that can package an application and its dependencies in a virtual container that can run on any Linux server. This helps enable flexibility and portability on where the application can run, whether on premises, public cloud, private cloud, bare metal, etc. Virtual machines and containers have similar resource isolation and allocation benefits, but containers virtualize the operating system whereas virtual machines virtualize hardware. Therefore, containers are more portable and efficient. Multiple containers can run on the same machine and share the operating system kernel with other containers, each running as isolated processes in user space. Containers images are typically tens of MBs in size, which is much than typical virtual machines. Containers can handle more applications and require fewer operating systems. Virtual machines as an abstraction of physical hardware turn one server into many servers. The hypervisor allows multiple virtual machines to run on a single machine. Each virtual machine contains a full copy of an operating system, the application, necessary binaries and libraries. This requires GBs of data - virtual machines require more disk space, physical resources and are slower to boot. The Figure below shows a comparison between multiple running Docker containers (left hand side) and multiple virtual machines (right hand side): Figure 3.1: Dockerized applications vs. VM-virtualized applications (image courtesy of docker.com). 3.2 Setting up the environment To install Docker on your machine, you need to download the appropriate Docker Engine Community distribution from the official download website. There also exists a Docker Machine installation package, which is a provisioning and installation software for Docker - we do not need it at this point as it helps us manage multiple remote Docker hosts. After first installation we can explore different installation settings and tools. Kitematic is a tool which provides a handy GUI to overview downloaded images, containers, volumes and their settings. We can use it as a side-tool for command-line commands we introduce below. We can check our Docker installation by running docker info to view some status details. Containers: 0 Running: 0 Paused: 0 Stopped: 0 Images: 0 Server Version: 18.09.2 Storage Driver: overlay2 ... If we experience permission problems, we need to allow our user to work with Docker. Follow the post-installation guidelines. Now we are ready to test the Docker installation by running the command docker run hello-world. If we see a similar message to the one below, we are ready to begin. Unable to find image &#39;hello-world:latest&#39; locally latest: Pulling from library/hello-world 1b930d010525: Pull complete Digest: sha256:6540fc08ee6e6b7b63468dc3317e3303aae178cb8a45ed3123180328bcc1d20f Status: Downloaded newer image for hello-world:latest Hello from Docker! This message shows that your installation appears to be working correctly. ... 3.3 Short introduction to Docker 3.3.1 Basics A Docker container is a runtime instance of an image - that is, an image with state, or a user process. You can see a list of available images using the command docker image ls: REPOSITORY TAG IMAGE ID CREATED SIZE docker-app latest 4b727c80cf90 4 minutes ago 475MB application latest cd497ca7013b 19 hours ago 538MB database latest 428448bc5e2d 19 hours ago 373MB ubuntu 18.04 4c108a37151f 4 weeks ago 64.2MB mysql 5 a1aa4f76fab9 5 weeks ago 373MB hello-world latest fce289e99eb9 6 months ago 1.84kB A container is launched by running an image. An image is an executable package that includes everything needed to run an application - the code, a runtime, libraries, environment variables, and configuration files. We have seen the docker run command that takes image name as a parameter. Image name looks like [username/]repository[:tag], where tag is latest by default. To see all the containers on your machine, issue the command docker container ls --all (the parameter –all will also list stopped containers): CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES f6a55e492493 docker-app &quot;/bin/sh -c &#39;python3…&quot; 14 seconds ago Up 13 seconds 0.0.0.0:8787-&gt;8787/tcp kind_blackwell 42b33bc532ab hello-world &quot;/hello&quot; 22 minutes ago Exited (0) 22 minutes ago unruffled_morse 3.3.2 Docker application example To better understand everything, let’s develop a simple web application and run it within a docker container (solution of this part is available in folder app_only in the GitHub repository). We have a simple Python web server implementation (server.py) from flask import Flask from flask import json from flask import request from flask import Response app = Flask(__name__) @app.route(&#39;/&#39;, methods=[&#39;GET&#39;]) def index(): content = open(&quot;index.html&quot;).read() return Response(content, mimetype=&quot;text/html&quot;) if __name__ == &#39;__main__&#39;: app.run(host=&#39;0.0.0.0&#39;, port=8787) and accompanying HTML web template (index.html) &lt;html&gt; &lt;head&gt; &lt;title&gt;..:: Sample application ::..&lt;/title&gt; &lt;link rel=&quot;stylesheet&quot; href=&quot;https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/css/bootstrap.min.css&quot; integrity=&quot;sha384-MCw98/SFnGE8fJT3GXwEOngsV7Zt27NXFoaoApmYm81iuXoPkFOJwJ8ERdknLPMO&quot; crossorigin=&quot;anonymous&quot;&gt; &lt;script src=&quot;https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/js/bootstrap.min.js&quot; integrity=&quot;sha384-ChfqqxuZUCnJSK3+MXmPNIyE6ZbWh2IMqE241rYiqJxyMiZ6OW/JmZQ5stwEULTy&quot; crossorigin=&quot;anonymous&quot;&gt;&lt;/script&gt; &lt;script src=&quot;https://code.jquery.com/jquery-3.4.1.min.js&quot;&gt;&lt;/script&gt; &lt;script&gt; $( document ).ready(function() { employees = [ {&quot;name&quot;: &quot;John Doe&quot;, &quot;hobbies&quot;: &quot;I like cycling, mountain biking and skiing!!!&quot;, &quot;role&quot;: &quot;Director of operations&quot;}, {&quot;name&quot;: &quot;Melanie Oesch&quot;, &quot;hobbies&quot;: &quot;As the best jodeling singer, the love of my life is singing all day long. I come from Switzerland and have achieved many prizes. Maybe I also visit your hometown and get to know you.&quot;, &quot;role&quot;: &quot;Chairman of kids programme&quot;}, {&quot;name&quot;: &quot;Vladimir Zookeeper&quot;, &quot;hobbies&quot;: &quot;Animals are the nicest and very polite creatures in our world. I have observed many species already and I have not found an animal that would harm me without a reason (in comparison to a human being). My dream is to play with animals every day.&quot;, &quot;role&quot;: &quot;Animal feeder&quot;} ]; function addEmployee(employee) { $(&quot;#personnelListing&quot;).append(` &lt;a href=&quot;#&quot; class=&quot;list-group-item list-group-item-action flex-column align-items-start&quot;&gt; &lt;div class=&quot;d-flex w-100 justify-content-between&quot;&gt; &lt;h5 class=&quot;mb-1&quot;&gt;${employee.name}&lt;/h5&gt; &lt;/div&gt; &lt;p class=&quot;mb-1&quot;&gt;${employee.hobbies}&lt;/p&gt; &lt;small&gt;${employee.role}&lt;/small&gt; &lt;/a&gt; `); } $.each(employees, function( index, employee ) { addEmployee(employee); }); function processForm() { employee = { &quot;name&quot;: $(&quot;#name&quot;).val(), &quot;hobbies&quot;: $(&quot;#hobbies&quot;).val(), &quot;role&quot;: $(&quot;#role&quot;).val() }; addEmployee(employee); $(&quot;#name&quot;).val(&quot;&quot;), $(&quot;#hobbies&quot;).val(&quot;&quot;), $(&quot;#role&quot;).val(&quot;&quot;) } // Fetch all the forms we want to apply custom Bootstrap validation styles to var forms = document.getElementsByClassName(&#39;needs-validation&#39;); // Loop over them and prevent submission var validation = Array.prototype.filter.call(forms, function(form) { form.addEventListener(&#39;submit&#39;, function(event) { event.preventDefault(); event.stopPropagation(); if (form.checkValidity() === true) { processForm(); form.classList.remove(&#39;was-validated&#39;); } else { form.classList.add(&#39;was-validated&#39;); } }, false); }); }); &lt;/script&gt; &lt;/head&gt; &lt;body&gt; &lt;div class=&quot;container&quot;&gt; &lt;div class=&quot;row&quot;&gt; &lt;div class=&quot;col-1&quot;&gt;&lt;/div&gt; &lt;div class=&quot;col-10&quot;&gt; &lt;h1&gt;Personnel listing&lt;/h1&gt; &lt;/div&gt; &lt;div class=&quot;col-1&quot;&gt;&lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;row&quot;&gt; &lt;div class=&quot;col-1&quot;&gt;&lt;/div&gt; &lt;div class=&quot;col-10&quot;&gt; &lt;h2&gt;Add an employee&lt;/h2&gt; &lt;/div&gt; &lt;div class=&quot;col-1&quot;&gt;&lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;row&quot;&gt; &lt;div class=&quot;col-1&quot;&gt;&lt;/div&gt; &lt;div class=&quot;col-10&quot;&gt; &lt;form id=&quot;newEmployeeForm&quot; class=&quot;needs-validation&quot; novalidate&gt; &lt;div class=&quot;form-group&quot;&gt; &lt;label for=&quot;name&quot;&gt;Name&lt;/label&gt; &lt;input type=&quot;text&quot; class=&quot;form-control&quot; id=&quot;name&quot; placeholder=&quot;Enter your name&quot; required&gt; &lt;div class=&quot;invalid-feedback&quot;&gt; Please provide a valid city. &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;form-group&quot;&gt; &lt;label for=&quot;hobbies&quot;&gt;Hobbies&lt;/label&gt; &lt;textarea rows=&quot;3&quot; class=&quot;form-control&quot; id=&quot;hobbies&quot; placeholder=&quot;Describe your hobbies&quot; required&gt;&lt;/textarea&gt; &lt;/div&gt; &lt;div class=&quot;form-group&quot;&gt; &lt;label for=&quot;role&quot;&gt;Role&lt;/label&gt; &lt;input type=&quot;text&quot; class=&quot;form-control&quot; id=&quot;role&quot; placeholder=&quot;Role in the organization&quot; required&gt; &lt;/div&gt; &lt;button type=&quot;submit&quot; class=&quot;btn btn-primary&quot;&gt;Save&lt;/button&gt; &lt;/form&gt; &lt;/div&gt; &lt;div class=&quot;col-1&quot;&gt;&lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;row&quot;&gt; &lt;div class=&quot;col-1&quot;&gt;&lt;/div&gt; &lt;div class=&quot;col-10&quot;&gt; &lt;h2&gt;Employees&lt;/h2&gt; &lt;/div&gt; &lt;div class=&quot;col-1&quot;&gt;&lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;row&quot;&gt; &lt;div class=&quot;col-1&quot;&gt;&lt;/div&gt; &lt;div class=&quot;col-10&quot;&gt; &lt;div class=&quot;list-group&quot; id=&quot;personnelListing&quot;&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;col-1&quot;&gt;&lt;/div&gt; &lt;/div&gt; &lt;/body&gt; &lt;/html&gt; To run the application above, install the Flask library in your Python 3 environment and run the server as python server.py: * Serving Flask app &quot;server&quot; (lazy loading) * Environment: production WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead. * Debug mode: off * Running on http://0.0.0.0:8787/ (Press CTRL+C to quit) Your web application is now accessible at http://localhost:8787. It is a simple JS-based we page, where you can add new employees into a local JS list. 3.3.2.1 Dockerfiles To define a container and be able to create an image, we must write a special file, called Dockerfile. Dockerfile defines what goes on in the environment inside your container. Access to resources like networking interfaces and disk drives is virtualized inside this environment, which is isolated from the rest of our system, so we need to map ports to the outside world, and be specific about what files we want to “copy in” to that environment. After doing that, we can expect that the build of our application defined in this Dockerfile behaves identically wherever it runs. Let’s create a fil named Dockerfile with the following content: # Use an official Ubuntu runtime as a parent image FROM ubuntu:18.04 # Set the current working directory to /work WORKDIR /work # Copy the current directory contents into the container at /work ADD ./ . # Install and configure your environment RUN apt-get update \\ &amp;&amp; apt-get install -y python3 python3-pip \\ &amp;&amp; pip3 install flask # Make port 8787 available to the world outside this container (i.e. docker world) EXPOSE 8787 # Run server.py when the container launches ENTRYPOINT python3 server.py A full list of Dockerfile commands is described at the official Dockerfile documentation. If there is a need to prepare some specifics in the environment, it can be tedious to manually write all the RUN commands and check whether the application runs as expected. We can therefore always directly run the parent image and enter it’s shell to manually prepare the environment. We then copy the working commands into a Dockerfile and build our image. To create a container from a specific image and access it’s shell, run the following command: docker run -it ubuntu:18.04 /bin/bash. 3.3.2.2 Adding everything together Now we have the files server.py, index.html and Dockerfile in the same folder. If we move to the same folder, we can build a Docker image named docker-app using the following command docker build -t docker-app .: Sending build context to Docker daemon 9.728kB Step 1/6 : FROM ubuntu:18.04 ---&gt; 4c108a37151f .... Successfully built 4b727c80cf90 Successfully tagged docker-app:latest Now the image is available in our Docker system (verify that using docker image command as we did above). To create a running container based on our image, run docker run -p 8787:8787 docker-app: * Serving Flask app &quot;server&quot; (lazy loading) * Environment: production WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead. * Debug mode: off * Running on http://0.0.0.0:8787/ (Press CTRL+C to quit) With this command we also map the host’s port 8787 to the Docker container’s port 8787. For additional options of docker run command, see the official reference. Our web application is now accessible at the same address as before, but is now running in a Docker container! Sometimes we would still like to directly access the shell of a running Docker container and check something (another option would be to install an ssh server into the container and map a port to the host). First, we get the ID of our container using the command docker ps, then we execute docker exec -it CONTAINER_ID /bin/bash. Now we are connected directly to the “machine” that hosts our web application. If our running application generates logs, we can easily access them using docker logs CONTAINER_ID. 3.3.3 Volumes By default all files created inside a container are stored on a writable container layer. Note that: The data doesn’t persist when that container no longer exists and it can be difficult to get the data out of the container if another process needs it. A container’s writable layer is tightly coupled to the host machine on which the container is running. You can’t easily move the data somewhere else. Writing into a container’s writable layer requires a storage driver to manage the filesystem. The storage driver provides a union filesystem, using the Linux kernel. This extra abstraction reduces performance as compared to using data volumes, which write directly to the host filesystem. As a result, best practices are to always create containers read-only. Docker thus provides two solutions to store files, which are persisted also after the container removal - volumes and bind mounts. Volumes are the best way to persist data in Docker. Commands to manage Docker volumes start with docker volume (add –help parameter to list all options). After the volume is created, we can map it to a specific mount point when running the container (see docker run command reference for more). 3.3.4 Docker application example with multiple services When developing an application that consist of multiple services, we can also create one “fat” container hosting all the services. This is not the Docker best practice as therefore each service should be run within a separate Docker container. Let’s upgrade our web application and add database support to it (solution of this part is available in folder app_and_database in the GitHub repository). 3.3.4.1 Web application service The web application service is similar to the example above. The differences are the installation of an additional mysql-connector-python library into the container (Dockerfile), implementation of REST calls to the backend (index.html) and server endpoints implementation (server.py). In the implementation we should notice the connection settings from the application to the database. As we said, both services will run separately in a Docker container, so we should provide a connection. When we run the services, we should name the containers and create a network among running services. Connection settings look like: config = { &#39;user&#39;: &#39;root&#39;, &#39;password&#39;: &#39;BeReSeMi.DataScience&#39;, &#39;host&#39;: &#39;database&#39;, &#39;database&#39;: &#39;data_science&#39;, &#39;raise_on_warnings&#39;: False } Note that the host setting and port 3306 are used to connect to a MySQL database. In the MySQL container that port will also needed to be exposed within the Dockerfile. Docker services can be connected using separate virtual networks and by default, bridge networks are created to interconnect running containers. For more sophisticated examples, see the official network setting guidelines. 3.3.4.2 Database service For the database service, we will use already created image, which is available from the public MySQL image repository (for more info on repositories, see section 3.3.4.4). Along with the image, there are also instructions of how to use it with the parameters, where to put initial script files, versions, Dockerfile sources and other references, which ease the deployment process without creating complicated Dockerfiles by ourselves. In our case the database dockerfile looks as follows: # Parent image FROM mysql:5 # Copying all files in curent dir to container ADD ./ /docker-entrypoint-initdb.d # Updating permissions for the copied files RUN /bin/bash -c &#39;chmod a+rx /docker-entrypoint-initdb.d/*&#39; Observe that we only copied files into a special folder in the container (/docker-entrypoint-initdb.d) which was previously set as init folder, where all the .sql scripts are run at startup. To test our Dockerfile, we can also run it separately using the command (from the database folder) docker build -t docker-db . &amp;&amp; docker run -p 3306:3306 -e MYSQL_ROOT_PASSWORD=BeReSeMi docker-db. We set the database password of the root user using an environment variable, which can be used to connect to the database. Now we can connect to the database from the host machine - e.g. use MySQL Workbench and connection settings to localhost:3306. A database named data_science should now be created along with the employees table and few entries. 3.3.4.3 Docker compose Now we know how to create docker containers separately. What remains is to connect them and run them more easily. We will utilize the Docker compose tool. Docker Compose is a tool for defining and running multi-container Docker applications. It uses YAML files to configure the application’s services and performs the creation and start-up process of all the containers with a single command. The docker-compose CLI utility allows us to run commands on multiple containers at once, for example, building images, scaling containers, running containers that were stopped, and more. The docker-compose.yml file is used to define an application’s services and includes various configuration options. Before proceeding, we’ll stop all running containers and move to the app_and_database folder. First, let’s check the docker-compose.yml file: version: &#39;3.7&#39; services: database: image: database build: ./database command: --default-authentication-plugin=mysql_native_password restart: always environment: MYSQL_ROOT_PASSWORD: BeReSeMi.DataScience ports: - 3306:3306 volumes: - ds_databases:/var/lib/mysql networks: - dsnet application: image: application build: ./application depends_on: - database restart: always ports: - &quot;8787:8787&quot; networks: - dsnet networks: dsnet: driver: bridge volumes: ds_databases: In the docker-compose configuration we define two containers, one named database and other application. Parameter build defines the folder with an appropriate Dockerfile or directly an already built image name. Parameter restart instructs the container to automaticallt restart its service if it suddenly stops due to crash or error. Parameter depends_on created dependencies between containers, so the application container will be started after the database container is running. Be aware that this is based on the time when service is initializing, so if database script initialization takes a long time, the database service may run later than the application service. With the volumes parameter we map a specific folder in the container to a named volume db_databases - this folder will contain the database data. We also define a network named dsnet with the bridge configuration and therefore both containers will be able to communicate with each other. To run the docker-compose configuration, run docker-compose up (for detached mode add parameter -d at the end). Both containers should be created and running - to verify, navigate to http://localhost:8787 and add some employees. Data should be persistent and stored into the database. To shutdown the containers, press CTRL+C (or run docker-compose down if you started containers in a detached mode). 3.3.4.4 Image registries We have already mentioned that there exist prepared images, which can be retried from the public repositories. Repositories can be public or private. Two main public registries are Docker Hub and Docker Cloud. Docker Hub is the default registry where Docker looks for images. Docker clients connect to Docker repositories to download (pull) images for use or upload (push) images that they have built. We have published the image of the first example in the repository szitnik/docker-ds-app (link). You can pull the image (docker pull command) or run it directly with the command: docker run -p 8787:8787 szitnik/docker-ds-app The functionality should be the same as with the image you created during this tutorial. To publish images to Docker Hub, we first need to create and account. For example, we could take the built image docker-app from the first example and push it to the Docker Hub using the following commands: # Provide credentials and login docker login # Tag your local image docker tag docker-app USERNAME/PUBLIC_IMAGE_NAME:TAG # Publish your image docker push USERNAME/PUBLIC_IMAGE_NAME:TAG If we do not define the tag, it will be latest by default. We can access our image via Web interface at https://hub.docker.com/r/USERNAME/PUBLIC_IMAGE_NAME*, where we can also add a description, instructions, publish the Dockerfile* or connect its content to your Git repository. Now anybody can pull &amp; run our image using the above procedure. 3.4 Further reading and references Check the Docker official web page and the Docker tutorial. For more advanced topics get to know container orchestration solutions like Docker Swarm or Kubernetes. Docker in Action book teaches readers how to create, deploy, and manage applications hosted in Docker containers. The Docker Book is inteded for complete beginners offering 268 pages of demos and live tutorials. Docker Cookbook presents more advanced Docker techniques like mounting data across multiple servers, distributed containers, detailed monitoring, networking across multiple hosts, accessing Docker in the cloud and merging with other platforms like Kubernetes. 3.5 Learning outcomes Data science students should work towards obtaining the knowledge and the skills that enable them to: Use and extend existing Docker image. Package an application consisting of multiple services into one or more containers and manage them. Publish reproducible algorithms to Docker Hub. 3.6 Practice problems Create a simple Python program or web application, write a Dockerfile to compile an image an publish it on Docker Hub. If you are more proficient with some other language, use a framework of your choice. Write a service which needs multiple servers deployed and run everything using Docker Compose. Dockerize your Introduction to Data Science project and publish it to Docker Hub. Add instructions of how to use your image. References "],
["web-scraping.html", "Chapter 4 Web scraping", " Chapter 4 Web scraping Now let’s talk details. TODO: dodati branje podatkov iz JSON-a, XML-ja "],
["summarizing-data-the-basics.html", "Chapter 5 Summarizing data - the basics 5.1 Descriptive statistics for univariate distributions 5.2 Descriptive statistics for bivariate distributions 5.3 Further reading and references 5.4 Learning outcomes 5.5 Practice problems", " Chapter 5 Summarizing data - the basics Data summarization is the science and art of conveying information more effectivelly and efficiently. Data summarization is typically numerical, visual or a combination of the two. It is a key skill in data analysis - we use it to provide insights both to others and to ourselves. Data summarization is also an integral part of exploratory data analysis. In this chapter we focus on the basic techniques for univariate and bivariate data. Visualization and more advanced data summarization techniques will be covered in later chapters. 5.1 Descriptive statistics for univariate distributions We humans are not particularly good at thinking in multiple dimensions, so, in practice, there will be a tendency to look at individual variables and dimensions. That is, in practice, we will most of the time be summarizing univariate distributions. Univariate distributions come from various sources. It might be a theoretical distribution, an empirical distribution of a data sample, a probabilistic opinion from a person, a posterior distribution of a parameter from a Bayesian model, and many others. Descriptive statistics apply to all of these cases in the same way, regardless of the source of the distribution. Before we proceed with introducing the most commonly used descriptive statistics, we discuss their main purpose. The main purpose of any sort of data summarization technique is to (a) reduce the time and effort of delivering information to the reader in a way that (b) we lose as little relevant information as possible. That is, to compress the information. All summarization methods do (a) but we must be careful to choose an appropriate method so that we also get (b). Summarizing out relevant information can lead to misleading summaries, as we will illustrate with several examples. 5.1.1 Central tendency The most common first summary of a distribution is its typical value, also known as the location or central tendency of a distribution. The most common summaries of the location of a distribution are: the mean (the mass centre of the distribution), the median or 2nd quartile (the value such that half of the mass is on one and half on the other side), the mode (the most probable value or the value with the highest density). Given a sample of data, the estimate of the mean is the easiest to compute (we compute the average), but the median and mode are more robust to outliers - extreme and possibly unrepresentative values. In the case of unimodal approximately symmetrical distributions, such as the univariate normal distribution, all these measures of central tendency will be similar and all will be an excellent summary of location. However, if the distribution is asymmetrical or skewed, they will differ. In such cases it is our job to determine what information we want to convey and which summary of central tendency is the most appropriate, if any. For example, observe the Gamma(1.5, 0.1) distribution and its mean (red), median (blue) and mode (green): x &lt;- seq(0, 50, 0.01) a &lt;- 1.5 b &lt;- 0.1 y &lt;- dgamma(x, a, b) library(ggplot2) ggplot(data.frame(x,y), aes(x = x, y = y)) + geom_line() + ylab(&quot;p(x)&quot;) + geom_vline(xintercept = (a-1) / b, colour = &quot;green&quot;, lty = &quot;dashed&quot;, lwd = 1.5) + geom_vline(xintercept = a / b, colour = &quot;red&quot;, lty = &quot;dashed&quot;, lwd = 1.5) + geom_vline(xintercept = qgamma(0.5, a, b), colour = &quot;blue&quot;, lty = &quot;dashed&quot;, lwd = 1.5) In the case of multi-modal distributions, no single measure of central tendency will adequately summarize the distribution - they will all be misleading. For example, look at this bimodal distribution: x &lt;- seq(-10, 20, 0.01) y &lt;- 0.6 * dnorm(x, 2, 1) + 0.4 * dnorm(x, 12, 2) library(ggplot2) ggplot(data.frame(x,y), aes(x = x, y = y)) + geom_line() + ylab(&quot;p(x)&quot;) + geom_vline(xintercept = 0.6* 2 + 0.4 * 15, colour = &quot;red&quot;, lty = &quot;dashed&quot;, lwd = 1.5) + geom_vline(xintercept = 2, colour = &quot;green&quot;, lty = &quot;dashed&quot;, lwd = 1.5) 5.1.2 Dispersion Once location is established, we are typically interested in whether the values of the distribution cluster close to the location or are spread far from the location. The most common ways of measuring such dispersion (or spread or scale) of a distribution are: variance (mean of quadratic distances from mean) or, more commonly, standard deviation (root of variance, so we are on the same scale as the measurement) median absolute deviation (median of absolute distances from mean), quantile-based intervals, in particular the inter-quartile range (IQR) (interval between the 1st and 3rd quartiles, 50% of the mass/density lies in this interval). Standard deviation is the most commonly used and median absolute deviation is more robust to outliers. Again, in the case of distributions that are approximately normal, the standard deviation and the mean will be the practically optimal choice for summarization, because they correspond directly to the two parameters of the normal distribution. That is, they completely summarize the distribution without loss of information. We also know that approximately 95% (99%) of the normal density lies within 2 (3) standard deviations from the mean. Standard deviation is useful even if the distribution is not approximately normal as it does provide some information, combined with the sample size (producing the standard error), on how certain we can be in our estimate of the mean. But, as before, the more we deviate from normality, the less meaningful standard deviation becomes and it makes more sense to use quantile-based intervals. For example, if we estimate the mean and \\(\\pm\\) 2 standard deviations for samples from the Gamma distribution from before, we get the following: set.seed(0) x &lt;- rgamma(1000, a, b) cat(sprintf(&quot;%.2f +/- %.2f\\n&quot;, mean(x), 2*sd(x))) ## 14.66 +/- 24.49 That is, the 95% interval estimated this way also includes negative values, which is misleading and absurd - Gamma distributed variables are positive. Computing the IQR or the 95% range interval provides a more sensible summary of this skewed distribution and, together with the mean also serve as an indicator that the distribution is skewed (the mean is not the centre of the intervals): set.seed(0) x &lt;- rgamma(1000, a, b) cat(sprintf(&quot;%.2f, IQR = [%.2f, %.2f], 95pct = [%.2f, %.2f]\\n&quot;, mean(x), quantile(x, 0.25), quantile(x, 0.75), quantile(x, 0.025), quantile(x, 0.975))) ## 14.66, IQR = [5.72, 20.76], 95pct = [1.18, 46.09] And, again, for multi-modal distributions, we can adequately summarize them only by identifying the modes visually and/or describing each mode individually. 5.1.3 Skewness and kurtosis As mentioned above, ranges can be used to indicate a distributions asymmetry (skewness) or fat-tailedness (kurtosis). Although less commonly used, there exist numerical summaries of skewness and kurtosis that can be used instead. The following example shows the kurtosis and skewness for a gamma, normal, logistic and bimodal distribution. Observe how how the standard way of calculating kurtosis fails for the bimodal and assigns it the lowest kurtosis: library(moments) set.seed(0) tmp &lt;- NULL for (i in 1:4) { if (i == 1) x &lt;- rgamma(1000, a, b) if (i == 2) x &lt;- rnorm(1000, 0, 9) if (i == 3) x &lt;- rlogis(1000, 0, 9) if (i == 4) x &lt;- c(rnorm(700, 0, 9), rnorm(300, 35, 4)) s &lt;- paste0(&quot;kurtosis = &quot;, round(kurtosis(x), 2), &quot; skewness =&quot;, round(skewness(x), 2)) tmp &lt;- rbind(tmp, data.frame(x = x, name = s)) } ggplot(tmp, aes(x = x)) + geom_histogram(bins = 50) + facet_wrap(.~name) 5.1.4 Nominal variables Nominal variables are typically represented with the relative frequencies or probabilities, numerically or visually. Note that the methods discussed so far in this chapter apply to numerical variables (rational, interval and to some extent, ordinal) but not nominal variables, because the notions of location and distance (dispersion) do not exist in the nominal case. The only exception to this is the mode, which is the level of the nominal variable with the highest relative frequency or probability. One summary that is often useful for summarizing the dispersion or the uncertainty associated with a nominal variable is entropy. Observe the following example: entropy &lt;- function(x) { x &lt;- x[x != 0] -sum(x * log2(x)) } entropy(c(0.5, 0.5)) # fair coin ## [1] 1 entropy(c(0.8, 0.2)) # biased coin ## [1] 0.7219281 entropy(c(1.0, 0.0)) # coin with heads on both sides ## [1] 0 A fair coin has exactly 1 bit of entropy - we receive 1 bit of information by observing the outcome of a flip. This is also the maximum achievable entropy for a binary variable. A biased coin has lower entropy - we receive less information. In the extreme case of a coin with heads on both sides, the entropy is 0 - the outcome of a flip brings no new information, as we already know it will be heads. When we want to compare entropy across variables with different numbers of levels/categories, we can normalize it by dividing it with the maximum achieavable entropy. For example, observe a fair coin and a fair 6-sided die - in absolute terms, the 6-sided die has higher entropy due to having more possible values. However, relatively to the maximum achievable entropy, both represent maximally uncertain distributions: entropy(c(0.5, 0.5)) # fair coin ## [1] 1 entropy(rep(1/6, 6)) # fair 6-sided die ## [1] 2.584963 Note that entropy can easily be calculated for any discrete random variable. Entropy also has a continuous analogue - differential entropy, which we will not discuss here. 5.1.5 Testing the shape of a distribution Often we want to check if the distribution that underlies our data has the shape of some hypothesized distribution (for example, the normal distribution) or if two samples come from the same distribution. Here, we will present two of the most common methods used: the Kolmogorov-Smirnov test and the *Chi-squared goodness-of-fit test. Both of these are Null-hypothesis significance tests (NHST), so, before we proceed, be aware of two things. First, do not use NHST blindly, without a good understanding of their properties and how to interpret their results. And second, if you are more comfortable with thinking in terms of probabilities of hypotheses as opposed to significance and p-values, there always exist Bayesian alternatives to NHST. The Kolmogorov-Smirnov test (KS) is a non-parametric test for testing the equality of two cumulative distribution functions (CDF). These can be two empirical CDFs or an empirical CDF and a theoretical CDF. The KS test statistic is the maximum distance between the two corresponding CDFs. That is, we compute the distribution of this statistic under the null-hypothesis that the CDFs are the same and then observe how extreme the maximum distance is on the sample. To illustrate the KS test, we use it to test the normality of the underlying distributions for two samples - one from a logistic distribution, one from a standard normal distribution. And then to test if the two samples come from the same distribution: set.seed(0) x1 &lt;- rlogis(80, 0, 1) x2 &lt;- rnorm(80, 0, 1) ks.test(x1, y = &quot;pnorm&quot;, 0, 1) ## ## One-sample Kolmogorov-Smirnov test ## ## data: x1 ## D = 0.1575, p-value = 0.03362 ## alternative hypothesis: two-sided ks.test(x2, y = &quot;pnorm&quot;, 0, 1) ## ## One-sample Kolmogorov-Smirnov test ## ## data: x2 ## D = 0.070067, p-value = 0.801 ## alternative hypothesis: two-sided ks.test(x1, x2) ## ## Two-sample Kolmogorov-Smirnov test ## ## data: x1 and x2 ## D = 0.175, p-value = 0.173 ## alternative hypothesis: two-sided So, with a 5% risk (95% confidence), we would reject the null hypothesis that our sample x1 is from a standard normal distribution. On the other hand, we would not reject that our sample x1 is from a standard normal distribution. Finally, we would not reject the null hypothesis that x1 and x2 come from the same distribution. The only guarantee that comes with these results is that we will in the long run falsely reject a true null-hypothesis at most 5% of the time. It says very little about our overall performance, because we do not know the ratio of cases when the null-hypothesis will be true. This example also illustrates the complexity of interpreting NHST results or rather all the tempting traps laid out for us - we might be tempted to conclude, based on the high p-value, that x2 does indeed come from a standard normal, but that then leads us to a weird predicament that we are willing to claim that x1 is not standard normal, x2 is standard normal, but we are less sure that x1 and x2 have different underlying distributions. Note that typical implementations of the KS test assume that the underlying distributions are continuous and ties are therefore impossible. However, the KS test can be generalized to discrete and mixed distributions (see R package KSgeneral). Differences in between distributions can also be assessed visually, through the QQ-plot, a plot that compares the quantiles of the two distributions. If the distributions have the same shape, their quantiles, plotted together, should lie on a line. The samples from the logistic distribution obviously deviate from the theoretical quantiles of a normal distribution: tmp &lt;- data.frame(y = x1) ggplot(tmp, aes(sample = y)) + stat_qq() + stat_qq_line() The Chi-squared goodness-of-fit (CHISQ) test is a non-parametric test for testing the equality of two categorical distributions. The CHISQ test can also be used on discrete or even continuous data, if there is a reasonable way of binning the data into a finite number of bins. The test statistic is based on a similar idea as the KS test statistic, but instead of observing just the maximum difference, we sum the squared difference between the relative frequency of the two distributions for a bin across all bins. We illustrate the CHISQ test by testing the samples for a biased coin against a theoretical fair coin and the samples from an unbiased 6-sided die against a theoretical fair 6-sided die. set.seed(0) x &lt;- table(sample(0:1, 30, rep = T, prob = c(0.7, 0.3))) chisq.test(x, p = c(0.5, 0.5)) # the default is to compare with uniform theoretical, but we make it explicit here ## ## Chi-squared test for given probabilities ## ## data: x ## X-squared = 3.3333, df = 1, p-value = 0.06789 x &lt;- table(sample(0:1, 40, rep = T, prob = c(0.7, 0.3))) chisq.test(x, p = c(0.5, 0.5)) ## ## Chi-squared test for given probabilities ## ## data: x ## X-squared = 10, df = 1, p-value = 0.001565 x &lt;- table(sample(1:6, 40, rep = T)) chisq.test(x) ## ## Chi-squared test for given probabilities ## ## data: x ## X-squared = 5.3, df = 5, p-value = 0.3804 So, with a 5% risk (95% confidence), we would reject the null hypothesis that our coin is fair, but only in the case with 40 samples. Because fair or close-to fair coins have high entropy, we typically require a lot of samples to distinguish between their underlying probabilities. For a more real-world example, let us take the exit-poll data for the 2016 US Presidential election, broken down by gender, taken from here: n &lt;- 24588 male &lt;- round(0.47 * n * c(0.41, 0.52, 0.07)) # some rounding, but it should not affect results female &lt;- round(0.53 * n * c(0.54, 0.41, 0.05)) x &lt;- rbind(male, female) colnames(x) &lt;- c(&quot;Clinton&quot;, &quot;Trump&quot;, &quot;other/no answer&quot;) print(x) # this is also known as a contingency table and the subsequent test as a contingency test ## Clinton Trump other/no answer ## male 4738 6009 809 ## female 7037 5343 652 chisq.test(x) ## ## Pearson&#39;s Chi-squared test ## ## data: x ## X-squared = 417.71, df = 2, p-value &lt; 2.2e-16 So, at any reasonable confidence level, we would reject the null-hypothesis and conclude that there is a difference in how men and women voted. In fact, we do not even need a test, because the difference is so obvious and the sample size so large. The differences between those who earned less or more than 100k$, however, appear smaller, so a test makes more sense: n &lt;- 24588 less100 &lt;- round(0.66 * n * c(0.49, 0.45, 0.06)) # some rounding, but it should not affect results more100 &lt;- round(0.34 * n * c(0.47, 0.47, 0.06)) x &lt;- rbind(less100, more100) colnames(x) &lt;- c(&quot;Clinton&quot;, &quot;Trump&quot;, &quot;other/no answer&quot;) print(x) ## Clinton Trump other/no answer ## less100 7952 7303 974 ## more100 3929 3929 502 chisq.test(x) ## ## Pearson&#39;s Chi-squared test ## ## data: x ## X-squared = 9.3945, df = 2, p-value = 0.00912 Still, we can at most typical levels of confidence reject the null-hypothesis and conclude that that there is a pattern here as well. 5.2 Descriptive statistics for bivariate distributions When dealing with a joint distribution of two variables (that is, paired samples), the first thing we are typically interested in is dependence between the two variables or lack thereof. If two distributions are independent, we can summarize each separately without loss of information. If they are not, then the distributions carry information about eachother. The predictability of one variable from another is another (equivalent) way of looking at dependence of variables. The most commonly used numerical summary of dependence is the Pearson correlation coefficient or Pearson’s \\(\\rho\\). It summarizes the linear dependence, with \\(\\rho = 1\\) and \\(\\rho = - 1\\) indicating perfect colinearity (increasing or decreasing) and \\(\\rho = 0\\) indicating linear independence. As such, Pearson’s \\(\\rho\\) is directly related (the squared root) to the coefficient of determination \\(R^2\\), a goodness-of-fit measure for linear models and the proportion of variance in one explained by the other variable. An important consideration is that the statement that linear independence implies independence is not true in general (the converse implication is). One notable exception where this implication is true is the multivariate Normal distribution, where the dependence structure is expressed through linear dependence only. Two of the most popular alternatives to Pearson’s \\(\\rho\\) are Spearman’s \\(\\rho\\) and Kendalls \\(\\tau\\). The former measures the degree to which one variable can be expressed as monotonic function of the other. The latter measures the proportion of concordant pairs among all possible pairs (pairs (x1,y1) and (x2, y2), wher if x1 &gt; x2 then y1 &gt; y2). As such, they can capture non-linear dependence and is more appropriate for data with outliers or data where distance might have no meaning, such as ordinal data. Spearman’s \\(\\rho\\) and Kendall’s \\(\\tau\\) are more robust but do not have as clear an interpretation as Pearson’s \\(\\rho\\). Kendall’s tau is also computationally more expensive. Below are a few examples of bivariate samples that illustrate the strengths and limitations of the above correlation coefficients: set.seed(0) library(MASS) m &lt;- 100 dat &lt;- NULL # data 1 sigma &lt;- matrix(c(1, 0, 0, 1), nrow = 2) x &lt;- mvrnorm(m, mu = c(0, 0), Sigma = sigma ) txt &lt;- sprintf(&quot;Pearson&#39;s = %.2f\\nSpearman&#39;s = %.2f\\nKendall&#39;s = %.2f&quot;, cor(x)[1,2], cor(x, method = &quot;spearman&quot;)[1,2], cor(x, method = &quot;kendall&quot;)[1,2]) dat &lt;- rbind(dat, data.frame(x = x[,1], y = x[,2], example = txt)) # data 2 sigma &lt;- matrix(c(1, -0.5, -0.5, 1), nrow = 2) x &lt;- mvrnorm(m, mu = c(0, 0), Sigma = sigma ) txt &lt;- sprintf(&quot;Pearson&#39;s = %.2f\\nSpearman&#39;s = %.2f\\nKendall&#39;s = %.2f&quot;, cor(x)[1,2], cor(x, method = &quot;spearman&quot;)[1,2], cor(x, method = &quot;kendall&quot;)[1,2]) dat &lt;- rbind(dat, data.frame(x = x[,1], y = x[,2], example = txt)) # data 3 sigma &lt;- matrix(c(1, 0.95, 0.95, 1), nrow = 2) x &lt;- mvrnorm(m, mu = c(0, 0), Sigma = sigma ) txt &lt;- sprintf(&quot;Pearson&#39;s = %.2f\\nSpearman&#39;s = %.2f\\nKendall&#39;s = %.2f&quot;, cor(x)[1,2], cor(x, method = &quot;spearman&quot;)[1,2], cor(x, method = &quot;kendall&quot;)[1,2]) dat &lt;- rbind(dat, data.frame(x = x[,1], y = x[,2], example = txt)) # data 4 x &lt;- rnorm(m, 0, 1) y &lt;- 2 * pnorm(x) + rnorm(m, 0, 0.05) x &lt;- cbind(x, y) txt &lt;- sprintf(&quot;Pearson&#39;s = %.2f\\nSpearman&#39;s = %.2f\\nKendall&#39;s = %.2f&quot;, cor(x)[1,2], cor(x, method = &quot;spearman&quot;)[1,2], cor(x, method = &quot;kendall&quot;)[1,2]) dat &lt;- rbind(dat, data.frame(x = x[,1], y = x[,2], example = txt)) # data 5 sigma1 &lt;- matrix(c(1, -0.5, -0.5, 1), nrow = 2) sigma2 &lt;- matrix(c(0.1, 0, 0, 0.1), nrow = 2) x &lt;- rbind(mvrnorm(m, mu = c(0, 0), Sigma = sigma ), mvrnorm(50, mu = c(3, -2), Sigma = sigma2 )) txt &lt;- sprintf(&quot;Pearson&#39;s = %.2f\\nSpearman&#39;s = %.2f\\nKendall&#39;s = %.2f&quot;, cor(x)[1,2], cor(x, method = &quot;spearman&quot;)[1,2], cor(x, method = &quot;kendall&quot;)[1,2]) dat &lt;- rbind(dat, data.frame(x = x[,1], y = x[,2], example = txt)) # data 6 z &lt;- runif(m, 0, 2*pi) x &lt;- cbind(cos(z), sin(z)) txt &lt;- sprintf(&quot;Pearson&#39;s = %.2f\\nSpearman&#39;s = %.2f\\nKendall&#39;s = %.2f&quot;, cor(x)[1,2], cor(x, method = &quot;spearman&quot;)[1,2], cor(x, method = &quot;kendall&quot;)[1,2]) dat &lt;- rbind(dat, data.frame(x = x[,1], y = x[,2], example = txt)) ggplot(dat, aes(x = x, y = y)) + geom_point() + facet_wrap(.~example, ncol = 3, scales = &quot;free&quot;) Like similar questions about other parameters of interest, the question Is a strong correlation? is a practical question. Unless the correlation is 0 (no correlation) or 1/-1 (perfectly correlated, can’t be more correlated than this), the meaning of the magnitude of correlation depends on the practical setting and its interpretation depends on some reference level. Even a very low correlation, such as 0.001 (if we are reasonably sure that it is around 0.001) can be practically meaningful. For example, if it is correlation between the even and odd numbers generated by a uniform random number generator (RNG), that would be more than enough correlation to stop using this RNG. 5.3 Further reading and references For a more comprehensive treatment of the most commonly used summarization techniques see: Holcomb, Z. C. (2016). Fundamentals of descriptive statistics. Routledge. More on the practice of summarization techniques and hypothesis testing: Bruce, P., &amp; Bruce, A. (2017). Practical statistics for data scientists: 50 essential concepts. &quot; O’Reilly Media, Inc.“. (Chapters 1 and 3) 5.4 Learning outcomes Data science students should work towards obtaining the knowledge and the skills that enable them to: Reproduce the techniques demonstrated in this chapter using their language/tool of choice. Recognize when a type of summary is appropriate and when it is not. Apply data summarization techiques to obtain insights from data. Once introduced to the bootstrap and other estimation techniques, to be able to combine descriptive statistics with a quantification of uncertainty, such as confidence intervals. 5.5 Practice problems Download the Football Manager Players dataset or use a similarly rich dataset with numerical, binary and categorical variables. With Python or R demonstrate the application and interpretation of results for each of the summarization techniques from this chapter. Find one or more real-world examples (data sets) where a standard summary of univariate or bivariate data fails. That is, where important information is lost in the summary. "],
["summarizing-data-visualization.html", "Chapter 6 Summarizing data - visualization 6.1 Histograms and density plots 6.2 Bar plot 6.3 Pie chart 6.4 Scatterplot 6.5 2D density plot 6.6 Boxplot 6.7 Violin plot 6.8 Correlogram 6.9 A comprehensive summary 6.10 Further reading and references 6.11 Learning outcomes 6.12 Practice problems", " Chapter 6 Summarizing data - visualization Data summarization is the science and art of conveying information more effectivelly and efficiently. Data summarization is typically numerical, visual or a combination of the two. It is a key skill in data analysis - we use it to provide insights both to others and to ourselves. Data summarization is also an integral part of exploratory data analysis. In this chapter we will focus on the basic visualization techniques for univariate and bivariate data. Advanced data summarization techniques will be covered in a later chapter. We will be using R and ggplot2 but the contents of this chapter are meant to be tool-agnostic. Readers should use the programming language and tools that they are most comfortable with. However, do not sacrifice expresiveness or profesionallism for the sake of convenience - if your current toolbox limits you in any way, learn new tools! In most examples in this section we’ll be using the NBA players dataset that contains some basic information about NBA players in the period up to year 2009. library(ggplot2) dat &lt;- read.csv(&quot;./data/NBAplayers.csv&quot;) dat &lt;- dat[complete.cases(dat),] dat$height &lt;- dat$h_feet * 30.48 + dat$h_inches * 2.54 # in cm dat$weight &lt;- dat$weight * 0.4536 summary(dat) ## ilkid firstname lastname position firstseason ## ABDELAL01: 1 John : 90 Williams: 66 C: 627 Min. :1946 ## ABDULKA01: 1 Bob : 88 Smith : 54 F:1665 1st Qu.:1967 ## ABDULMA01: 1 Jim : 70 Johnson : 52 G:1614 Median :1980 ## ABDULTA01: 1 Mike : 66 Jones : 44 Mean :1979 ## ABDURSH01: 1 Bill : 54 Davis : 37 3rd Qu.:1995 ## ABERNTO01: 1 Tom : 46 Brown : 34 Max. :2009 ## (Other) :3900 (Other):3492 (Other) :3619 ## lastseason h_feet h_inches weight ## Min. :1946 Min. :5.000 Min. :-6.000 Min. : 60.33 ## 1st Qu.:1970 1st Qu.:6.000 1st Qu.: 3.000 1st Qu.: 83.92 ## Median :1985 Median :6.000 Median : 6.000 Median : 92.99 ## Mean :1983 Mean :6.022 Mean : 5.581 Mean : 93.91 ## 3rd Qu.:2001 3rd Qu.:6.000 3rd Qu.: 8.000 3rd Qu.:102.06 ## Max. :2009 Max. :7.000 Max. :11.500 Max. :149.69 ## ## college birthdate ## : 200 1945-01-01 00:00:00: 8 ## University of California - Los Angeles: 72 1944-01-01 00:00:00: 5 ## University of North Carolina : 71 1921-01-01 00:00:00: 4 ## University of Kentucky : 67 1931-01-01 00:00:00: 4 ## Indiana University : 54 1919-01-01 00:00:00: 3 ## University of Kansas : 52 1923-09-18 00:00:00: 3 ## (Other) :3390 (Other) :3879 ## height ## Min. :160.0 ## 1st Qu.:190.5 ## Median :198.1 ## Mean :197.7 ## 3rd Qu.:205.7 ## Max. :231.1 ## Visual summaries are often a more informative, faster and more concise alternative to numerical summaries. This will also be our guideline for improving our visualizations. Can we convey the same information in substantially less time/space? Can we convey more information without using more time/space? If the answer is yes, then we should! We’ll illustrate this point with an example that features some common mistakes or inefficiencies people do when visualizing data. Our goal will be to summarize how the average height of NBA Centers, Forwards, and Guards that started that year has changed over time. Let’s plot the averages over time: tmp &lt;- dat[dat$position == &quot;G&quot;,] plot(tapply(tmp$height, tmp$firstseason, mean)) tmp &lt;- dat[dat$position == &quot;F&quot;,] plot(tapply(tmp$height, tmp$firstseason, mean)) tmp &lt;- dat[dat$position == &quot;C&quot;,] plot(tapply(tmp$height, tmp$firstseason, mean)) The plots reveal information that the average height of rookies has been increasing over time for all three groups of players. However, we should immediately recognize that plotting 3 things separately that could be plotted on the same plot wastes space. It also wastes the reader’s time, having to jump back and forth from one plot to another. This also makes comparison of the time series very difficult. Remember that how we present our results is how we treat our reader. These kind of plots say to the reader that our time and convenience are more important than theirs. Let’s remedy this mistake: tmp &lt;- dat[dat$position == &quot;G&quot;,] plot(tapply(tmp$height, tmp$firstseason, mean), col = &quot;green&quot;, ylim = c(180, 220)) tmp &lt;- dat[dat$position == &quot;F&quot;,] points(tapply(tmp$height, tmp$firstseason, mean), col = &quot;red&quot;) tmp &lt;- dat[dat$position == &quot;C&quot;,] points(tapply(tmp$height, tmp$firstseason, mean), col = &quot;black&quot;) This plot uses up only a third of the space and simplifies comparison. This reveals new information, such as that centers (black) are higher than forwards (red) who are higher than guards (green), that there was a more steep increase in the beginning of the period and that forwards these days are as high on average as centers were in the past. However, there are several things that we can still improve on. The first will be one of the fundamental rules of statistical plotting - always label your axes! The reader should never look elsewhere for information about what is plotted. We will also take this opportunity to reorganize our data: library(reshape2) tmp &lt;- melt(tapply(dat$height, list(dat$position, dat$firstseason), mean)) names(tmp) &lt;- c(&quot;position&quot;, &quot;year&quot;, &quot;height&quot;) plot(tmp$year, tmp$height, col = tmp$position, xlab = &quot;year&quot;, ylab = &quot;average rookie height (cm)&quot;) This is starting to look better. However, we should also include the legend - if we describe the meaning of the colors in the figure caption (or worse, in text), the reader will have to jump from text to figure, wasting time. Additionally, some people (and publications) prefer to add a title to their plot, explaining concisely what is in it, therefore making it more self containes. Others prefer to explain the plot in the caption. We’ll add a title: tmp &lt;- melt(tapply(dat$height, list(dat$position, dat$firstseason), mean)) names(tmp) &lt;- c(&quot;position&quot;, &quot;year&quot;, &quot;height&quot;) plot(tmp$year, tmp$height, col = tmp$position, xlab = &quot;year&quot;, ylab = &quot;average rookie height (cm)&quot;) legend(1997, 190, legend=c(&quot;centers&quot;, &quot;forwards&quot;, &quot;guards&quot;), col=c(&quot;black&quot;, &quot;red&quot;, &quot;green&quot;), lty = 1, cex=0.8) title(&quot;The average height of NBA rookies by playing position over time.&quot;) This is now a quite decent and self-contained plot. Next, we’ll add a bit of polish. Pleasing aesthetics might not add much to the informativeness of a plot, but they do make our work look more professional. They are also indicate that we put in the extra effort. Of course, we should never let aesthetics get in the way of efficiency and informativeness (see pie-chart example in the following section): tmp &lt;- melt(tapply(dat$height, list(dat$position, dat$firstseason), mean)) names(tmp) &lt;- c(&quot;position&quot;, &quot;year&quot;, &quot;height&quot;) levels(tmp$position) &lt;- c(&quot;centers&quot;, &quot;forwards&quot;, &quot;guards&quot;) ggplot(tmp, aes(x = year, y = height, colour = position)) + geom_point() + xlab(&quot;year&quot;) + ylab(&quot;average rookie height (cm)&quot;) + ggtitle(&quot;The average height of NBA rookies by playing position over time.&quot;) + theme_bw() By using ggplot2 we can make our visualizations look better, but it is also very convenient for adding some extra layers to our plots. For example, a smoothed line with standard errors to help us focus on the trend and not the individual data points: tmp &lt;- melt(tapply(dat$height, list(dat$position, dat$firstseason), mean)) names(tmp) &lt;- c(&quot;position&quot;, &quot;year&quot;, &quot;height&quot;) levels(tmp$position) &lt;- c(&quot;centers&quot;, &quot;forwards&quot;, &quot;guards&quot;) ggplot(tmp, aes(x = year, y = height, colour = position)) + geom_point() + geom_smooth(method = &quot;loess&quot;) + xlab(&quot;year&quot;) + ylab(&quot;average rookie height (cm)&quot;) + ggtitle(&quot;The average height of NBA rookies by playing position over time.&quot;) + theme_bw() There is one more thing that we can typically do in such cases - label the data directly and omit the legend. Always keep related objects visually close! This saves both space and user’s time, especially if we have several lines/colors in our plot: tmp &lt;- melt(tapply(dat$height, list(dat$position, dat$firstseason), mean)) names(tmp) &lt;- c(&quot;position&quot;, &quot;year&quot;, &quot;height&quot;) levels(tmp$position) &lt;- c(&quot;centers&quot;, &quot;forwards&quot;, &quot;guards&quot;) ggplot(tmp, aes(x = year, y = height, colour = position)) + geom_point() + geom_smooth(method = &quot;loess&quot;) + xlab(&quot;year&quot;) + ylab(&quot;average rookie height (cm)&quot;) + ggtitle(&quot;The average height of NBA rookies by playing position over time.&quot;) + theme_bw() + theme(legend.position = &quot;none&quot;) + annotate(&quot;text&quot;, x = 1970, y = 212, label = &quot;centers&quot;, colour = &quot;red&quot;) + annotate(&quot;text&quot;, x = 1970, y = 203, label = &quot;forwards&quot;, colour = &quot;darkgreen&quot;) + annotate(&quot;text&quot;, x = 1970, y = 192, label = &quot;guards&quot;, colour = &quot;blue&quot;) Figure 6.1: The data show that the average heights of all three groups of players have been increasing. The difference between forwards and centers is approximately the same throughout the period while the average height of guards has been increasing at a slower pace. Note that the lines are loess smoothed lines with standard error estimates. We’ve equipped the above plot with a caption that states the information that we would like to point out to the reader (the plot serves as a visual summary and argument). There is always something we can tweek and improve in a plot, depending on the situation, but if all of our plots will be at least at the level of this plot, that will be of sufficient standards. 6.1 Histograms and density plots The most elementary way of summarizing data is to plot their density. Of course, the true density is unknown and we can only estimate it by using a model or a non-parametric (smoothing) kernel density estimation. A histogram (binning the data and plotting the frequencies) can be viewed as a more coarse or discrete way of estimating the density of the data. In both density plots and histograms we need to specify the amount of smoothing (smoothing kernel width or bin size) - most built-in functions do it for us, but there is no optimal way of doing it, so we can often improve the plot by selecting a more appropriate degree of smoothing. When we have more data, we can get away with less smooting and reveal more characterisics of the underlying distribution. We illustrate these two plots by summarizing NBA player weight: ggplot(dat, aes(x = weight)) + geom_histogram(aes(y=..density..), alpha=0.5, position=&quot;identity&quot;, binwidth = 7) + geom_density(lwd = 1, col = &quot;black&quot;) + theme_bw() Figure 6.2: Histogram and density estimation of NBA player weight 6.2 Bar plot Bar plots are the most common choice for summarizing the (relative) frequencies for categorical or ordinal data with a manageable number of unique values. It is similar to a histogram, except that the categories/values provide a natural way of binning the data: set.seed(0) tmp &lt;- data.frame(University = dat$college) x &lt;- table(tmp) x &lt;- x[x &gt;= 5] x &lt;- sample(x, 10, rep = F) x &lt;- sort(x) ggplot(data.frame(x), aes(x = tmp, y = Freq)) + geom_bar(stat = &quot;identity&quot;) + coord_flip() + xlab(&quot;University&quot;) + ylab(&quot;number of players&quot;) Figure 6.3: The number of NBA players that came from these 10 Universities. When the number of unique values is large, it will not be not possible to visualize all of them (try visualizing the frequencies for all universities in the above example). In such cases we may opt to group some values or show only certain values. 6.3 Pie chart Pie charts are quite possibly the easiest chart type to work with, because there is only one rule to using pie charts - don’t use pie charts. Let’s visualize the data from the bar chart example: y &lt;- x / sum(x) ggplot(data.frame(y), aes(x = &quot;&quot;, y = Freq, fill = tmp)) + geom_bar(width = 1, stat = &quot;identity&quot;) + coord_polar(&quot;y&quot;, start=0) + theme_bw() + labs(fill = &quot;University&quot;) Figure 6.4: The relative frequencies of NBA players that came from these Universities. The pie chart is a great example (and warning!) of how aesthetics can get in the way of function and effectiveness. It is well documented that people are poor at comparing areas and especially angles. Could you recognize quickly from the above pie chart that University of Washington gave approximately twice as many players as University of Texas? How quickly would you be able to judge these relationships from the bar chart? Angles can also play tricks on our eyes. Which color pie slice is the largest on the first plot below? Which on the second plot? y &lt;- data.frame(Name = c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;), Value = rep(1, 3)) ggplot(data.frame(y), aes(x = &quot;&quot;, y = Value, fill = Name)) + geom_bar(width = 1, stat = &quot;identity&quot;) + coord_polar(&quot;y&quot;, start=0) + theme_bw() + theme(legend.position = &quot;none&quot;) ggplot(data.frame(y), aes(x = &quot;&quot;, y = Value, fill = Name)) + geom_bar(width = 1, stat = &quot;identity&quot;) + coord_polar(&quot;y&quot;, start=pi/3) + theme_bw() + theme(legend.position = &quot;none&quot;) For more information on how people perceive visual objects and relationships between them (distances, angles, areas), we recommend the pioneering work of Cleveland and McGill (see Further reading). Note that while a pie chart (the same applies to donut charts) is arguably a poor chart from a statistical graphics perspective, it is visually appealing and more engaging. That is, the pie chart still has merit as a techinque for visual presentation of information as witnessed by its widespread use. 6.4 Scatterplot The scatterplot is the most common plot for summarizing the relationship between two numerical variables. In this chapter we’ve already seen several examples of scatterplots. Here, we use three of them to summarize the relationship between player weight and player height by position. ggplot(dat, aes(x = height, y = weight)) + geom_jitter(width = 3) + theme_bw() + facet_wrap(.~position) Figure 6.5: Relationship between player weight and height by player position. Note that we introduced a bit of jitter - this is a common approach to dealing with numerical data where we have a limited number of unique values (such as rounded data)to reveal where we have more points. 6.5 2D density plot When individual points are of little interest and we just want to summarize the density of the joint distribution, a 2D density plot is a good alternative to the scatterplot. ggplot(dat, aes(x = height, y = weight, colour = position)) + geom_density_2d() + theme_bw() + theme_bw() + theme(legend.position = &quot;none&quot;) + annotate(&quot;text&quot;, y = 90, x = 212, label = &quot;centers&quot;, colour = &quot;red&quot;) + annotate(&quot;text&quot;, y = 80, x = 203, label = &quot;forwards&quot;, colour = &quot;darkgreen&quot;) + annotate(&quot;text&quot;, y = 73, x = 192, label = &quot;guards&quot;, colour = &quot;blue&quot;) Figure 6.6: Relationship between player weight and height by player position. 6.6 Boxplot A boxplot is named by the boxes that typicaly summarize the quartiles of the data. Sometimes, whiskers are added to indicate outliers. It useful for quickly visually summarizing, side-by-side, several numerical variables or to summarize the relationship between a categorical/ordinal variable and a numerical variable. Here, we use it so summarize weight by playing position. ggplot(dat, aes(x = position, y = weight)) + geom_boxplot(width = 0.1) + theme_bw() Figure 6.7: Summary of player weight by position. 6.7 Violin plot The boxplot shows only the quartiles so it can sometimes be misleading or hide information. An alternative is to plot the entire density estimates. Such a plot is called a violin plot. ggplot(dat, aes(x = position, y = weight)) + geom_violin(fill = &quot;lightblue&quot;) + geom_boxplot(width = 0.05) + theme_bw() Figure 6.8: Summary of player weight by position. Note that we combined the violin plot with the boxplot to facilitate comparison. 6.8 Correlogram When we want to quickly inspect if there are any correlations between numerical variables, we can summarize correlation coefficients in a single plot. Such a plot is also known as a correlogram. Here we do it for the iris dataset: library(ggcorrplot) ## Warning: package &#39;ggcorrplot&#39; was built under R version 3.5.3 corr &lt;- round(cor(iris[,-5]),2) ggcorrplot(corr, hc.order = TRUE, type = &quot;lower&quot;, outline.col = &quot;white&quot;, lab = T) Figure 6.9: Summary of the numerical variables in the iris dataset. 6.9 A comprehensive summary Sometimes it will be useful to summarize the density/histogram of several numerical variables and the correlation between them. Here is an example of how we can combine histograms/density plots, scatterplots, and information about correlation: library(psych) ## Warning: package &#39;psych&#39; was built under R version 3.5.3 ## ## Attaching package: &#39;psych&#39; ## The following objects are masked from &#39;package:ggplot2&#39;: ## ## %+%, alpha pairs.panels(iris[,-5]) Figure 6.10: Summary of the numerical variables in the iris dataset. 6.10 Further reading and references This book is a must-read for anyone that aspires to be a professional data analyst: Tufte, E. R. (2001). The visual display of quantitative information (Vol. 2). Cheshire, CT: Graphics press. To learn more about ggplot2, the go-to plotting library for R users, see: ggplot2: Elegant Graphics for Data Analysis (Use R!) 1st ed. 2009. Corr. 3rd printing 2010 Edition. The go-to plotting library for Python users is Matplotlib. A great starting point for understanding human perception of graphical representations: Graphical Perception: Theory, Experimentation, and Application to the Development of Graphical Methods William S. Cleveland and Robert McGill Journal of the American Statistical Association Vol. 79, No. 387 (Sep., 1984), pp. 531-554. 6.11 Learning outcomes Data science students should work towards obtaining the knowledge and the skills that enable them to: Reproduce the techniques demonstrated in this chapter using their language/tool of choice. Produce visualizations that are aesthetic and without major techical flaws. Recognize when a type of summary is appropriate and when it is not. Apply data summarization techiques to obtain insights from data. Once introduced to the bootstrap and other estimation techniques, to be able to combine descriptive statistics with a quantification of uncertainty, such as confidence intervals. 6.12 Practice problems Read Chapter 1 of Tufte’s book (see Further reading). Gather 3-5 statistical plots from popular media and 3-5 statistical plots from scientific research papers. Comment on if and how each plot could be improved. Download the Football Manager Players dataset or use a similarly rich dataset with numerical, binary and categorical variables. With Python or R demonstrate the application and interpretation of results for each of the visualization techniques from this chapter. "],
["dynamic-reports-and-reproducibility.html", "Chapter 7 Dynamic reports and reproducibility", " Chapter 7 Dynamic reports and reproducibility Now let’s talk details. "],
["dealing-with-missing-data.html", "Chapter 8 Dealing with missing data", " Chapter 8 Dealing with missing data Now let’s talk details. "],
["summarizing-data-multivariate-data.html", "Chapter 9 Summarizing data - multivariate data 9.1 Principal Component Analysis (PCA) 9.2 Factor analysis (FA) 9.3 Multi-dimensional scaling (MDS) 9.4 t-Distributed Stochastic Neighbor Embedding (t-SNE) 9.5 Clustering 9.6 Further reading and references 9.7 Learning outcomes 9.8 Practice problems", " Chapter 9 Summarizing data - multivariate data Data summarization is the science and art of conveying information more effectivelly and efficiently. Data summarization is typically numerical, visual or a combination of the two. It is a key skill in data analysis - we use it to provide insights both to others and to ourselves. Data summarization is also an integral part of exploratory data analysis. In this chapter we will focus on the some advanced techniques for multivariate data. We will be using R and ggplot2, but the contents of this chapter are meant to be tool-agnostic. Readers should use the programming language and tools that they are most comfortable with. However, do not sacrifice expresiveness or profesionallism for the sake of convenience - if your current toolbox limits you in any way, learn new tools! As we already mentioned, we humans prefer low-dimensional representations of information. This makes sense, because we’ve adapted to living in a 3-dimensional world. The natural way (and the only way) of dealing with high-dimensional data is therefore to map it to fewer dimensions. Often, a lower-dimensional representation can offer useful information and sometimes it can even be done without substantial loss of information. Dimensionality reduction is often also used as a preprocessing step before prediction or inference - fewer dimensions reduce computation times and simplify interpretation of input variables’ importance. Here, we will discuss some of the most common techniques. 9.1 Principal Component Analysis (PCA) PCA is typicaly the first method we use and often the only method, due to its simplicity, interpretability and speed. PCA is based on an orthogonal transformation of possibly correlated variables into new linearly uncorrelated variables which we call principal components. The first principal component accounts for as much of the variability as possible and each next component in turn has the highest possible variance, conditional to being orthogonal to all the previous components. The proportion of variance explained serves as an indicator of the importance of that principal component. If some principal components explain only a small part of the variability, we can discard them and therefore reduce the dimensionality of the representation. Because PCA produces orthogonal (uncorrelated, linearly independent) variables, we can use it as a preprocessing step before linear modelling. If we can interpret the principal components this will simplify the interpretation of the linear model. Note that PCA is sensitive to the relative scales of the variables. That is, scaling a variable would increase its variance and make it a priority for the principal components to include. Before applying PCA we should scale the variables according to their practical scale or, if we have no preference, standardize them so that they have equal relative importance. We demonstrate PCA on a dataset of decathlon results. We hypothesize that decathlon results might be explained by a smaller set of dimensions that correspond to the athlete’s strength, explosiveness, and stamina. We’ll use the decathlon dataset that can be found in the FactoMineR R package. First, we load the data: dat &lt;- read.csv(&quot;./data/decathlon.csv&quot;) dat &lt;- dat[,2:11] print(summary(dat)) ## X100m Long.jump Shot.put High.jump ## Min. :10.44 Min. :6.61 Min. :12.68 Min. :1.850 ## 1st Qu.:10.85 1st Qu.:7.03 1st Qu.:13.88 1st Qu.:1.920 ## Median :10.98 Median :7.30 Median :14.57 Median :1.950 ## Mean :11.00 Mean :7.26 Mean :14.48 Mean :1.977 ## 3rd Qu.:11.14 3rd Qu.:7.48 3rd Qu.:14.97 3rd Qu.:2.040 ## Max. :11.64 Max. :7.96 Max. :16.36 Max. :2.150 ## X400m X110m.hurdle Discus Pole.vault ## Min. :46.81 Min. :13.97 Min. :37.92 Min. :4.200 ## 1st Qu.:48.93 1st Qu.:14.21 1st Qu.:41.90 1st Qu.:4.500 ## Median :49.40 Median :14.48 Median :44.41 Median :4.800 ## Mean :49.62 Mean :14.61 Mean :44.33 Mean :4.762 ## 3rd Qu.:50.30 3rd Qu.:14.98 3rd Qu.:46.07 3rd Qu.:4.920 ## Max. :53.20 Max. :15.67 Max. :51.65 Max. :5.400 ## Javeline X1500m ## Min. :50.31 Min. :262.1 ## 1st Qu.:55.27 1st Qu.:271.0 ## Median :58.36 Median :278.1 ## Mean :58.32 Mean :279.0 ## 3rd Qu.:60.89 3rd Qu.:285.1 ## Max. :70.52 Max. :317.0 Next, we prepare the data by standardizing the columns - we don’t want 1500m running to be more important just because it has a larger scale! We also take the negative value of the running events - we want all the variables to be of the type “larger is better” to simplify interpretation. dat[,c(1,4,5,6,10)] &lt;- -dat[,c(1,4,5,6,10)] dat &lt;- scale(dat) Now we are ready to do PCA: res &lt;- prcomp(dat) prop_explained &lt;- res$sdev^2 / sum(res$sdev^2) data.frame(prop_explained, cumsum(prop_explained)) ## prop_explained cumsum.prop_explained. ## 1 0.32719055 0.3271906 ## 2 0.17371310 0.5009037 ## 3 0.14049167 0.6413953 ## 4 0.10568504 0.7470804 ## 5 0.06847735 0.8155577 ## 6 0.05992687 0.8754846 ## 7 0.04512353 0.9206081 ## 8 0.03968766 0.9602958 ## 9 0.02148149 0.9817773 ## 10 0.01822275 1.0000000 We can see that the first two principal components explain half of the variability in the data. And if we keep half of the principal components, we lose only about 13% of the variability. We could now argue that the latent dimensionality of this data is lower than the original 10 dimensions. Of course, in order to produce a meaningful summary of the data, we must provide an explanation of what these principal components represent: round(res$rotation,2) ## PC1 PC2 PC3 PC4 PC5 PC6 PC7 PC8 PC9 PC10 ## X100m 0.43 -0.14 0.16 -0.04 0.37 -0.30 0.38 -0.46 -0.10 0.42 ## Long.jump 0.41 -0.26 0.15 -0.10 -0.04 -0.31 -0.63 -0.02 0.48 -0.08 ## Shot.put 0.34 0.45 -0.02 -0.19 -0.13 0.31 0.31 -0.31 0.43 -0.39 ## High.jump -0.32 -0.27 0.22 -0.13 0.67 0.47 -0.09 -0.13 0.24 -0.11 ## X400m 0.38 -0.43 -0.11 0.03 -0.11 0.33 -0.12 -0.21 -0.55 -0.41 ## X110m.hurdle 0.41 -0.17 0.08 0.28 0.20 0.10 0.36 0.71 0.15 -0.09 ## Discus 0.31 0.46 0.04 0.25 0.13 0.45 -0.43 0.04 -0.15 0.45 ## Pole.vault 0.03 -0.14 0.58 -0.54 -0.40 0.26 0.10 0.18 -0.08 0.28 ## Javeline 0.15 0.24 -0.33 -0.69 0.37 -0.16 -0.11 0.30 -0.25 -0.09 ## X1500m 0.03 -0.36 -0.66 -0.16 -0.19 0.30 0.08 -0.01 0.31 0.43 As we described in the beginning, each principal component is a linear combination of the original variables. Because we standardized the variables, the corresponding coefficients serve as an indicator of importance. For example, pole vaulting and 1500m running have little importance in the first principal component, while these two disciplines have the highest weight in the fourth principal components. The meaning of principal components can more easily be discerend by plotting pairs of components, their relationship with the original variables, and individual observations. Typically, we first plot the first two principal components - components which explain most of the variance: biplot(res, cex = 0.8) This reveals two major axes in the data - (1) athletes that have high values in PC1 and PC2 are better at javeline, discus, and shotput (events that require strength) and worse at high jump (events where having a lot of muscle mass is detrimental) and (2) atletes that have high PC1 but low PC2 are better at all the shorter running events and long jump (events that depend on running speed). Pole vaulting and 1500m running results are not explained well by the first two principal components. 9.2 Factor analysis (FA) Another commonly used dimensionality reduction and exploratory data analysis technique is factor analysis. In application and interpretation it is similar to PCA, but it solves a different problem. PCA tries to explain the variability in the data with linear combinations of the variables. FA on the other hand assumes the existence of unmeasurable (latent) variables, also known as factors, and that the measured data can be explained as a linear combination of these factors. We demonstrate FA on the same dataset as PCA. In its basic form, FA requires us to specify the number of factors. We’ll assume from previous experience that there are two main factors that explain most of the variability in decathlon results: dat &lt;- read.csv(&quot;./data/decathlon.csv&quot;) dat &lt;- dat[,2:11] dat[,c(1,4,5,6,10)] &lt;- -dat[,c(1,4,5,6,10)] dat &lt;- scale(dat) res &lt;- factanal(dat, factors = 2) library(psych) ## Warning: package &#39;psych&#39; was built under R version 3.5.3 library(GPArotation) res &lt;- fa(dat, nfactors = 2, rotate = &quot;varimax&quot;) biplot(res) We can see from the plot that the results are visually similar to PCA results only rotated. Rotation is an important concept in FA. Because we are simultaneously fitting latent factors and the coefficients (loadings) of the linear combination of these factors, the solution is invariant to rotation. That is, there are infinitely many (equivalent) solutions to the FA problem. Before we interpret the results, we have to pick one rotation. For easier interpretation, we prefer rotations that keep the factors orthogonal and reduce the number of variables that each factor relates to (loads on). One such rotation is Varimax, which we used in the above example. Before we interpret the results, let’s print the loadings: print(res$loadings, cutoff = 0.4) ## ## Loadings: ## MR1 MR2 ## X100m 0.638 ## Long.jump 0.717 ## Shot.put 0.879 ## High.jump -0.515 ## X400m 0.858 ## X110m.hurdle 0.631 ## Discus 0.715 ## Pole.vault ## Javeline ## X1500m ## ## MR1 MR2 ## SS loadings 2.157 1.967 ## Proportion Var 0.216 0.197 ## Cumulative Var 0.216 0.412 First, observe that not all factors have a substantial influence (loading) on each variable, whereas in PCA each component was moderately influenced by most variables. This is the result of using a rotation that minimizes the complexity of the model in this sense. The interpretation of the two factor is as follows (this can be derived from the above plot as well as the printed loadings). The first latent factor (MR1) correlates positively with all the shorter running events and long jump. We can interpret this factor as the explosiveness of the athlete. The second latent factor (MR1) correlates positively with shot put and discus and negatively with high jump. We can interpret this factor as the strength of the athlete. Note two more things regarding the interpretation of latent factors. First, the meaning of latent factors can only be inferred through their influence on measured variables and always relies on domain specific knowledge. And second, interpreting latend factors is not an exact science - in the above example we revealed some interesting and useful structure in the data, but more than half of the variance still remains unexplained (see the loadings output above) and the two factors also affect other variables, albeit less than the ones shown in the output. 9.3 Multi-dimensional scaling (MDS) MDS is a dimensionality reduction technique that is based on the idea of representing the original data in a lower (typically 2-dimensional) space in a way that best preserves the distances between observations in the original space. The results of MDS are of course sensitive to the distance (metric) that we choose and, similar to PCA, scaling of the variables. There are many variants of MDS: classic MDS uses Euclidean distance and a generalization of that to an arbitrary metric is called metric MDS. There are also non-metric variants of MDS that allow for monotonic transformations of distance. Unlike PCA, MDS does not assume that the high-dimensional structure of the input data can be reduced in a linear fashion and is not sensitive to outliers. That is, MDS can reduce dimensionality in a more robust way than PCA and can be used to detect outliers. Note that MDS, unlike PCA, can be used on data where only the relative distances are known (imagine knowing distances betwee cities but not their geographic locations). We’ll be using non-metric MDS and we’ll first apply it to the decathlon data from the PCA example: library(MASS) d &lt;- dist(dat) # compute Euclidean distances between observations res &lt;- isoMDS(d, trace = F)$points plot(res, xlab = &quot;MDS1&quot;, ylab = &quot;MDS2&quot;, col = &quot;white&quot;) text(res, labels = 1:nrow(dat)) In order to understand MDS1 and MDS2 dimensions, we would have to look at the characteristics of the athletes on the left-hand or right-hand side (higher and lower). Fortunately, we do not have to go through the effort, because we can readily compare this result to the result of PCA and we will quickly determine that they are very similar (up to rotation). To better illustrate where PCA might fail but MDS would give reasonable results, we’ll use a 3D ball. library(scatterplot3d) library(MASS) dataset &lt;- readRDS(&quot;./data/dataset.rds&quot;)$ball dataset &lt;- dataset[sample(1:nrow(dataset),1000, rep = F),] scatterplot3d(dataset[,1:3], color = dataset[,4], pch = 16) All three dimensions are almost identical, so PCA can only produce principal components that are a rotation of the ball. That is, the projection on the first two components does not add any value over just visualizing the first two original dimensions: # PCA res &lt;- prcomp(dataset[,1:3]) rot &lt;- as.matrix(dataset[,1:3]) %*% t(res$rotation) plot(rot[,1:2], col = dataset[,4], pch = 16, xlab = &quot;PCA1&quot;, ylab = &quot;PCA2&quot;) On the other hand, MDS works with distances (topology) and groups points that are on similar layers of the ball closer together, producing a more useful 2D projection of the layers of the ball: # MDS d &lt;- dist(dataset[,1:3]) # compute Euclidean distances between observations res &lt;- isoMDS(d, trace = F)$points plot(res, xlab = &quot;MDS1&quot;, ylab = &quot;MDS2&quot;, col = dataset[,4], pch = 16) 9.4 t-Distributed Stochastic Neighbor Embedding (t-SNE) t-SNE is an advanced state-of-the-art non-linear technique for dimensionality reduction and visualization of high-dimensional data. The key idea of t-SNE is to minimize the divergence between a distribution that measures pairwise similarities of the input objects and a distribution that measures pairwise similarities of the corresponding low-dimensional points in the embedding. Note that t-SNE is mainly a data exploration and visualization technique - the input features are no longer identifiable in the embedding, so inference can not be done only with t-SNE output. The expressivenes of t-SNE makes it very useful for visualizing complex datasets that require non-linear transformations. On the other hand, as with all complex methods, there is the added computational complexity, additional parameters that need to be tuned, and fewer guarantees regarding convergence to a sensible solution. Due to its stochastic nature, t-SNE output can vary on the same data or, if we set the seed, we can get substantially different results for different seeds. We illustrate t-SNE on data known as the Swiss-roll: library(scatterplot3d) dataset &lt;- readRDS(&quot;./data/dataset.rds&quot;)$roll dataset &lt;- dataset[sample(1:nrow(dataset),1000, rep = F),] scatterplot3d(dataset[,1:3], color = dataset[,4], pch = 16) MDS and PCA preserve the x and y dimensions of the data: # MDS d &lt;- dist(dataset[,1:3]) # compute Euclidean distances between observations res &lt;- isoMDS(d, trace = F)$points plot(res, xlab = &quot;MDS1&quot;, ylab = &quot;MDS2&quot;, col = dataset[,4], pch = 16) # PCA res &lt;- prcomp(dataset[,1:3]) rot &lt;- as.matrix(dataset[,1:3]) %*% t(res$rotation) plot(rot[,1:2], col = dataset[,4], pch = 16) t-SNE on the other hand projects the data manifold into 2 dimensions, which produces an arguably more useful visualization of the characteristics of the data: library(Rtsne) ## Warning: package &#39;Rtsne&#39; was built under R version 3.5.3 set.seed(321) res &lt;- Rtsne(dataset[,1:3], perplexity = 50) plot(res$Y, col = dataset[,4], xlab = &quot;t-SNE1&quot;, ylab = &quot;t-SNE2&quot;) 9.5 Clustering Another common summarizatin technique is to group the observations into a finite number of groups or clusters. There are numerous clustering techniques, but they all have the same underlying general idea - we want to cluster the points in such a way that similar observations are in the same cluster and dissimilar observations are in different clusters. Instead of hard assignments to clusters, we can also probabilistically assign observations to clusters or assign some other type of assignment weight. This is known as soft clustering and we’ll not be discussing it in this text. 9.5.1 k-means clustering One of the most common and useful clustering methods is k-means clustering. It is based on the assumption that the data were generated by k multivariate normal distributions, each representing one cluster. An observation is assigned to the cluster that it was most likely to have been generated from - and, because we are assuming normal distributions, this reduces to assigning the observation to the mean (centroid) that is closest in terms of Euclidean distance. We’ll illustrate k-means clustering on the swiss dataset from R - data about 47 provinces of Switzerland in about 1888: summary(swiss) ## Fertility Agriculture Examination Education ## Min. :35.00 Min. : 1.20 Min. : 3.00 Min. : 1.00 ## 1st Qu.:64.70 1st Qu.:35.90 1st Qu.:12.00 1st Qu.: 6.00 ## Median :70.40 Median :54.10 Median :16.00 Median : 8.00 ## Mean :70.14 Mean :50.66 Mean :16.49 Mean :10.98 ## 3rd Qu.:78.45 3rd Qu.:67.65 3rd Qu.:22.00 3rd Qu.:12.00 ## Max. :92.50 Max. :89.70 Max. :37.00 Max. :53.00 ## Catholic Infant.Mortality ## Min. : 2.150 Min. :10.80 ## 1st Qu.: 5.195 1st Qu.:18.15 ## Median : 15.140 Median :20.00 ## Mean : 41.144 Mean :19.94 ## 3rd Qu.: 93.125 3rd Qu.:21.70 ## Max. :100.000 Max. :26.60 We’ll suppose that there are 2 clusters in the data (automatic determination of the number of clusters will be discussed later in this chapter): res &lt;- kmeans(swiss, centers = 2) print(res$cluster) ## Courtelary Delemont Franches-Mnt Moutier Neuveville ## 1 2 2 1 1 ## Porrentruy Broye Glane Gruyere Sarine ## 2 2 2 2 2 ## Veveyse Aigle Aubonne Avenches Cossonay ## 2 1 1 1 1 ## Echallens Grandson Lausanne La Vallee Lavaux ## 1 1 1 1 1 ## Morges Moudon Nyone Orbe Oron ## 1 1 1 1 1 ## Payerne Paysd&#39;enhaut Rolle Vevey Yverdon ## 1 1 1 1 1 ## Conthey Entremont Herens Martigwy Monthey ## 2 2 2 2 2 ## St Maurice Sierre Sion Boudry La Chauxdfnd ## 2 2 2 1 1 ## Le Locle Neuchatel Val de Ruz ValdeTravers V. De Geneve ## 1 1 1 1 1 ## Rive Droite Rive Gauche ## 1 1 Unless we understand the meaning of the observations (in this case, province names), cluster assignments carry little information. This is similar to latent factors - unless we have some domain specific knowledge, we will not be able to interpret the meaning of the clusters. In fact, clustering is only a special case of latent modeling. Instead of a set of numerical factors we have one factor that explains the data - the cluster assignment. To gain more insight, we can plot the clusters. This is typically done with some dimensionality reduction method that projects the data into 2 dimensions and preserves as much information as possible - the methods we’ve been discussing so far in this chapter. We’ll plot the clusters in the space determined by the first two principal components: library(factoextra) ## Warning: package &#39;factoextra&#39; was built under R version 3.5.3 ## Loading required package: ggplot2 ## ## Attaching package: &#39;ggplot2&#39; ## The following objects are masked from &#39;package:psych&#39;: ## ## %+%, alpha ## Welcome! Related Books: `Practical Guide To Cluster Analysis in R` at https://goo.gl/13EFCZ pca &lt;- prcomp(swiss, scale=TRUE) fviz_pca_biplot(pca, label=&quot;var&quot;, habillage=as.factor(res$cluster)) + labs(color=NULL) + theme(text = element_text(size = 15), panel.background = element_blank(), panel.grid.major = element_blank(), panel.grid.minor = element_blank(), axis.line = element_line(colour = &quot;black&quot;), legend.position=&quot;none&quot;) Now we can interpret the two clusters as predominately Catholic and predominately Protestant provinces. PCA also shows some other characteristics associated with the clusters - blue is associated with higher fertility and more agriculture, while red is associated with higher education. K-means clustering is simple and fast. It is, however, sensitive to outliers and, if the typical iterative algorithm is used, it might not converge to the same solution every time (solution might be sensitive to choice of initial centroids). K-means clustering also performs poorly in cases where the modelling assumption of k multivariate normal distributions does not hold. 9.5.2 Determining the number of clusters A very simple but often good enough way of determining the most appropriate number of clusters is the so-called elbow method. The idea is to find the number of clusters after which, if we furter increase the number of clusters, does no longer substantially decrease within-cluster variability. Within-cluster variability is decreasing in the number of clusters - more clusters will always lead to clusters being more similar, up to the point of assigning every point to its own cluster, which leads to 0 within-cluster variability. However, if adding a cluster decreases the within-cluster variability by a relatively small amount, that indicates that we might just be splitting already very homogeneous clusters. We demonstrate this technique by plotting the within-cluster variability for different numbers of clusters (also called a scree plot): wss &lt;- 0 maxk &lt;- 10 for (i in 1:maxk) { km &lt;- kmeans(swiss, centers = i) wss[i] &lt;- km$tot.withinss } # Plot total within sum of squares vs. number of clusters plot(1:maxk, wss, type = &quot;b&quot;, xlab = &quot;Number of Clusters&quot;, ylab = &quot;Within groups sum of squares&quot;) We see that after 2 clusters, the within-cluster variability decreases slowly. Therefore, according to this technique, k = 2 (or k = 3) is a good choice for the number of clusters. Sometimes there will be no distinct elbow in the scree plot and we will not be able to use this technique effectively. Another popular technique is to compute the silhouette index. The silhouette index measures how similar an observation is to its own cluster compared to other clusters ). It ranges from -1 to +1, where a high value indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters. If most objects have a high value, then the clustering configuration is appropriate. If many points have a low or negative value, then the clustering configuration may have too many or too few clusters. Because it takes into account within and between-cluster variability, the silhouette index, unlike within-cluster variability, does not necessarily increase with the number of clusters - at some point it typically starts decreasing. library(cluster) si &lt;- 0 maxk &lt;- 10 for (i in 2:maxk) { km &lt;- kmeans(swiss, centers = i) si[i] &lt;- mean(silhouette(km$cluster, dist(swiss))[,3]) # mean Silhouette } si ## [1] 0.0000000 0.6284002 0.5368920 0.4406436 0.4341318 0.4089358 0.4146117 ## [8] 0.3922444 0.3738533 0.3278620 In this case the Silhouette technique leads to the same conclusion - k = 2 is the best choice for the number of clusters. Alternatively, we can use clustering methods such as Affinity propagation and Mean shift clustering where the number of clusters is determined automatically. However, such methods introduce other parameters, such as kernel bandwidth. 9.5.3 Agglomerative hierarchical clustering Hierarchical clustering is an umbrella term for a host of clustering methods that build a hierarchy of clusters. Here we’ll talk about agglomerative clustering - hierarchical clustering that starts with each observation in its own cluster and procedes by joining the most similar clusters, until only one cluster remains. The steps of this procedure form a hierarchical cluster structure. Divisive hierarchical clustering approaches instead start with one cluster and procede by splitting the clusters. There are also several different criteria for determining the two most similar clusters A and B: smallest minimum distance between a point from A and a point from B (single linkage), smallest maximum distance (complete linkage), average distance, and many other, each with its own advantages and disadvantages. Here, we’ll apply hierarchical agglomerative clustering with joining clusters according to average distance. Of course, hierarchical clustering works for any distance (metric). We choose Euclidean distance. Here are the results for the swiss dataset: res_hk &lt;- hclust(dist(swiss), method = &quot;average&quot;) plot(res_hk) The above dendrogram visualizes the entire hierarchical clustering structure. We can see how a hierarchical clustering contains not just one, but a clustering for every number of clusters between 1 and the number of points. Comparing with k-means results for 2 clusters, we can see that the two methods return identical clusters: hk2 &lt;- cutree(res_hk, k = 2) res &lt;- kmeans(swiss, centers = 2) km2 &lt;- res$cluster rbind(hk2, km2) ## Courtelary Delemont Franches-Mnt Moutier Neuveville Porrentruy Broye ## hk2 1 2 2 1 1 2 2 ## km2 1 2 2 1 1 2 2 ## Glane Gruyere Sarine Veveyse Aigle Aubonne Avenches Cossonay Echallens ## hk2 2 2 2 2 1 1 1 1 1 ## km2 2 2 2 2 1 1 1 1 1 ## Grandson Lausanne La Vallee Lavaux Morges Moudon Nyone Orbe Oron ## hk2 1 1 1 1 1 1 1 1 1 ## km2 1 1 1 1 1 1 1 1 1 ## Payerne Paysd&#39;enhaut Rolle Vevey Yverdon Conthey Entremont Herens ## hk2 1 1 1 1 1 2 2 2 ## km2 1 1 1 1 1 2 2 2 ## Martigwy Monthey St Maurice Sierre Sion Boudry La Chauxdfnd Le Locle ## hk2 2 2 2 2 2 1 1 1 ## km2 2 2 2 2 2 1 1 1 ## Neuchatel Val de Ruz ValdeTravers V. De Geneve Rive Droite Rive Gauche ## hk2 1 1 1 1 1 1 ## km2 1 1 1 1 1 1 However, for 3 clusters, the results are, at first glance, completely different: hk2 &lt;- cutree(res_hk, k = 3) res &lt;- kmeans(swiss, centers = 3) km2 &lt;- res$cluster rbind(hk2, km2) ## Courtelary Delemont Franches-Mnt Moutier Neuveville Porrentruy Broye ## hk2 1 2 2 1 1 2 2 ## km2 3 1 1 3 3 1 1 ## Glane Gruyere Sarine Veveyse Aigle Aubonne Avenches Cossonay Echallens ## hk2 2 2 2 2 1 1 1 1 1 ## km2 1 1 1 1 3 3 3 3 3 ## Grandson Lausanne La Vallee Lavaux Morges Moudon Nyone Orbe Oron ## hk2 1 1 1 1 1 1 1 1 1 ## km2 3 3 3 3 3 3 3 3 3 ## Payerne Paysd&#39;enhaut Rolle Vevey Yverdon Conthey Entremont Herens ## hk2 1 1 1 1 1 2 2 2 ## km2 3 3 3 3 3 2 2 2 ## Martigwy Monthey St Maurice Sierre Sion Boudry La Chauxdfnd Le Locle ## hk2 2 2 2 2 2 1 1 1 ## km2 2 1 2 2 1 3 3 3 ## Neuchatel Val de Ruz ValdeTravers V. De Geneve Rive Droite Rive Gauche ## hk2 1 1 1 3 3 3 ## km2 3 3 3 3 3 3 When comparing different clusterings of the same data, we must keep in mind that a clustering is invariant to relabelling. That is, the cluster label or number has no meaning - a clusters meaning is determined by the observations in that cluster. This is anoter case of the invariance to rotation problem with FA that is common to all latent models. As a consequence, there exist special summaries for cluster similarity. One of the most commonly used is the Rand index. It is defined as the ratio of concordant pairs in all pairs of points. A pair of points is concordant if the points are either in the same cluster in both clusterings or in different clusters in both clusterings. Typicaly, we would use an exisitng implementation, but here we implement the basic Rand Index ourselves: concordant &lt;- 0 for (i in 2:length(hk2)) { for (j in 1:(i-1)) { if ((hk2[i] == hk2[j]) == (km2[i] == km2[j])) concordant &lt;- concordant + 1 } } rand_idx &lt;- concordant / choose(length(hk2), 2) round(rand_idx, 2) ## [1] 0.87 By definition, the Rand index is between 0 and 1. 9.6 Further reading and references For more information on multivariate methods, we recommend Hair, J. F., Black, W. C., Babin, B. J., &amp; Anderson, R. E. (2009). Multivariate Data Analysis 7th Edition Pearson Prentice Hall. The following chapters are particularly relevant to what we are trying to learn in this course: Chapter 1: Overview of multivariate methods. Chapter 2. Examining your data. And these two chapters provide more details on the multivariate techniques that we discussed: Chapter 3. Exploratory factor analysis. Chapter 8: Cluster Analysis Chapter 9: Multidimensional Scaling and Correspondence Analysis 9.7 Learning outcomes Data science students should work towards obtaining the knowledge and the skills that enable them to: Reproduce the techniques demonstrated in this chapter using their language/tool of choice. Recognize when a technique is appropriate and when it is not. Apply data summarization techiques to obtain insights from data. 9.8 Practice problems Download the Football Manager Players dataset or use a similarly rich dataset with numerical, binary and categorical variables. With Python or R demonstrate the application and interpretation of results for each of the multivariate summarization techniques from this chapter. "],
["relational-databases.html", "Chapter 10 Relational databases", " Chapter 10 Relational databases Now let’s talk details. "],
["predictive-modelling-introduction.html", "Chapter 11 Predictive modelling - Introduction", " Chapter 11 Predictive modelling - Introduction Tools: Python + scikit? Typical predictive tasks (classification, regression) A couple of simple learning paradigms (linear regression, kNN?) Empirical error (accuracy, MSE, etc.) Estimating generalization error (train/test, CV) "],
["predictive-modelling-learning-paradigms.html", "Chapter 12 Predictive modelling - Learning paradigms", " Chapter 12 Predictive modelling - Learning paradigms Things to consider when choosing a learning paradigm: empirical error, complexity, interpretability, computational intensity. A few typical paradigms (linear models, kNN, decision trees, SVM?, ANN?), how to use them and their advantages disadvantages. Purely descriptive, no math. "],
["predictive-modelling-feature-selection.html", "Chapter 13 Predictive modelling - Feature selection", " Chapter 13 Predictive modelling - Feature selection Key: sometimes we need to reduce number of features to even run the model; often we can run the model, but we still want to reduce the number of features to speed it up and improve interpretability of the model. Main approaches: - Use a method that scales (linear models), - Dimensionality reduction via removing features or transforming feature space (already partially covered in previous summarization chapters): * before the model is applied (filtering) * together with the model (wrapper-based) "],
["large-datasets-and-non-relational-databases.html", "Chapter 14 Large datasets and non relational databases", " Chapter 14 Large datasets and non relational databases Now let’s talk details. "],
["parallel-and-distributed-computing.html", "Chapter 15 Parallel and distributed computing", " Chapter 15 Parallel and distributed computing Now let’s talk details. "],
["other-important-topics.html", "Other important topics", " Other important topics We need visiting lecturers to cover these topis: Effective/efficient communication (visual or otherwise) Specifics of data science project management privacy, ethics, security IPR "],
["references.html", "References", " References "]
]
