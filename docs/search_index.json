[
["index.html", "Introduction to data science Preface", " Introduction to data science Tomaž Curk, Erik Štrumbelj 2019-07-05 Preface These are the course notes for the Introduction to data science course of the Data Science Master’s at University of Ljubljana, Faculty of computer and information science. "],
["python-programming-language.html", "Chapter 1 Python programming language 1.1 Basic characteristics 1.2 Why Python? 1.3 Setting up the environment 1.4 Installing dependencies 1.5 Jupyter notebooks 1.6 Short introduction to Python 1.7 Python ecosystem for Data Science 1.8 Further reading and references 1.9 Learning outcomes 1.10 Practice problems", " Chapter 1 Python programming language 1.1 Basic characteristics Python is an interpreted, dynamically typed, object-oriented programming language. Advantages: Simple. Easy to learn. Extensive packages. Cross-platform. Free and open source. Large user base. Disadvantages: Being interpreted results in slower execution. Dynamic typing can lead to errors that only show up at runtime. Higher memory consumption, less suitable for memory-intensive tasks. 1.2 Why Python? The Python language is one of the two most popular languages for data science (the other being R). The two main reasons are: The advantages of Python fit the typical data science workflow and its disadvantages are not that deterimental to the data science workflow. Python has a large ecosystem of packages, libraries and tools for data science, some of which are discussed later in this chapter. Often libraries and software developed in other languages provide Python API or bindings. The typical data science workflow consists of acquiring and manipulating data and applying standard machine learning or statistical methods. In essence, the data flows through different methods. The emphasis is on obtaining results - extracting meaningful information from data. The advantages of Python are extremely beneficial to such a workflow: Being simple, easy to learn and free, it is accessible to a larger user base, including users with little or no prior programming experience. Being an interpreted language (and straightforward piecewise execution through read-eval-print loop shells or REPL) makes Python very flexible - multiple alternatives can be explored and quick decisions made on how to procede, depending on intermediary results. The disadvantages of Python are of minor consequence: The data science workflow is not time-critical - even an order-of-magnitude slowdown typically makes little difference. Code maintainability is less important - data science scripts are often short and discarded after use. Specific platforms development or enterprise-level applications development are also not part of the typical data science workflow. Data science products used in such applications are normally rewritten as final models in a production-ready code. 1.3 Setting up the environment Before using Python, we need to select and install a desired distribution. One can choose to install a pure Python distribution or an Anaconda Python distribution. Some advantages of using Anaconda distribution are: Anaconda gives the User ability to make an easy install of the version of python he/she wants. Anaconda will also resolve all the problems with admin privileges if a user does not have admin rights for his system. Anaconda Accelerate can provide a user with high performance computing and several other components. Anaconda removes bottlenecks involved in installing the right packages while taking into considerations their compatibility with various other packages as might be encountered while using the traditional package manager (pip). There is no risk of messing up required system libraries. There are also many open source packages available for Anaconda, which are not within the pip repo. We encourage you to use the Anaconda Python distributions but the final choice is yours. 1.3.1 Anaconda distribution installation Install the desired Anaconda Python distribution. Useful way of managing multiple Python project is by using Conda environments. An environment enables you to use a specific version of Python along with specific dependencies completely separately on one system. To create and use an environment with a name itds, issue the following command: $ conda create -n itds $ conda activate itds At the beginning of a line in the console you can see currently activated environment. To run Python within the current evironment, issue python command in the console. After running the command, you should see something similar to the following: (itds)$ python Python 3.6.8 |Anaconda, Inc.| (default, Dec 29 2018, 19:04:46) [GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)] on darwin Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information. &gt;&gt;&gt; To exit Python interpreter, enter and commit the command exit(); To exit the environment, use conda deactivate. To show existing environments and their locations, issue conda info --envs. 1.3.2 Pure Python distribution installation You can also install a pure Python distribution directly to your system from . To run Python, issue python command in the console (there may be more interpreters installed on your machine and Python 3.5 might be run also using python3.5). After running the command, you should see something similar to the following: $ python Python 3.5.2 (v3.5.2:4def2a2901a5, Jun 26 2016, 10:47:25) [GCC 4.2.1 (Apple Inc. build 5666) (dot 3)] on darwin Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information. &gt;&gt;&gt; When using the distribution directly, all packages and settings are changed system-wide, which may lead into problems when maintaining multiple Python projects (e.g. problems in having installed different versions of a specific library). Similarly to Anaconda distribution, one can use virtualenv virtual environments. First, we need to install virtualenv via pip: $ pip3 install virtualenv $ virtualenv --version 16.6.1 To set up a virtual environment for pa project, first create a project folder and set up a new environment in that folder. The latter will create Python executables within that folder and a copy of pip library that is used to install libraries local to the environment (parameter p is optional). $ cd itds_project $ virtualenv -p /usr/local/bin/python2 itds To activate the environment, run the script venv/bin/activate from the project folder and use project specific Python: $ source itds/bin/activate (itds)$ python Python 2.7.14 (default, Mar 9 2018, 23:57:12) [GCC 4.2.1 Compatible Apple LLVM 9.0.0 (clang-900.0.39.2)] on darwin Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information. &gt;&gt;&gt; After you finish work with your project, deactivate current virtual environment: (itds)$ deactivate $ 1.4 Installing dependencies Lots of useful libraries are available online, mostly in the Python Package Index (PyPI) repository. Otherwise, well build libraries consist of installation instructions and setup.py file to install them. Common path to install libraries is into the folder %PYTHON_BASE%/lib, %PYTHON_BASE%/site-packages or %PYTHON_BASE%/Lib. Packages can be installed using pip command. For example, to install the NLTK library, one would issue the following command: $ pip install nltk It can happen that in some cases, packages will be prebuilt (for specific OS) because istallation of its dependencies can be tedious. In such cases, wheel packages can be provided and installed using the following command: $ pip install YOUR_DOWNLOADED_PACKAGE.whl Anaconda distribution provides its own package repository and in addition to pip, Anaconda packages can be installed as follows: $ conda install nltk If a package is not available in the official repository, it may be available from some private or community-led channels, for example conda-forge: $ conda install -c conda-forge pyspark 1.5 Jupyter notebooks The Jupyter Notebook is an open-source web application that allows you to create and share documents that contain live code, equations, visualizations and narrative text. Uses include: data cleaning and transformation, numerical simulation, statistical modeling, data visualization, machine learning, and much more. To add support for Anaconda environments to Jupyter, issue the command below. This will add Conda extensions to Jupyter (the feature will be installed for currently active environment, so you can install it also to the base environment and run Jupyter always from it). When running Jupyter, you will notice Conda tab in the file browser, which will enable listing and selection of existing Anaconda environments, overview of the installed packages in the environment, installing new packages from the available package list, checking for updates on selected (or all) packages and updating selected (or all) packages in the environment. $ conda install nb_conda 1.5.1 Running a Jupyter notebook Prior to running a Jupyter notebook we need first to start a Jupyter web server by issuing a command: $ jupyter notebook By default the server is started in a current folder and accessible via Web interface: http://localhost:8888. The root web page shows a file browser, where we can create a new Jupyter notebook or open an existing one: To get similar view, save prepared a Jupyter notebook for this section into the folder where you ran jupyter notebook command. Click the filename of the notebook and it will open in a new window: As you notice, a notebook consists of linearly ordered block of types Markdown, Code, Raw NBConvert and Heading. If you double click a text block you can edit it using Markdown syntax. To evaluate block again and show rendered view, click Run button or press Alt+Enter. Same also holds for other types of blocks. Sometimes you would like just to view or execute a notebook online. There exist multiple services offering such functionality, as for example https://gke.mybinder.org/. 1.6 Short introduction to Python All the examples presented in this section are also available in an accompanying Jupyter notebook. Basics Let’s first say hello: print(&quot;Hello Data science!&quot;) ## Hello Data science! 1.6.0.1 Variables and types Python defines whole numbers (int, long) and real numbers (float). Whole numbers are integers (\\(\\pm 2^{31}\\) or \\(\\pm 2^{63}\\)) and long numbers, limited by the memory size. Long is a number with a trailing L added at the end (Python 2, in Python 3 is merged with int). Complex numbers are also supported using a trailing j to the imaginary part. Bool type is based on integer - value of 1 as True or anything else as False. String values are represented as sequence of characters within &quot; or '. A constant None is defined to represent nonexistence of a value. a = 2864 print(&quot;Type of a is {}&quot;.format(type(a))) ## Type of a is &lt;type &#39;int&#39;&gt; b = 2864L print(&quot;Type of b is {}&quot;.format(type(b))) ## Type of b is &lt;type &#39;long&#39;&gt; c = 18+64j print(&quot;Type of c is {}&quot;.format(type(c))) ## Type of c is &lt;type &#39;complex&#39;&gt; d = False print(&quot;Type of d is {}&quot;.format(type(d))) ## Type of d is &lt;type &#39;bool&#39;&gt; e = &quot;I&#39;m loving it!&quot; print(&quot;Type of e is {}&quot;.format(type(e))) ## Type of e is &lt;type &#39;str&#39;&gt; f = None print(&quot;Type of f is {}&quot;.format(type(f))) ## Type of f is &lt;type &#39;NoneType&#39;&gt; Numbers and basic operators Basic data manipulations: a = 3 b = 2.5 c = a + b print(&quot;Addition: {}&quot;.format(c)) ## Addition: 5.5 c = a * b print(&quot;Multiplication: {}&quot;.format(c)) ## Multiplication: 7.5 c = a / b print(&quot;Division: {}&quot;.format(c)) ## Division: 1.2 c = True + 5 print(&quot;Addition to Boolean: {}&quot;.format(c)) ## Addition to Boolean: 6 c = &quot;5&quot; * 5 print(&quot;String multiplication: {}&quot;.format(c)) ## String multiplication: 55555 1.6.0.2 Strings, concatenation and formatting Basic strings manipulations: a = &quot;Data science&quot; b = &#39;a multi-disciplinary field&#39; # we can use double or single quotes c = a + &quot; &quot; + b print(&quot;Concatenated string: {}&quot;.format(c)) ## Concatenated string: Data science a multi-disciplinary field first = c[:4] last = c[-5:] print(&quot;First word: &#39;{}&#39; and last word: &#39;{}&#39;.&quot;.format(first, last)) ## First word: &#39;Data&#39; and last word: &#39;field&#39;. firstLower = first.lower() lastUpper = last.upper() print(&quot;First word lowercased: &#39;{}&#39; and last word uppercased: &#39;{}&#39;.&quot;.\\ format(firstLower, lastUpper)) ## First word lowercased: &#39;data&#39; and last word uppercased: &#39;FIELD&#39;. management = c.replace(&quot;science&quot;, &quot;management&quot;) print(&quot;Substring replacement: &#39;{}&#39;&quot;.format(management)) ## Substring replacement: &#39;Data management a multi-disciplinary field&#39; Explore more about strings in the official Python 3 documentation for strings. # string package import string print(&quot;Punctuation symbols: &#39;{}&#39;&quot;.format(string.punctuation)) ## Punctuation symbols: &#39;!&quot;#$%&amp;&#39;()*+,-./:;&lt;=&gt;?@[\\]^_`{|}~&#39; It is useful to format strings to provide machine readable outputs when needed. For more sophisticated examples, see https://pyformat.info/. number = 6/.7 text = &quot;dyslexia&quot; format0 = &quot;Number: &quot; + str(round(number*100)/100.0) + &quot;, Text: &quot; + \\ &quot; &quot;*(15-len(text)) + text print(format0) ## Number: 8.57, Text: dyslexia format1 = &quot;Number: {:5.2f}, Text: {:&gt;15}&quot;.format(number, text) print(format1) ## Number: 8.57, Text: dyslexia format2 = &quot;Number: %5.2f, Text: %15s&quot; % (number, text) print(format2) ## Number: 8.57, Text: dyslexia 1.6.0.3 Data stuctures: Lists, Tuples, Sets, Dictionaries Below we create some of the data structures available in Python. Explore more of their functions in the official Python documentation. l = [1, 2, 3, &quot;a&quot;, 10] # List t = (1, 2, 3, &quot;a&quot;, 10) # Tuple (immutable) s = {&quot;a&quot;, &quot;b&quot;, &quot;c&quot;} # Set dict = { &quot;title&quot;: &quot;Introduction to Data Science&quot;, &quot;year&quot;: 1, &quot;semester&quot;: &quot;fall&quot;, &quot;classroom&quot;: &quot;P02&quot; } dict[&quot;classroom&quot;] = &quot;P03&quot; You will often use inline functions to map, filter or calculate values on a given iterable. For example, apply a function to all values (map), filter out not needed values or use all values in calculation: #from functools import reduce # Python 3 import for reduce (not needed for Python 2) l = [6, 8, 22, 4, 12] doubled = map(lambda x: x*2, l) print(&quot;Doubled: {}&quot;.format(doubled)) ## Doubled: [12, 16, 44, 8, 24] filtered = filter(lambda x: x &gt; 10, l) print(&quot;Filtered: {}&quot;.format(filtered)) ## Filtered: [22, 12] sum = reduce(lambda x, y: x+y, l) print(&quot;Sum value: {}&quot;.format(sum)) ## Sum value: 52 l = [6, 8, 22, 4, 12] newList = [x**2 for x in l if x &gt;= 5 and x &lt;= 10] print(&quot;Squared values between 5 and 10: {}&quot;.format(newList)) ## Squared values between 5 and 10: [36, 64] 1.6.1 Control flow operations Many operations can be written inline or using multiple lines. Let’s check how to use if statements and loops. a = 2 if a &gt; 1: print(&#39;a is greater than 1&#39;) elif a == 1: print(&#39;a is equal to 1&#39;) else: print(&#39;a is less than 1&#39;) ## a is greater than 1 # Inline if statement a = 2 print(&#39;a is greater than 1&#39; if a &gt; 1 else &#39;a is lower or equal to 2&#39;) ## a is greater than 1 Loops: for i in range(4, 6): print(i) ## 4 ## 5 people_list = [&#39;Ann&#39;, &#39;Bob&#39;, &#39;Charles&#39;] for person in people_list: print(person) ## Ann ## Bob ## Charles i = 1 while i &lt;= 3: print(i) i = i + 1 ## 1 ## 2 ## 3 1.6.2 Functions We organize our code into logical units and if possible, such units should be generic and reused which results in less boilerplate code. Below we start with a function named greetMe that takes one parameter (name) as input and prints some string. After we declare function, we need to call it and at that time, the code will be executed. def greetMe(name): print(&quot;Hello my friend {}!&quot;.format(name)) greetMe(&quot;Janez&quot;) ## Hello my friend Janez! Sometimes our functions will have many parameters, out of which some will often be optional or have a default value. In the example below we add a parameter with a default value. If there are multiple optional parameters we can set only specific ones by naming it. def greet(name, title = &quot;Mr.&quot;): print(&quot;Hello {} {}!&quot;.format(title, name)) greet(&quot;Janez&quot;) ## Hello Mr. Janez! greet(&quot;Mojca&quot;, &quot;Mrs.&quot;) ## Hello Mrs. Mojca! greet(&quot;Mojca&quot;, title = &quot;Mrs.&quot;) ## Hello Mrs. Mojca! A function can also call itself and return a value. def sumUpTo(value): if value &gt; 0: return value + sumUpTo(value-1) else: return 0 print(&quot;Sum of all positive integers up to 50 is: {}&quot;.format(sumUpTo(50))) ## Sum of all positive integers up to 50 is: 1275 Python encapsulates variables within functions, so therefore they are not accessible outside the function. Still we can use global keyword for variables to be accessible everywhere (use with caution!) def playWithVariables(value1, list1): global globVal globVal = 3 value1 = 10 list1.append(22) print(&quot;Within function: {} and {} and {}&quot;.format(value1, list1, globVal)) value1 = 5 list1 = [3, 6, 9] print(&quot;Before function: {} and {}&quot;.format(value1, list1)) playWithVariables(value1, list1) print(&quot;After function: {} and {} and {}&quot;.format(value1, list1, globVal)) ## Before function: 5 and [3, 6, 9] ## Within function: 10 and [3, 6, 9, 22] and 3 ## After function: 5 and [3, 6, 9, 22] and 3 In some cases we can also define functions that accept undefined number of parameters. Some of them can also be named (kwargs). def paramsWriter(*args, **kwargs): print(&quot;Non-named arguments: {}\\nNamed arguments: {}&quot;.format(args, kwargs)) paramsWriter(1, &quot;a&quot;, [1,5,6], studentIds = [234, 451, 842], maxScore = 100.0) ## Non-named arguments: (1, &#39;a&#39;, [1, 5, 6]) ## Named arguments: {&#39;maxScore&#39;: 100.0, &#39;studentIds&#39;: [234, 451, 842]} When naming functions, classes, objects, packages, … we need to be careful not to overwrite existing objects. The snippet below may not seem important but such bugs can be very tedious to discover. def greeter(): print(&quot;Hello to everyone!&quot;) greeter() greeter = &quot;Mr. John Hopkins&quot; greeter() # Error - greeter is now string value Hello to everyone! Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt; TypeError: &#39;str&#39; object is not callable 1.6.3 Classes and objects Python is also object-oriented and therefore enables us to encapsulate data and functionality into classes. Instances of classes are objects. A class below consists of one class variable, one class method, three object methods and two object variables. All class-based variables are accessible using class name directly without having instantiated object. Object-based methods are accessible only through an instantiated object and can also directly modify object properties (self.*). All object methods accept self as an implicit parameter, which points to the current object. Below we first show an example of object declaration and then its usage. More detailed explanation can be found in the official Python documentation. class Classroom: classCounter = 0 def numClasses(): return Classroom.classCounter def __init__(self, name): Classroom.classCounter += 1 self.name = &quot;Best of Data Science class &quot; + name self.students = [] def enroll(self, student): self.students.append(student) def __str__(self): return &quot;Class: &#39;{}&#39;, students: &#39;{}&#39;&quot;.format(self.name, &quot;, &quot;.join(self.students)) class1 = Classroom(&quot;best of millenials&quot;) class2 = Classroom(&quot;old sports&quot;) print(&quot;Num classes: {}&quot;.format(Classroom.classCounter)) print(&quot;Num classes: {}&quot;.format(Classroom.numClasses())) class2.enroll(&quot;Slavko Žitnik&quot;) class2.enroll(&quot;Erik Štrumbelj&quot;) class2.enroll(&quot;Tomaž Curk&quot;) print(class2) ## Num classes: 2 ## Num classes: 2 ## Class: &#39;Best of Data Science class old sports&#39;, students: &#39;Slavko Žitnik, Erik Štrumbelj, Tomaž Curk&#39; 1.6.4 Python IDE’s and code editors An IDE (Integrated Development Environment) is a program dedicated to software development. As the name implies, IDEs integrate several tools specifically designed for software development. These tools usually include: An editor designed to handle code with features such as syntax highlighting and auto-completion. Build, execution, and debugging tools. Some form of source control support. IDEs are generally large and take time to download and install. You may also need advanced knowledge to use them properly. In contrast, a dedicated code editor can be as simple as a text editor with syntax highlighting and code formatting capabilities. Most good code editors can execute code and control a debugger. The very best ones interact with source control systems as well. Compared to an IDE, a good dedicated code editor is usually smaller and quicker, but often less feature rich. Below we list some popular Python IDEs/code editors that are available for major operating systems (Windows, Linux and Mac OS): IDLE - a default code editor, installed together with Python distribution. It includes Python shell window (interactive interpreter), auto-completion, syntax highlighting, smart indentation and a basic integrated debugger. We do not recommend it for larger projects. Sublime Text, Atom, Visual Studio Code - highly customizable code editors with rich features of an IDE. They support installation of additional extensions and also provide intelligent code completion, linting for potential errors, debugging, unit testing and so on. These editors are becoming quite popular among Python (and also Web) developers. PyCharm - an IDE for professional developers. There are two available versions: Community (free) and Professional (paid version, free for students). PyCharm provides all major features that a good IDE should provide: code completion, code inspections, error-highlighting and fixes, debugging, version control system and code refactoring, etc.. 1.7 Python ecosystem for Data Science The Python ecosystem of libraries, frameworks, and tools is large and ever-growing. Python can be used for web scraping, machine learning, general scientific computing, and many other computing and scripting uses. We list just some of widely used libraries in the field of data science. You will discover more in the course of existing work exploration and your own work. NumPy - NumPy is the fundamental package for scientific computing with Python. Among other thing, the common tools that you will use, are: a powerful N-dimensional array object, sophisticated (broadcasting) functions, tools for integrating C/C++ and Fortran code and most importantly, useful linear algebra, Fourier transform, and random number capabilities. NumPy can also be used as an efficient multi-dimensional container of generic data, where arbitrary data-types can be defined. Matplotlib - A 2D plotting library which produces publication quality figures in a variety of hardcopy formats and interactive environments across platforms. Matplotlib can be used in Python scripts, the Python and IPython shells, the Jupyter notebooks or web application servers. You can generate plots, histograms, power spectra, bar charts, errorcharts, scatterplots, etc., with just a few lines of code. It provides a MATLAB-like interface, particularly when combined with IPython. For the power user, you have full control of line styles, font properties, axes properties, etc. SciPy - “Sigh Pie”&quot; is a Python-based ecosystem of open-source software for mathematics, science, and engineering. In particular, it connects together some of the following core packages: NumPy, SciPy library (fundamentals for scientific computing), Matplotlib, IPython, Sympy (symbolic mathematics) and Pandas. scikit-learn - Machine learning library in Python. It provides simple and efficient tools for data mining and data analysis. It offers many algorithms and framework for classification, regression, clustering, dimensionality reduction, model selection and preprocessing. The library is open source and build on NumPy, SciPy and matplotlib. Pandas - Python Data Analysis Library (pandas) is an open source, BSD-licensed library providing high-performance, easy-to-use data structures and data analysis tools for the Python programming language. Some of the library highlights are a fast and efficient DataFrame object for data manipulation with integrated indexing, tools for reading and writing data between in-memory data structures and different formats, intelligent data alignment and integrated handling of missing data, flexible reshaping and pivoting of data sets, high performance merging and joining of data sets, an intuitive way of working with high-dimensional data in a lower-dimensional data structure, time series-functionality. TensorFlow - TensorFlow is an end-to-end open source platform for machine learning in the field of deep learning. It has a comprehensive, flexible ecosystem of tools, libraries and community resources that lets researchers push the state-of-the-art in ML and developers easily build and deploy ML powered applications. Keras - In comparison to TensorFlow, Keras is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, CNTK, or Theano backends. It was developed with a focus on enabling fast experimentation, being able to go from idea to result with the least possible delay is key to doing good research. Keras allows for easy and fast prototyping (through user friendliness, modularity, and extensibility), supports both convolutional networks and recurrent networks, as well as combinations of the two and runs seamlessly on CPU and GPU. 1.8 Further reading and references Apart from the references above, you should also check more comprehensive guides into Python programming. We list only a few of them: Official Python Tutorials - Tutorial accompanying official Python documentation. See this resource for latest features. Beginning Python by Magnus Hetland - The book is written for beginners but last chapters are useful also for more experienced programmers. Non-Programmer’s Tutorial for Python - a well organized Wikibook. Think Python - similar book to the previous one, but a bit more intermediate. [Wikibook: Programming Python] - similar to the above two with more advanced topics. How to Think Like a Computer Scientist - well organized sections for self-paced learning. Python za programerje by Janez Demšar - Well known Slovene book, written by the professor at our Faculty. You can find electronic versions online, but printed version is accessible to buy at publishing house FRI (at the entrance). 1.9 Learning outcomes Data science students should work towards obtaining the knowledge and the skills that enable them to: Use the Python programming language for common programming tasks, data manipulation, file I/0, etc. Identify the Python IDE(s) that best fit their requirements. Find suitable Python packages for the task at hand and use them. Recognize when Python is and when it is not a suitable language to use. 1.10 Practice problems Install Anaconda Python, run the provided Jupyter notebook within a new conda environment and then export all the installed dependencies into an environment.yml file (see reference). Check the file, remove not needed data (location, library versions, libraries in lower dependency trees), create a new environment based on the exported file and run the notebook again (it should work without the need to install additiona packages manually). Check some Python IDEs to have a subjective opinion for some of them. Download, explore and run some scripts from the Keras examples repository. "],
["source-code-control.html", "Chapter 2 Source code control", " Chapter 2 Source code control Now let’s talk details. "],
["docker-container-platform.html", "Chapter 3 Docker container platform 3.1 Basic characteristics 3.2 Why Docker? 3.3 Setting up 3.4 Docker in 15 minutes 3.5 Further reading and references 3.6 Learning outcomes 3.7 Practice problems", " Chapter 3 Docker container platform 3.1 Basic characteristics TODO 3.2 Why Docker? TODO 3.3 Setting up TODO 3.4 Docker in 15 minutes Hello world! Images and containers Volumes Dockerfiles Docker compose Docker Hub 3.5 Further reading and references Docker tutorial: https://docs.docker.com/get-started/ Kubernetes, Vagrant, … 3.6 Learning outcomes Data science students should work towards obtaining the knowledge and the skills that enable them to: Use an existing Docker image. Package a project into a Docker image and publish it on Docker Hub. 3.7 Practice problems Create a simple Python program, write a Dockerfile to compile an image an publish it on Docker Hub. "],
["web-scraping.html", "Chapter 4 Web scraping", " Chapter 4 Web scraping Now let’s talk details. TODO: dodati branje podatkov iz JSON-a, XML-ja "],
["basic-data-summarization.html", "Chapter 5 Basic data summarization 5.1 Descriptive statistics for univariate distributions 5.2 Descriptive statistics for bivariate distributions 5.3 Visualization 5.4 Further reading and references 5.5 Learning outcomes 5.6 Practice problems", " Chapter 5 Basic data summarization Data summarization, numerical or visual, is a key skill in data analysis as we use it to provide insights both to others and to ourselves. Data summarization is also an important component of exploratory data analysis. In this chapter we will focus on the basic techniques for univariate and bivariate data. Other, more advanced data summarization techniques will be covered in a later chapter. We will be using R and ggplot2, but the contents of this chapter are meant to be tool-agnostic. Readers should use the programming language and tools that they are most comfortable with. However, do not sacrifice expresiveness or profesionallism for the sake of convenience - if your current toolbox limits you in any way, learn new tools! 5.1 Descriptive statistics for univariate distributions We humans are on average not particularly good at thinking in multiple dimensions, so, in practice, there will be a tendency to look at individual variables/dimensions. That is, in practice, we will most of the time be summarizing univariate distributions. Univariate distributions come from various sources. It might be a theoretical distribution, an empirical cumulative distribution function of a data sample, a probabilistic opinion from a person, a posterior distribution of a parameter from a Bayesian model, and many others. Descriptive statistics apply to all of these cases in the same way, regardless of the source of the distribution. Before we proceed with introducing the most commonly used descriptive statistics, let us discuss their main purpose. The main purpose of any sort of data summarization technique is to (a) reduce the time/effort of delivering information to the reader in a way that (b) we lose as little relevant information as possible. That is, to compress the information. For example, in the extreme case we could just show the reader the entire distribution/density ever time, but that would consume a lot of space and a lot of the readers time and effort. All summarization methods do (a) but we must be careful to choose an appropriate method so that we also get (b). Summarizing out relevant information can lead to misleading summaries, as we will illustrate with several examples later in the chapter. 5.1.1 Central tendency The most common ways of measuring central tendency (or location) of a distribution are: the mean (the mass centre of the distribution), the median or 2nd quartile (the value such that half of the mass is on one and half on the other side), the mode (the most probable value or the value with the highest density). Given a sample of data, the mean is the easiest to estimate/compute (we compute the average), but the median and mode are more robust to outliers. In the case of unimodal approximately symmetrical distributions, such as the univariate normal distribution, all these measures of central tendency will be similar and all will be an excellent summary of location. If the distribution is asymmetrical or skewed, they will differ. In such cases it is our job to determine what information we want to convey and which summary of central tendency is the most appropriate, if any. For example, observe the Gamma(1.5, 0.1) distribution and its mean (red), median (blue) and mode (green): x &lt;- seq(0, 50, 0.01) a &lt;- 1.5 b &lt;- 0.1 y &lt;- dgamma(x, a, b) library(ggplot2) ggplot(data.frame(x,y), aes(x = x, y = y)) + geom_line() + ylab(&quot;p(x)&quot;) + geom_vline(xintercept = (a-1) / b, colour = &quot;green&quot;, lty = &quot;dashed&quot;, lwd = 1.5) + geom_vline(xintercept = a / b, colour = &quot;red&quot;, lty = &quot;dashed&quot;, lwd = 1.5) + geom_vline(xintercept = qgamma(0.5, a, b), colour = &quot;blue&quot;, lty = &quot;dashed&quot;, lwd = 1.5) In the case of multi-modal distributions, no single measure of central tendency will adequately summarize the distribution - they will all be misleading. For example, look at this bimodal distribution: x &lt;- seq(-10, 20, 0.01) y &lt;- 0.6 * dnorm(x, 2, 1) + 0.4 * dnorm(x, 12, 2) library(ggplot2) ggplot(data.frame(x,y), aes(x = x, y = y)) + geom_line() + ylab(&quot;p(x)&quot;) + geom_vline(xintercept = 0.6* 2 + 0.4 * 15, colour = &quot;red&quot;, lty = &quot;dashed&quot;, lwd = 1.5) + geom_vline(xintercept = 2, colour = &quot;green&quot;, lty = &quot;dashed&quot;, lwd = 1.5) 5.1.2 Dispersion The most common ways of measuring dispersion (or spread or scale) of a distribution are: variance (mean of quadratic distances from mean) or, more commonly, standard deviation (root of variance, so we are on the same scale as the measurement) median absolute deviation (median of absolute distances from mean), quantile-based intervals, in particular the inter-quartile range (IQR) (interval between the 1st and 3rd quartiles, 50% of the mass/density lies in this interval). As the measures of central tendency summarize the centre of the distribution, measures of dispersion summarize how far spread out is the distribution around its centre. Standard deviation is the most commonly used and median absolute deviation is more robust to outliers. Again, in the case of distributions that are approximately normal, the standard deviation and the mean will be the practically optimal choice for summarization, because they correspond directly to the two parameters of the normal distribution. That is, they completely summarize the distribution without loss of information. We also know that approximately 95% (99%) of the normal density lies within 2 (3) standard deviations from the mean. Standard deviation is useful even if the distribution is not approximately normal as it does provide some information, combined with the sample size (producing the standard error), on how certain we can be in our estimate of the mean. But, as before, the more we deviate from normality, the less meaningful standard deviation becomes and it makes more sense to use quantile-based intervals. For example, if we estimate the mean and \\(\\pm\\) 2 standard deviations for samples from the Gamma distribution from before, we get the following: set.seed(0) x &lt;- rgamma(1000, a, b) cat(sprintf(&quot;%.2f +/- %.2f\\n&quot;, mean(x), 2*sd(x))) ## 14.66 +/- 24.49 That is, the 95% interval estimated this way also includes negative values, which is absurd and misleading - we know the Gamma distributed variables are positive. Computing the IQR or the 95% range interval provides a more sensible summary of this skewed distribution and, together with the mean also serve as an indicator that the distribution is skewed (the mean is not the centre of the intervals): set.seed(0) x &lt;- rgamma(1000, a, b) cat(sprintf(&quot;%.2f, IQR = [%.2f, %.2f], 95pct = [%.2f, %.2f]\\n&quot;, mean(x), quantile(x, 0.25), quantile(x, 0.75), quantile(x, 0.025), quantile(x, 0.975))) ## 14.66, IQR = [5.72, 20.76], 95pct = [1.18, 46.09] And, again, for multi-modal distributions, we can adequately summarize them only by identifying the modes visually and/or describing each mode individually. 5.1.3 Skewness and kurtosis As mentioned above, ranges can be used to indicate a distributions asymmetry (skewness) or fat-tailedness (kurtosis). Although less commonly used, there exist numerical summaries of skewness and kurtosis that can be used instead. The following example shows the kurtosis and skewness for a gamma, normal, logistic and bimodal distribution. Observe how how the bimodal distribution has the lowest kurtosis, because the method fails: library(moments) set.seed(0) tmp &lt;- NULL for (i in 1:4) { if (i == 1) x &lt;- rgamma(1000, a, b) if (i == 2) x &lt;- rnorm(1000, 0, 9) if (i == 3) x &lt;- rlogis(1000, 0, 9) if (i == 4) x &lt;- c(rnorm(700, 0, 9), rnorm(300, 35, 4)) s &lt;- paste0(&quot;kurtosis = &quot;, round(kurtosis(x), 2), &quot; skewness =&quot;, round(skewness(x), 2)) tmp &lt;- rbind(tmp, data.frame(x = x, name = s)) } ggplot(tmp, aes(x = x)) + geom_histogram(bins = 50) + facet_wrap(.~name) 5.1.4 Nominal variables Nominal variables are typically represented with the relative frequencies or probabilities, numerically or visually, but that is not really a summary, because it is also all the information about a nominal variable. That the methods discussed so far in this section apply to numerical variables (rational, interval and to some extent, ordinal) but not nominal variables, because the notions of location and distance do not exist in the nominal case. The only exception to this is the mode, which is the level of the nominal variable with the highest relative frequency or probability. One summary that is often useful for summarizing the dispersion or the uncertainty associated with a nominal variable is entropy. Observe the following example: entropy &lt;- function(x) { x &lt;- x[x != 0] -sum(x * log2(x)) } entropy(c(0.5, 0.5)) # fair coin ## [1] 1 entropy(c(0.8, 0.2)) # biased coin ## [1] 0.7219281 entropy(c(1.0, 0.0)) # coin with heads on both sides ## [1] 0 A fair coin has exactly 1 bit of entropy - we receive 1 bit of information by observing the outcome of a flip. This is also the maximum achievable entropy for a binary variable. A biased coin has lower entropy - we receive less information. In the extreme case of a coin with heads on both sides, the entropy is 0 - the outcome of a flip brings no new information, as we already know it will be heads. When we want to compare entropy across variables with different numbers of levels/categories, we can normalize it by dividing it with the maximum achieavable entropy. For example, observe a fair coin and a fair 6-sided die - in absolute terms, the 6-sided die has higher entropy due to having more possible values. However, relatively to the maximum achievable entropy, both represent maximally uncertain distributions: entropy(c(0.5, 0.5)) # fair coin ## [1] 1 entropy(rep(1/6, 6)) # fair 6-sided die ## [1] 2.584963 Note that entropy can easily be calculated for any discrete random variable. Entropy also has a continuous analogue - differential entropy. 5.1.5 Testing the shape of a distribution Often we want to check if the distribution that underlies our data has the shape of some hypothesized distribution (for example, the normal distribution) or if two samples come from the same distribution. Here, we will present two of the most common methods used: the Kolmogorov-Smirnov test and the Chi-squared goodness-of-fit test. Both of these are Null-hypothesis significance tests (NHST), so, before we proceed, be aware of two things. First, do not use NHST blindly, without a good understanding of their properties and how to interpret their results. And second, if you are more comfortable with thinking in terms of probabilities of hypotheses as opposed to significance and p-values, there always exist Bayesian alternatives to NHST. The Kolmogorov-Smirnov test (KS) is a non-parametric test for testing the equality of two cumulative distribution functions (CDF). These can be two empirical CDFs or an empirical CDF and a theoretical CDF. The KS test statistic is the maximum distance between the two corresponding CDFs. That is, we compute the distribution of this statistic under the null-hypothesis that the CDFs are the same and then observe how extreme the maximum distance is on the sample. To illustrate the KS test, we use it to test the normality of the underlying distributions for two samples - one from a logistic distribution, one from a standard normal distribution. And then to test if the two samples come from the same distribution: set.seed(0) x1 &lt;- rlogis(80, 0, 1) x2 &lt;- rnorm(80, 0, 1) ks.test(x1, y = &quot;pnorm&quot;, 0, 1) ## ## One-sample Kolmogorov-Smirnov test ## ## data: x1 ## D = 0.1575, p-value = 0.03362 ## alternative hypothesis: two-sided ks.test(x2, y = &quot;pnorm&quot;, 0, 1) ## ## One-sample Kolmogorov-Smirnov test ## ## data: x2 ## D = 0.070067, p-value = 0.801 ## alternative hypothesis: two-sided ks.test(x1, x2) ## ## Two-sample Kolmogorov-Smirnov test ## ## data: x1 and x2 ## D = 0.175, p-value = 0.173 ## alternative hypothesis: two-sided So, with a 5% risk (95% confidence), we would reject the null hypothesis that our sample x1 is from a standard normal distribution. On the other hand, we would not reject that our sample x1 is from a standard normal distribution. Finally, we would not reject the null hypothesis that x1 and x2 come from the same distribution. The only guarantee that comes with these results is that we will in the long run falsely reject a true null-hypothesis at most 5% of the time. It says very little about our overall performance, because we do not know the ratio of cases when the null-hypothesis will be true. This example also illustrates the complexity of interpreting NHST results or rather all the tempting traps laid out for us - we might be tempted to conclude, based on the high p-value, that x2 does indeed come from a standard normal, but that then leads us to a weird predicament that we are willing to claim that x1 is not standard normal, x2 is standard normal, but we are less sure that x1 and x2 have different underlying distributions. Note that typical implementations of the KS test assume that the underlying distributions are continuous and ties are therefore impossible. However, the KS test can be generalized to discrete and mixed distributions (see R package KSgeneral). Differences in between distributions can also be assessed visually, through the QQ-plot, a plot that compares the quantiles of the two distributions. If the distributions have the same shape, their quantiles, plotted together, should lie on the same line. The samples from the logistic distribution obviously deviate from the theoretical quantiles of a normal distribution: tmp &lt;- data.frame(y = x1) ggplot(tmp, aes(sample = y)) + stat_qq() + stat_qq_line() The Chi-squared goodness-of-fit test (CHISQ) is a non-parametric test for testing the equality of two categorical distributions. The CHISQ test can also be used on discrete or even continuous data, if there is a reasonable way of binning the data into a finite number of bins. The test statistic is based on a similar idea as the KS test statistic, but instead of observing just the maximum difference, we sum the squared difference between the relative frequency of the two distributions for a bin across all bins. We illustrate the CHISQ test by testing the samples for a biased coin against a theoretical fair coin and the samples from an unbiased 6-sided die against a theoretical fair 6-sided die. set.seed(0) x &lt;- table(sample(0:1, 30, rep = T, prob = c(0.7, 0.3))) chisq.test(x, p = c(0.5, 0.5)) # the default is to compare with uniform theoretical, but we make it explicit here ## ## Chi-squared test for given probabilities ## ## data: x ## X-squared = 3.3333, df = 1, p-value = 0.06789 x &lt;- table(sample(0:1, 40, rep = T, prob = c(0.7, 0.3))) chisq.test(x, p = c(0.5, 0.5)) ## ## Chi-squared test for given probabilities ## ## data: x ## X-squared = 10, df = 1, p-value = 0.001565 x &lt;- table(sample(1:6, 40, rep = T)) chisq.test(x) ## ## Chi-squared test for given probabilities ## ## data: x ## X-squared = 5.3, df = 5, p-value = 0.3804 So, with a 5% risk (95% confidence), we would reject the null hypothesis that our coin is fair, but only in the case with 40 samples. Because fair or close-to fair coins have high entropy, we typically require a lot of samples to distinguish between their underlying probabilities. For a more real-world example, let us take the exit-poll data for the 2016 US Presidential election, broken down by gender, taken from (https://edition.cnn.com/election/2016/results/exit-polls): n &lt;- 24588 male &lt;- round(0.47 * n * c(0.41, 0.52, 0.07)) # some rounding, but it should not affect results female &lt;- round(0.53 * n * c(0.54, 0.41, 0.05)) x &lt;- rbind(male, female) colnames(x) &lt;- c(&quot;Clinton&quot;, &quot;Trump&quot;, &quot;other/no answer&quot;) print(x) # this is also known as a contingency table and the subsequent test as a contingency test ## Clinton Trump other/no answer ## male 4738 6009 809 ## female 7037 5343 652 chisq.test(x) ## ## Pearson&#39;s Chi-squared test ## ## data: x ## X-squared = 417.71, df = 2, p-value &lt; 2.2e-16 So, at any reasonable confidence level, we would reject the null-hypothesis and conclude that there is a difference in how men and women voted. In fact, we do not even need a test, because the difference is so obvious and the sample size so large. The differences between those who earned less or more than 100k$, however, appear smaller, so a test makes more sense: n &lt;- 24588 less100 &lt;- round(0.66 * n * c(0.49, 0.45, 0.06)) # some rounding, but it should not affect results more100 &lt;- round(0.34 * n * c(0.47, 0.47, 0.06)) x &lt;- rbind(less100, more100) colnames(x) &lt;- c(&quot;Clinton&quot;, &quot;Trump&quot;, &quot;other/no answer&quot;) print(x) ## Clinton Trump other/no answer ## less100 7952 7303 974 ## more100 3929 3929 502 chisq.test(x) ## ## Pearson&#39;s Chi-squared test ## ## data: x ## X-squared = 9.3945, df = 2, p-value = 0.00912 Still, we can at most typical levels of confidence reject the null-hypothesis and conclude that that there is a pattern here as well. 5.2 Descriptive statistics for bivariate distributions 5.3 Visualization 5.3.1 Basic principles 5.3.2 Univariate data 5.3.3 Bivariate data 5.4 Further reading and references Kanji, G. K. (2006). 100 statistical tests. Sage. Fundamentals of Descriptive Statistics 1st Edition by Zealure Holcomb (Author) ggplot2: Elegant Graphics for Data Analysis (Use R!) 1st ed. 2009. Corr. 3rd printing 2010 Edition Visualize This: The FlowingData Guide to Design, Visualization, and Statistics Graphical Perception: Theory, Experimentation, and Application to the Development of Graphical Methods William S. Cleveland and Robert McGill Journal of the American Statistical Association Vol. 79, No. 387 (Sep., 1984), pp. 531-554 5.5 Learning outcomes Data science students should work towards obtaining the knowledge and the skills that enable them to: Reproduce the techniques demonstrated in this chapter using their language/tool of choice. Produce visualizations that are aesthetic and without major techical flaws. Recognize when a type of summary is appropriate and when it is not. Apply data summarization techiques to obain insights from data. Once introduced to the bootstrap and other estimation techniques, to be able to combine descriptive statistics with a quantification of uncertainty, such as confidence intervals. 5.6 Practice problems TODO: practice problems and/or homework "],
["dynamic-reports-and-reproducibility.html", "Chapter 6 Dynamic reports and reproducibility", " Chapter 6 Dynamic reports and reproducibility Now let’s talk details. "],
["more-exploratory-data-analysis.html", "Chapter 7 More exploratory data analysis", " Chapter 7 More exploratory data analysis Now let’s talk details. "],
["relational-databases.html", "Chapter 8 Relational databases", " Chapter 8 Relational databases Now let’s talk details. "],
["predictive-modelling.html", "Chapter 9 Predictive modelling", " Chapter 9 Predictive modelling Now let’s talk details. "],
["large-datasets-and-non-relational-databases.html", "Chapter 10 Large datasets and non relational databases", " Chapter 10 Large datasets and non relational databases Now let’s talk details. "],
["parallel-and-distributed-computing.html", "Chapter 11 Parallel and distributed computing", " Chapter 11 Parallel and distributed computing Now let’s talk details. "],
["who-intellectual-property-and-software-licenses.html", "Chapter 12 (who?) Intellectual property and software licenses", " Chapter 12 (who?) Intellectual property and software licenses Now let’s talk details. "],
["who-privacy-security-and-ethics.html", "Chapter 13 (who?) Privacy, security and ethics", " Chapter 13 (who?) Privacy, security and ethics Now let’s talk details. "],
["other-important-topics.html", "Chapter 14 Other important topics 14.1 Testing 14.2 R code 14.3 Python code", " Chapter 14 Other important topics These are optional, but it would be nice if we can get a visiting lecturer to cover them Effective/efficient communication (visual or otherwise) Specifics of data science project management 14.1 Testing \\[\\begin{equation*} \\frac{d}{dx}\\left( \\int_{a}^{x} f(u)\\,du\\right)=f(x) \\end{equation*}\\] \\[\\frac{x}{2}\\] Theorem 14.1 A labeled theorem here. See Theorem 14.1 R Core Team (2016) (R Core Team 2016) 14.2 R code x = rnorm(100) plot(x) 14.3 Python code library(reticulate) print(&quot;Hello World&quot;) ## Hello World a = 33 b = 33 if b &gt; a: print(&quot;b is greater than a&quot;) elif a == b: print(&quot;a and b are equal&quot;) ## a and b are equal References "],
["references.html", "References", " References "]
]
